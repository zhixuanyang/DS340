{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab3-PartB.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpyby_9lsUv1",
        "colab_type": "code",
        "outputId": "4af0027a-0274-4b5b-8b0b-f007899c670f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "%matplotlib inline\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from __future__ import print_function, division\n",
        "import itertools\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import time\n",
        "import csv\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch.nn.init as init\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6q3xTv4seLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "colors = [[31, 120, 180], [51, 160, 44]]\n",
        "colors = [(r / 255, g / 255, b / 255) for (r, g, b) in colors]\n",
        "\n",
        "\n",
        "def plot_losses(train_history, val_history):\n",
        "    x = np.arange(1, len(train_history) + 1)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, train_history, color=colors[0], label=\"Training loss\", linewidth=2)\n",
        "    plt.plot(x, val_history, color=colors[1], label=\"Validation loss\", linewidth=2)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title(\"Evolution of the training and validation loss\")\n",
        "    plt.show()\n",
        "seed = 5678\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.cuda.manual_seed(seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDwXS_EQsgNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def verify_str_arg(value, valid_values):\n",
        "    assert value in valid_values\n",
        "    return value\n",
        "\n",
        "\n",
        "def image_loader(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert('RGB')\n",
        "\n",
        "\n",
        "def load_labels(path):\n",
        "    with open(path, newline='') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        headers = next(reader)\n",
        "        return [{\n",
        "            headers[column_index]: row[column_index]\n",
        "            for column_index in range(len(row))\n",
        "        }\n",
        "                for row in reader]\n",
        "\n",
        "\n",
        "class Kaokore(Dataset):\n",
        "\n",
        "    def __init__(self, root, split='train', category='gender', transform=None):\n",
        "        self.root = root = os.path.expanduser(root)\n",
        "\n",
        "        self.split = verify_str_arg(split, ['train', 'dev', 'test'])\n",
        "\n",
        "        self.category = verify_str_arg(category, ['gender', 'status'])\n",
        "\n",
        "        labels = load_labels(os.path.join(root, 'labels.csv'))\n",
        "        self.entries = [\n",
        "            (label_entry['image'], int(label_entry[category]))\n",
        "            for label_entry in labels\n",
        "            if label_entry['set'] == split and os.path.exists(\n",
        "                os.path.join(self.root, 'images_256', label_entry['image']))\n",
        "        ]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entries)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_filename, label = self.entries[index]\n",
        "\n",
        "        image_filepath = os.path.join(self.root, 'images_256', image_filename)\n",
        "        image = image_loader(image_filepath)\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xue-94a2sjjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_workers = 2\n",
        "test_batch_size = 4\n",
        "transform_new = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Resize(100),\n",
        "    transforms.RandomCrop(size = (100,100),padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
        "\n",
        "root = '/content/drive/My Drive/Colab Notebooks/kaokore-master/kaokore-master/kaokore/'\n",
        "train_set = Kaokore(root,split = 'train',category = 'status',transform = transform_new)\n",
        "dev_set = Kaokore(root,split = 'dev',category = 'status',transform = transform_new)\n",
        "test_set = Kaokore(root,split = 'test',category = 'status',transform = transform_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynxfm3wsub03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=test_batch_size,\n",
        "                                          num_workers=num_workers)\n",
        "val_loader = torch.utils.data.DataLoader(dev_set, batch_size=128,\n",
        "                                          num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=test_batch_size,\n",
        "                                        num_workers=num_workers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX1zjdfku1qL",
        "colab_type": "code",
        "outputId": "080770b1-0b9b-4482-ba51-038c88daf6fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def _weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            if option == 'A':\n",
        "                \"\"\"\n",
        "                For CIFAR10 ResNet paper uses option A.\n",
        "                \"\"\"\n",
        "                self.shortcut = LambdaLayer(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "            elif option == 'B':\n",
        "                self.shortcut = nn.Sequential(\n",
        "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                     nn.BatchNorm2d(self.expansion * planes)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=4):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "net = ResNet(BasicBlock, [3, 3, 3])\n",
        "print(net)\n",
        "\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): LambdaLayer()\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): LambdaLayer()\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNeDWR2eu3ab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createLoss():\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    return criterion\n",
        "def createOptimizer(net):  \n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
        "    return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dflq6gBTu6EQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_train_loader(batch_size):\n",
        "    return torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                          num_workers=num_workers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQbmNnZAvOI3",
        "colab_type": "code",
        "outputId": "895f3307-d58b-457c-994e-972093b1acd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def train(net, batch_size, n_epochs, learning_rate):\n",
        "    \"\"\"\n",
        "    Train a neural network and print statistics of the training\n",
        "    \n",
        "    :param net: (PyTorch Neural Network)\n",
        "    :param batch_size: (int)\n",
        "    :param n_epochs: (int)  Number of iterations on the training set\n",
        "    :param learning_rate: (float) learning rate used by the optimizer\n",
        "    \"\"\"\n",
        "    print(\"===== HYPERPARAMETERS =====\")\n",
        "    print(\"batch_size=\", batch_size)\n",
        "    print(\"n_epochs=\", n_epochs)\n",
        "    print(\"learning_rate=\", learning_rate)\n",
        "    print(\"=\" * 30)\n",
        "    \n",
        "    train_loader = get_train_loader(batch_size)\n",
        "    n_minibatches = len(train_loader)\n",
        "    train_history = []\n",
        "    val_history = []\n",
        "    criterion = createLoss()\n",
        "    optimizer = createOptimizer(net)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,40], gamma=0.1)\n",
        "    training_start_time = time.time()\n",
        "    best_error = np.inf\n",
        "    best_model_path = \"/content/drive/My Drive/Colab Notebooks/best_model_kaokore.pth\"\n",
        "    \n",
        "    net = net.to(device)\n",
        "\n",
        "    for epoch in range(n_epochs):  \n",
        "        running_loss = 0.0\n",
        "        print_every = n_minibatches // 10\n",
        "        start_time = time.time()\n",
        "        total_train_loss = 0\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            total_train_loss += loss.item()\n",
        "            if (i + 1) % (print_every + 1) == 0:    \n",
        "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
        "                      epoch + 1, int(100 * (i + 1) / n_minibatches), running_loss / print_every,\n",
        "                      time.time() - start_time))\n",
        "                running_loss = 0.0\n",
        "                start_time = time.time()\n",
        "        train_history.append(total_train_loss / len(train_loader))\n",
        "        \n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "          for inputs, labels in val_loader:\n",
        "              inputs, labels = inputs.to(device), labels.to(device)\n",
        "              predictions = net(inputs)\n",
        "              val_loss = criterion(predictions, labels)\n",
        "              total_val_loss += val_loss.item()\n",
        "        val_history.append(total_val_loss / len(val_loader))\n",
        "        if total_val_loss < best_error:\n",
        "            best_error = total_val_loss\n",
        "            torch.save(net.state_dict(), best_model_path)\n",
        "        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(val_loader)))\n",
        "        scheduler.step()\n",
        "    print(\"Training Finished, took {:.2f}s\".format(time.time() - training_start_time))\n",
        "    \n",
        "    net.load_state_dict(torch.load(best_model_path))\n",
        "    \n",
        "    return train_history, val_history\n",
        "\n",
        "train_history, val_history = train(net, batch_size=32, n_epochs=50, learning_rate=0.1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== HYPERPARAMETERS =====\n",
            "batch_size= 32\n",
            "n_epochs= 50\n",
            "learning_rate= 0.1\n",
            "==============================\n",
            "Epoch 1, 10% \t train_loss: 2.16 took: 83.41s\n",
            "Epoch 1, 21% \t train_loss: 1.47 took: 87.91s\n",
            "Epoch 1, 31% \t train_loss: 1.32 took: 80.60s\n",
            "Epoch 1, 42% \t train_loss: 1.29 took: 86.49s\n",
            "Epoch 1, 52% \t train_loss: 1.21 took: 78.90s\n",
            "Epoch 1, 63% \t train_loss: 1.28 took: 78.30s\n",
            "Epoch 1, 73% \t train_loss: 1.23 took: 75.40s\n",
            "Epoch 1, 84% \t train_loss: 1.21 took: 79.86s\n",
            "Epoch 1, 94% \t train_loss: 1.19 took: 85.66s\n",
            "Validation loss = 1.04\n",
            "Epoch 2, 10% \t train_loss: 1.23 took: 1.92s\n",
            "Epoch 2, 21% \t train_loss: 1.19 took: 1.69s\n",
            "Epoch 2, 31% \t train_loss: 1.20 took: 1.76s\n",
            "Epoch 2, 42% \t train_loss: 1.20 took: 1.81s\n",
            "Epoch 2, 52% \t train_loss: 1.15 took: 1.72s\n",
            "Epoch 2, 63% \t train_loss: 1.22 took: 1.80s\n",
            "Epoch 2, 73% \t train_loss: 1.18 took: 1.70s\n",
            "Epoch 2, 84% \t train_loss: 1.19 took: 1.79s\n",
            "Epoch 2, 94% \t train_loss: 1.17 took: 1.74s\n",
            "Validation loss = 1.01\n",
            "Epoch 3, 10% \t train_loss: 1.19 took: 1.99s\n",
            "Epoch 3, 21% \t train_loss: 1.17 took: 1.77s\n",
            "Epoch 3, 31% \t train_loss: 1.19 took: 1.80s\n",
            "Epoch 3, 42% \t train_loss: 1.18 took: 1.68s\n",
            "Epoch 3, 52% \t train_loss: 1.13 took: 1.69s\n",
            "Epoch 3, 63% \t train_loss: 1.19 took: 1.76s\n",
            "Epoch 3, 73% \t train_loss: 1.14 took: 1.70s\n",
            "Epoch 3, 84% \t train_loss: 1.17 took: 1.81s\n",
            "Epoch 3, 94% \t train_loss: 1.14 took: 1.79s\n",
            "Validation loss = 1.00\n",
            "Epoch 4, 10% \t train_loss: 1.18 took: 1.99s\n",
            "Epoch 4, 21% \t train_loss: 1.16 took: 1.65s\n",
            "Epoch 4, 31% \t train_loss: 1.20 took: 1.75s\n",
            "Epoch 4, 42% \t train_loss: 1.18 took: 1.67s\n",
            "Epoch 4, 52% \t train_loss: 1.12 took: 1.79s\n",
            "Epoch 4, 63% \t train_loss: 1.20 took: 1.71s\n",
            "Epoch 4, 73% \t train_loss: 1.15 took: 1.76s\n",
            "Epoch 4, 84% \t train_loss: 1.16 took: 1.74s\n",
            "Epoch 4, 94% \t train_loss: 1.11 took: 1.73s\n",
            "Validation loss = 0.98\n",
            "Epoch 5, 10% \t train_loss: 1.18 took: 1.85s\n",
            "Epoch 5, 21% \t train_loss: 1.14 took: 1.72s\n",
            "Epoch 5, 31% \t train_loss: 1.19 took: 1.80s\n",
            "Epoch 5, 42% \t train_loss: 1.14 took: 1.71s\n",
            "Epoch 5, 52% \t train_loss: 1.08 took: 1.76s\n",
            "Epoch 5, 63% \t train_loss: 1.19 took: 1.77s\n",
            "Epoch 5, 73% \t train_loss: 1.14 took: 1.74s\n",
            "Epoch 5, 84% \t train_loss: 1.14 took: 1.74s\n",
            "Epoch 5, 94% \t train_loss: 1.09 took: 1.76s\n",
            "Validation loss = 0.97\n",
            "Epoch 6, 10% \t train_loss: 1.14 took: 1.97s\n",
            "Epoch 6, 21% \t train_loss: 1.13 took: 1.75s\n",
            "Epoch 6, 31% \t train_loss: 1.15 took: 1.73s\n",
            "Epoch 6, 42% \t train_loss: 1.12 took: 1.74s\n",
            "Epoch 6, 52% \t train_loss: 1.08 took: 1.64s\n",
            "Epoch 6, 63% \t train_loss: 1.16 took: 1.74s\n",
            "Epoch 6, 73% \t train_loss: 1.11 took: 1.69s\n",
            "Epoch 6, 84% \t train_loss: 1.14 took: 1.80s\n",
            "Epoch 6, 94% \t train_loss: 1.06 took: 1.66s\n",
            "Validation loss = 0.95\n",
            "Epoch 7, 10% \t train_loss: 1.13 took: 2.01s\n",
            "Epoch 7, 21% \t train_loss: 1.13 took: 1.81s\n",
            "Epoch 7, 31% \t train_loss: 1.14 took: 1.71s\n",
            "Epoch 7, 42% \t train_loss: 1.10 took: 1.71s\n",
            "Epoch 7, 52% \t train_loss: 1.05 took: 1.76s\n",
            "Epoch 7, 63% \t train_loss: 1.14 took: 1.72s\n",
            "Epoch 7, 73% \t train_loss: 1.10 took: 1.75s\n",
            "Epoch 7, 84% \t train_loss: 1.10 took: 1.74s\n",
            "Epoch 7, 94% \t train_loss: 1.02 took: 1.81s\n",
            "Validation loss = 0.91\n",
            "Epoch 8, 10% \t train_loss: 1.11 took: 1.96s\n",
            "Epoch 8, 21% \t train_loss: 1.08 took: 1.73s\n",
            "Epoch 8, 31% \t train_loss: 1.06 took: 1.77s\n",
            "Epoch 8, 42% \t train_loss: 1.06 took: 1.70s\n",
            "Epoch 8, 52% \t train_loss: 1.02 took: 1.74s\n",
            "Epoch 8, 63% \t train_loss: 1.10 took: 1.66s\n",
            "Epoch 8, 73% \t train_loss: 1.04 took: 1.72s\n",
            "Epoch 8, 84% \t train_loss: 1.06 took: 1.75s\n",
            "Epoch 8, 94% \t train_loss: 1.03 took: 1.66s\n",
            "Validation loss = 0.87\n",
            "Epoch 9, 10% \t train_loss: 1.03 took: 1.95s\n",
            "Epoch 9, 21% \t train_loss: 1.03 took: 1.75s\n",
            "Epoch 9, 31% \t train_loss: 1.07 took: 1.75s\n",
            "Epoch 9, 42% \t train_loss: 1.03 took: 1.73s\n",
            "Epoch 9, 52% \t train_loss: 0.99 took: 1.72s\n",
            "Epoch 9, 63% \t train_loss: 1.09 took: 1.68s\n",
            "Epoch 9, 73% \t train_loss: 1.02 took: 1.75s\n",
            "Epoch 9, 84% \t train_loss: 0.99 took: 1.77s\n",
            "Epoch 9, 94% \t train_loss: 0.96 took: 1.75s\n",
            "Validation loss = 0.84\n",
            "Epoch 10, 10% \t train_loss: 1.02 took: 1.92s\n",
            "Epoch 10, 21% \t train_loss: 1.03 took: 1.68s\n",
            "Epoch 10, 31% \t train_loss: 1.02 took: 1.76s\n",
            "Epoch 10, 42% \t train_loss: 1.00 took: 1.81s\n",
            "Epoch 10, 52% \t train_loss: 0.95 took: 1.73s\n",
            "Epoch 10, 63% \t train_loss: 1.11 took: 1.76s\n",
            "Epoch 10, 73% \t train_loss: 0.96 took: 1.73s\n",
            "Epoch 10, 84% \t train_loss: 0.95 took: 1.69s\n",
            "Epoch 10, 94% \t train_loss: 0.93 took: 1.76s\n",
            "Validation loss = 0.82\n",
            "Epoch 11, 10% \t train_loss: 0.99 took: 1.92s\n",
            "Epoch 11, 21% \t train_loss: 0.97 took: 1.69s\n",
            "Epoch 11, 31% \t train_loss: 0.98 took: 1.66s\n",
            "Epoch 11, 42% \t train_loss: 0.98 took: 1.75s\n",
            "Epoch 11, 52% \t train_loss: 0.92 took: 1.81s\n",
            "Epoch 11, 63% \t train_loss: 1.02 took: 1.75s\n",
            "Epoch 11, 73% \t train_loss: 0.94 took: 1.72s\n",
            "Epoch 11, 84% \t train_loss: 0.94 took: 1.83s\n",
            "Epoch 11, 94% \t train_loss: 0.91 took: 1.75s\n",
            "Validation loss = 0.82\n",
            "Epoch 12, 10% \t train_loss: 0.94 took: 2.01s\n",
            "Epoch 12, 21% \t train_loss: 0.93 took: 1.77s\n",
            "Epoch 12, 31% \t train_loss: 0.98 took: 1.76s\n",
            "Epoch 12, 42% \t train_loss: 1.00 took: 1.77s\n",
            "Epoch 12, 52% \t train_loss: 0.88 took: 1.78s\n",
            "Epoch 12, 63% \t train_loss: 1.02 took: 1.71s\n",
            "Epoch 12, 73% \t train_loss: 0.95 took: 1.74s\n",
            "Epoch 12, 84% \t train_loss: 0.91 took: 1.71s\n",
            "Epoch 12, 94% \t train_loss: 0.85 took: 1.65s\n",
            "Validation loss = 0.82\n",
            "Epoch 13, 10% \t train_loss: 0.93 took: 1.99s\n",
            "Epoch 13, 21% \t train_loss: 0.89 took: 1.76s\n",
            "Epoch 13, 31% \t train_loss: 0.94 took: 1.75s\n",
            "Epoch 13, 42% \t train_loss: 0.92 took: 1.78s\n",
            "Epoch 13, 52% \t train_loss: 0.85 took: 1.71s\n",
            "Epoch 13, 63% \t train_loss: 0.97 took: 1.74s\n",
            "Epoch 13, 73% \t train_loss: 0.92 took: 1.70s\n",
            "Epoch 13, 84% \t train_loss: 0.87 took: 1.75s\n",
            "Epoch 13, 94% \t train_loss: 0.82 took: 1.74s\n",
            "Validation loss = 0.78\n",
            "Epoch 14, 10% \t train_loss: 0.91 took: 1.89s\n",
            "Epoch 14, 21% \t train_loss: 0.84 took: 1.69s\n",
            "Epoch 14, 31% \t train_loss: 0.94 took: 1.80s\n",
            "Epoch 14, 42% \t train_loss: 0.89 took: 1.74s\n",
            "Epoch 14, 52% \t train_loss: 0.84 took: 1.72s\n",
            "Epoch 14, 63% \t train_loss: 0.97 took: 1.73s\n",
            "Epoch 14, 73% \t train_loss: 0.89 took: 1.66s\n",
            "Epoch 14, 84% \t train_loss: 0.86 took: 1.65s\n",
            "Epoch 14, 94% \t train_loss: 0.78 took: 1.69s\n",
            "Validation loss = 0.77\n",
            "Epoch 15, 10% \t train_loss: 0.85 took: 1.98s\n",
            "Epoch 15, 21% \t train_loss: 0.84 took: 1.70s\n",
            "Epoch 15, 31% \t train_loss: 0.91 took: 1.76s\n",
            "Epoch 15, 42% \t train_loss: 0.89 took: 1.77s\n",
            "Epoch 15, 52% \t train_loss: 0.80 took: 1.72s\n",
            "Epoch 15, 63% \t train_loss: 0.93 took: 1.77s\n",
            "Epoch 15, 73% \t train_loss: 0.88 took: 1.77s\n",
            "Epoch 15, 84% \t train_loss: 0.79 took: 1.72s\n",
            "Epoch 15, 94% \t train_loss: 0.77 took: 1.75s\n",
            "Validation loss = 0.75\n",
            "Epoch 16, 10% \t train_loss: 0.83 took: 1.93s\n",
            "Epoch 16, 21% \t train_loss: 0.81 took: 1.76s\n",
            "Epoch 16, 31% \t train_loss: 0.88 took: 1.81s\n",
            "Epoch 16, 42% \t train_loss: 0.85 took: 1.75s\n",
            "Epoch 16, 52% \t train_loss: 0.82 took: 1.71s\n",
            "Epoch 16, 63% \t train_loss: 0.89 took: 1.67s\n",
            "Epoch 16, 73% \t train_loss: 0.83 took: 1.75s\n",
            "Epoch 16, 84% \t train_loss: 0.81 took: 1.68s\n",
            "Epoch 16, 94% \t train_loss: 0.77 took: 1.76s\n",
            "Validation loss = 0.77\n",
            "Epoch 17, 10% \t train_loss: 0.82 took: 1.95s\n",
            "Epoch 17, 21% \t train_loss: 0.79 took: 1.74s\n",
            "Epoch 17, 31% \t train_loss: 0.84 took: 1.84s\n",
            "Epoch 17, 42% \t train_loss: 0.81 took: 1.69s\n",
            "Epoch 17, 52% \t train_loss: 0.78 took: 1.65s\n",
            "Epoch 17, 63% \t train_loss: 0.88 took: 1.76s\n",
            "Epoch 17, 73% \t train_loss: 0.76 took: 1.69s\n",
            "Epoch 17, 84% \t train_loss: 0.77 took: 1.75s\n",
            "Epoch 17, 94% \t train_loss: 0.76 took: 1.77s\n",
            "Validation loss = 0.76\n",
            "Epoch 18, 10% \t train_loss: 0.81 took: 1.97s\n",
            "Epoch 18, 21% \t train_loss: 0.82 took: 1.67s\n",
            "Epoch 18, 31% \t train_loss: 0.86 took: 1.76s\n",
            "Epoch 18, 42% \t train_loss: 0.81 took: 1.80s\n",
            "Epoch 18, 52% \t train_loss: 0.74 took: 1.76s\n",
            "Epoch 18, 63% \t train_loss: 0.84 took: 1.80s\n",
            "Epoch 18, 73% \t train_loss: 0.78 took: 1.77s\n",
            "Epoch 18, 84% \t train_loss: 0.78 took: 1.77s\n",
            "Epoch 18, 94% \t train_loss: 0.73 took: 1.76s\n",
            "Validation loss = 0.74\n",
            "Epoch 19, 10% \t train_loss: 0.79 took: 1.94s\n",
            "Epoch 19, 21% \t train_loss: 0.72 took: 1.74s\n",
            "Epoch 19, 31% \t train_loss: 0.83 took: 1.82s\n",
            "Epoch 19, 42% \t train_loss: 0.78 took: 1.70s\n",
            "Epoch 19, 52% \t train_loss: 0.73 took: 1.70s\n",
            "Epoch 19, 63% \t train_loss: 0.81 took: 1.69s\n",
            "Epoch 19, 73% \t train_loss: 0.79 took: 1.69s\n",
            "Epoch 19, 84% \t train_loss: 0.78 took: 1.75s\n",
            "Epoch 19, 94% \t train_loss: 0.72 took: 1.73s\n",
            "Validation loss = 0.71\n",
            "Epoch 20, 10% \t train_loss: 0.76 took: 1.94s\n",
            "Epoch 20, 21% \t train_loss: 0.70 took: 1.73s\n",
            "Epoch 20, 31% \t train_loss: 0.80 took: 1.80s\n",
            "Epoch 20, 42% \t train_loss: 0.77 took: 1.63s\n",
            "Epoch 20, 52% \t train_loss: 0.66 took: 1.72s\n",
            "Epoch 20, 63% \t train_loss: 0.81 took: 1.70s\n",
            "Epoch 20, 73% \t train_loss: 0.74 took: 1.79s\n",
            "Epoch 20, 84% \t train_loss: 0.74 took: 1.70s\n",
            "Epoch 20, 94% \t train_loss: 0.69 took: 1.74s\n",
            "Validation loss = 0.71\n",
            "Epoch 21, 10% \t train_loss: 0.72 took: 1.98s\n",
            "Epoch 21, 21% \t train_loss: 0.68 took: 1.77s\n",
            "Epoch 21, 31% \t train_loss: 0.75 took: 1.69s\n",
            "Epoch 21, 42% \t train_loss: 0.76 took: 1.67s\n",
            "Epoch 21, 52% \t train_loss: 0.69 took: 1.65s\n",
            "Epoch 21, 63% \t train_loss: 0.81 took: 1.75s\n",
            "Epoch 21, 73% \t train_loss: 0.71 took: 1.74s\n",
            "Epoch 21, 84% \t train_loss: 0.75 took: 1.70s\n",
            "Epoch 21, 94% \t train_loss: 0.66 took: 1.76s\n",
            "Validation loss = 0.66\n",
            "Epoch 22, 10% \t train_loss: 0.72 took: 1.93s\n",
            "Epoch 22, 21% \t train_loss: 0.66 took: 1.73s\n",
            "Epoch 22, 31% \t train_loss: 0.72 took: 1.75s\n",
            "Epoch 22, 42% \t train_loss: 0.72 took: 1.75s\n",
            "Epoch 22, 52% \t train_loss: 0.66 took: 1.76s\n",
            "Epoch 22, 63% \t train_loss: 0.79 took: 1.75s\n",
            "Epoch 22, 73% \t train_loss: 0.68 took: 1.76s\n",
            "Epoch 22, 84% \t train_loss: 0.69 took: 1.73s\n",
            "Epoch 22, 94% \t train_loss: 0.65 took: 1.67s\n",
            "Validation loss = 0.69\n",
            "Epoch 23, 10% \t train_loss: 0.68 took: 1.98s\n",
            "Epoch 23, 21% \t train_loss: 0.64 took: 1.77s\n",
            "Epoch 23, 31% \t train_loss: 0.74 took: 1.75s\n",
            "Epoch 23, 42% \t train_loss: 0.73 took: 1.69s\n",
            "Epoch 23, 52% \t train_loss: 0.64 took: 1.70s\n",
            "Epoch 23, 63% \t train_loss: 0.77 took: 1.77s\n",
            "Epoch 23, 73% \t train_loss: 0.67 took: 1.76s\n",
            "Epoch 23, 84% \t train_loss: 0.66 took: 1.70s\n",
            "Epoch 23, 94% \t train_loss: 0.63 took: 1.69s\n",
            "Validation loss = 0.72\n",
            "Epoch 24, 10% \t train_loss: 0.70 took: 1.97s\n",
            "Epoch 24, 21% \t train_loss: 0.66 took: 1.67s\n",
            "Epoch 24, 31% \t train_loss: 0.71 took: 1.68s\n",
            "Epoch 24, 42% \t train_loss: 0.71 took: 1.77s\n",
            "Epoch 24, 52% \t train_loss: 0.63 took: 1.69s\n",
            "Epoch 24, 63% \t train_loss: 0.75 took: 1.79s\n",
            "Epoch 24, 73% \t train_loss: 0.65 took: 1.65s\n",
            "Epoch 24, 84% \t train_loss: 0.70 took: 1.72s\n",
            "Epoch 24, 94% \t train_loss: 0.57 took: 1.74s\n",
            "Validation loss = 0.65\n",
            "Epoch 25, 10% \t train_loss: 0.63 took: 1.92s\n",
            "Epoch 25, 21% \t train_loss: 0.62 took: 1.72s\n",
            "Epoch 25, 31% \t train_loss: 0.67 took: 1.76s\n",
            "Epoch 25, 42% \t train_loss: 0.69 took: 1.87s\n",
            "Epoch 25, 52% \t train_loss: 0.62 took: 1.75s\n",
            "Epoch 25, 63% \t train_loss: 0.76 took: 1.74s\n",
            "Epoch 25, 73% \t train_loss: 0.67 took: 1.78s\n",
            "Epoch 25, 84% \t train_loss: 0.67 took: 1.65s\n",
            "Epoch 25, 94% \t train_loss: 0.62 took: 1.75s\n",
            "Validation loss = 0.68\n",
            "Epoch 26, 10% \t train_loss: 0.64 took: 1.93s\n",
            "Epoch 26, 21% \t train_loss: 0.59 took: 1.81s\n",
            "Epoch 26, 31% \t train_loss: 0.66 took: 1.80s\n",
            "Epoch 26, 42% \t train_loss: 0.66 took: 1.69s\n",
            "Epoch 26, 52% \t train_loss: 0.59 took: 1.81s\n",
            "Epoch 26, 63% \t train_loss: 0.74 took: 1.67s\n",
            "Epoch 26, 73% \t train_loss: 0.64 took: 1.72s\n",
            "Epoch 26, 84% \t train_loss: 0.66 took: 1.72s\n",
            "Epoch 26, 94% \t train_loss: 0.61 took: 1.73s\n",
            "Validation loss = 0.67\n",
            "Epoch 27, 10% \t train_loss: 0.62 took: 1.95s\n",
            "Epoch 27, 21% \t train_loss: 0.57 took: 1.77s\n",
            "Epoch 27, 31% \t train_loss: 0.62 took: 1.69s\n",
            "Epoch 27, 42% \t train_loss: 0.65 took: 1.73s\n",
            "Epoch 27, 52% \t train_loss: 0.56 took: 1.71s\n",
            "Epoch 27, 63% \t train_loss: 0.71 took: 1.81s\n",
            "Epoch 27, 73% \t train_loss: 0.62 took: 1.73s\n",
            "Epoch 27, 84% \t train_loss: 0.61 took: 1.70s\n",
            "Epoch 27, 94% \t train_loss: 0.57 took: 1.71s\n",
            "Validation loss = 0.72\n",
            "Epoch 28, 10% \t train_loss: 0.62 took: 1.96s\n",
            "Epoch 28, 21% \t train_loss: 0.59 took: 1.70s\n",
            "Epoch 28, 31% \t train_loss: 0.63 took: 1.71s\n",
            "Epoch 28, 42% \t train_loss: 0.71 took: 1.81s\n",
            "Epoch 28, 52% \t train_loss: 0.59 took: 1.71s\n",
            "Epoch 28, 63% \t train_loss: 0.71 took: 1.79s\n",
            "Epoch 28, 73% \t train_loss: 0.58 took: 1.82s\n",
            "Epoch 28, 84% \t train_loss: 0.63 took: 1.75s\n",
            "Epoch 28, 94% \t train_loss: 0.54 took: 1.76s\n",
            "Validation loss = 0.69\n",
            "Epoch 29, 10% \t train_loss: 0.56 took: 1.92s\n",
            "Epoch 29, 21% \t train_loss: 0.55 took: 1.81s\n",
            "Epoch 29, 31% \t train_loss: 0.58 took: 1.78s\n",
            "Epoch 29, 42% \t train_loss: 0.66 took: 1.68s\n",
            "Epoch 29, 52% \t train_loss: 0.59 took: 1.70s\n",
            "Epoch 29, 63% \t train_loss: 0.72 took: 1.74s\n",
            "Epoch 29, 73% \t train_loss: 0.62 took: 1.70s\n",
            "Epoch 29, 84% \t train_loss: 0.59 took: 1.75s\n",
            "Epoch 29, 94% \t train_loss: 0.53 took: 1.77s\n",
            "Validation loss = 0.66\n",
            "Epoch 30, 10% \t train_loss: 0.56 took: 1.91s\n",
            "Epoch 30, 21% \t train_loss: 0.57 took: 1.68s\n",
            "Epoch 30, 31% \t train_loss: 0.56 took: 1.78s\n",
            "Epoch 30, 42% \t train_loss: 0.66 took: 1.75s\n",
            "Epoch 30, 52% \t train_loss: 0.56 took: 1.75s\n",
            "Epoch 30, 63% \t train_loss: 0.68 took: 1.77s\n",
            "Epoch 30, 73% \t train_loss: 0.56 took: 1.74s\n",
            "Epoch 30, 84% \t train_loss: 0.53 took: 1.69s\n",
            "Epoch 30, 94% \t train_loss: 0.53 took: 1.79s\n",
            "Validation loss = 0.66\n",
            "Epoch 31, 10% \t train_loss: 0.51 took: 1.96s\n",
            "Epoch 31, 21% \t train_loss: 0.49 took: 1.67s\n",
            "Epoch 31, 31% \t train_loss: 0.55 took: 1.71s\n",
            "Epoch 31, 42% \t train_loss: 0.52 took: 1.74s\n",
            "Epoch 31, 52% \t train_loss: 0.48 took: 1.71s\n",
            "Epoch 31, 63% \t train_loss: 0.57 took: 1.74s\n",
            "Epoch 31, 73% \t train_loss: 0.43 took: 1.74s\n",
            "Epoch 31, 84% \t train_loss: 0.46 took: 1.73s\n",
            "Epoch 31, 94% \t train_loss: 0.40 took: 1.69s\n",
            "Validation loss = 0.56\n",
            "Epoch 32, 10% \t train_loss: 0.45 took: 1.98s\n",
            "Epoch 32, 21% \t train_loss: 0.44 took: 1.70s\n",
            "Epoch 32, 31% \t train_loss: 0.44 took: 1.74s\n",
            "Epoch 32, 42% \t train_loss: 0.47 took: 1.76s\n",
            "Epoch 32, 52% \t train_loss: 0.44 took: 1.72s\n",
            "Epoch 32, 63% \t train_loss: 0.54 took: 1.76s\n",
            "Epoch 32, 73% \t train_loss: 0.41 took: 1.70s\n",
            "Epoch 32, 84% \t train_loss: 0.42 took: 1.77s\n",
            "Epoch 32, 94% \t train_loss: 0.39 took: 1.72s\n",
            "Validation loss = 0.57\n",
            "Epoch 33, 10% \t train_loss: 0.44 took: 1.95s\n",
            "Epoch 33, 21% \t train_loss: 0.45 took: 1.79s\n",
            "Epoch 33, 31% \t train_loss: 0.45 took: 1.75s\n",
            "Epoch 33, 42% \t train_loss: 0.45 took: 1.70s\n",
            "Epoch 33, 52% \t train_loss: 0.42 took: 1.69s\n",
            "Epoch 33, 63% \t train_loss: 0.52 took: 1.72s\n",
            "Epoch 33, 73% \t train_loss: 0.40 took: 1.74s\n",
            "Epoch 33, 84% \t train_loss: 0.43 took: 1.72s\n",
            "Epoch 33, 94% \t train_loss: 0.39 took: 1.80s\n",
            "Validation loss = 0.57\n",
            "Epoch 34, 10% \t train_loss: 0.42 took: 1.90s\n",
            "Epoch 34, 21% \t train_loss: 0.41 took: 1.82s\n",
            "Epoch 34, 31% \t train_loss: 0.44 took: 1.74s\n",
            "Epoch 34, 42% \t train_loss: 0.42 took: 1.77s\n",
            "Epoch 34, 52% \t train_loss: 0.40 took: 1.73s\n",
            "Epoch 34, 63% \t train_loss: 0.52 took: 1.75s\n",
            "Epoch 34, 73% \t train_loss: 0.38 took: 1.74s\n",
            "Epoch 34, 84% \t train_loss: 0.43 took: 1.74s\n",
            "Epoch 34, 94% \t train_loss: 0.37 took: 1.72s\n",
            "Validation loss = 0.57\n",
            "Epoch 35, 10% \t train_loss: 0.38 took: 1.96s\n",
            "Epoch 35, 21% \t train_loss: 0.40 took: 1.74s\n",
            "Epoch 35, 31% \t train_loss: 0.41 took: 1.74s\n",
            "Epoch 35, 42% \t train_loss: 0.42 took: 1.72s\n",
            "Epoch 35, 52% \t train_loss: 0.38 took: 1.74s\n",
            "Epoch 35, 63% \t train_loss: 0.48 took: 1.75s\n",
            "Epoch 35, 73% \t train_loss: 0.36 took: 1.72s\n",
            "Epoch 35, 84% \t train_loss: 0.40 took: 1.77s\n",
            "Epoch 35, 94% \t train_loss: 0.37 took: 1.71s\n",
            "Validation loss = 0.56\n",
            "Epoch 36, 10% \t train_loss: 0.38 took: 1.93s\n",
            "Epoch 36, 21% \t train_loss: 0.37 took: 1.76s\n",
            "Epoch 36, 31% \t train_loss: 0.41 took: 1.78s\n",
            "Epoch 36, 42% \t train_loss: 0.40 took: 1.71s\n",
            "Epoch 36, 52% \t train_loss: 0.40 took: 1.75s\n",
            "Epoch 36, 63% \t train_loss: 0.46 took: 1.73s\n",
            "Epoch 36, 73% \t train_loss: 0.34 took: 1.77s\n",
            "Epoch 36, 84% \t train_loss: 0.40 took: 1.72s\n",
            "Epoch 36, 94% \t train_loss: 0.37 took: 1.76s\n",
            "Validation loss = 0.58\n",
            "Epoch 37, 10% \t train_loss: 0.38 took: 1.96s\n",
            "Epoch 37, 21% \t train_loss: 0.37 took: 1.74s\n",
            "Epoch 37, 31% \t train_loss: 0.39 took: 1.66s\n",
            "Epoch 37, 42% \t train_loss: 0.40 took: 1.73s\n",
            "Epoch 37, 52% \t train_loss: 0.37 took: 1.68s\n",
            "Epoch 37, 63% \t train_loss: 0.46 took: 1.76s\n",
            "Epoch 37, 73% \t train_loss: 0.36 took: 1.73s\n",
            "Epoch 37, 84% \t train_loss: 0.39 took: 1.75s\n",
            "Epoch 37, 94% \t train_loss: 0.35 took: 1.80s\n",
            "Validation loss = 0.60\n",
            "Epoch 38, 10% \t train_loss: 0.35 took: 2.01s\n",
            "Epoch 38, 21% \t train_loss: 0.37 took: 1.70s\n",
            "Epoch 38, 31% \t train_loss: 0.38 took: 1.72s\n",
            "Epoch 38, 42% \t train_loss: 0.39 took: 1.69s\n",
            "Epoch 38, 52% \t train_loss: 0.37 took: 1.72s\n",
            "Epoch 38, 63% \t train_loss: 0.46 took: 1.78s\n",
            "Epoch 38, 73% \t train_loss: 0.33 took: 1.72s\n",
            "Epoch 38, 84% \t train_loss: 0.38 took: 1.77s\n",
            "Epoch 38, 94% \t train_loss: 0.37 took: 1.78s\n",
            "Validation loss = 0.57\n",
            "Epoch 39, 10% \t train_loss: 0.36 took: 1.89s\n",
            "Epoch 39, 21% \t train_loss: 0.37 took: 1.77s\n",
            "Epoch 39, 31% \t train_loss: 0.35 took: 1.79s\n",
            "Epoch 39, 42% \t train_loss: 0.38 took: 1.74s\n",
            "Epoch 39, 52% \t train_loss: 0.36 took: 1.68s\n",
            "Epoch 39, 63% \t train_loss: 0.44 took: 1.72s\n",
            "Epoch 39, 73% \t train_loss: 0.35 took: 1.69s\n",
            "Epoch 39, 84% \t train_loss: 0.35 took: 1.78s\n",
            "Epoch 39, 94% \t train_loss: 0.33 took: 1.68s\n",
            "Validation loss = 0.60\n",
            "Epoch 40, 10% \t train_loss: 0.32 took: 1.96s\n",
            "Epoch 40, 21% \t train_loss: 0.35 took: 1.73s\n",
            "Epoch 40, 31% \t train_loss: 0.37 took: 1.73s\n",
            "Epoch 40, 42% \t train_loss: 0.37 took: 1.70s\n",
            "Epoch 40, 52% \t train_loss: 0.35 took: 1.71s\n",
            "Epoch 40, 63% \t train_loss: 0.42 took: 1.73s\n",
            "Epoch 40, 73% \t train_loss: 0.34 took: 1.63s\n",
            "Epoch 40, 84% \t train_loss: 0.36 took: 1.70s\n",
            "Epoch 40, 94% \t train_loss: 0.31 took: 1.66s\n",
            "Validation loss = 0.58\n",
            "Epoch 41, 10% \t train_loss: 0.35 took: 1.97s\n",
            "Epoch 41, 21% \t train_loss: 0.35 took: 1.71s\n",
            "Epoch 41, 31% \t train_loss: 0.35 took: 1.72s\n",
            "Epoch 41, 42% \t train_loss: 0.35 took: 1.69s\n",
            "Epoch 41, 52% \t train_loss: 0.32 took: 1.61s\n",
            "Epoch 41, 63% \t train_loss: 0.39 took: 1.79s\n",
            "Epoch 41, 73% \t train_loss: 0.31 took: 1.79s\n",
            "Epoch 41, 84% \t train_loss: 0.31 took: 1.75s\n",
            "Epoch 41, 94% \t train_loss: 0.29 took: 1.77s\n",
            "Validation loss = 0.57\n",
            "Epoch 42, 10% \t train_loss: 0.33 took: 1.94s\n",
            "Epoch 42, 21% \t train_loss: 0.35 took: 1.76s\n",
            "Epoch 42, 31% \t train_loss: 0.36 took: 1.74s\n",
            "Epoch 42, 42% \t train_loss: 0.35 took: 1.76s\n",
            "Epoch 42, 52% \t train_loss: 0.31 took: 1.71s\n",
            "Epoch 42, 63% \t train_loss: 0.37 took: 1.79s\n",
            "Epoch 42, 73% \t train_loss: 0.28 took: 1.74s\n",
            "Epoch 42, 84% \t train_loss: 0.32 took: 1.71s\n",
            "Epoch 42, 94% \t train_loss: 0.27 took: 1.64s\n",
            "Validation loss = 0.59\n",
            "Epoch 43, 10% \t train_loss: 0.31 took: 1.95s\n",
            "Epoch 43, 21% \t train_loss: 0.33 took: 1.76s\n",
            "Epoch 43, 31% \t train_loss: 0.33 took: 1.66s\n",
            "Epoch 43, 42% \t train_loss: 0.34 took: 1.67s\n",
            "Epoch 43, 52% \t train_loss: 0.29 took: 1.71s\n",
            "Epoch 43, 63% \t train_loss: 0.37 took: 1.74s\n",
            "Epoch 43, 73% \t train_loss: 0.28 took: 1.72s\n",
            "Epoch 43, 84% \t train_loss: 0.31 took: 1.73s\n",
            "Epoch 43, 94% \t train_loss: 0.27 took: 1.65s\n",
            "Validation loss = 0.59\n",
            "Epoch 44, 10% \t train_loss: 0.34 took: 1.94s\n",
            "Epoch 44, 21% \t train_loss: 0.34 took: 1.72s\n",
            "Epoch 44, 31% \t train_loss: 0.34 took: 1.74s\n",
            "Epoch 44, 42% \t train_loss: 0.34 took: 1.67s\n",
            "Epoch 44, 52% \t train_loss: 0.30 took: 1.72s\n",
            "Epoch 44, 63% \t train_loss: 0.39 took: 1.75s\n",
            "Epoch 44, 73% \t train_loss: 0.30 took: 1.80s\n",
            "Epoch 44, 84% \t train_loss: 0.31 took: 1.71s\n",
            "Epoch 44, 94% \t train_loss: 0.29 took: 1.76s\n",
            "Validation loss = 0.56\n",
            "Epoch 45, 10% \t train_loss: 0.31 took: 1.95s\n",
            "Epoch 45, 21% \t train_loss: 0.34 took: 1.82s\n",
            "Epoch 45, 31% \t train_loss: 0.34 took: 1.76s\n",
            "Epoch 45, 42% \t train_loss: 0.31 took: 1.79s\n",
            "Epoch 45, 52% \t train_loss: 0.30 took: 1.72s\n",
            "Epoch 45, 63% \t train_loss: 0.37 took: 1.78s\n",
            "Epoch 45, 73% \t train_loss: 0.30 took: 1.70s\n",
            "Epoch 45, 84% \t train_loss: 0.31 took: 1.73s\n",
            "Epoch 45, 94% \t train_loss: 0.28 took: 1.74s\n",
            "Validation loss = 0.55\n",
            "Epoch 46, 10% \t train_loss: 0.30 took: 1.95s\n",
            "Epoch 46, 21% \t train_loss: 0.32 took: 1.82s\n",
            "Epoch 46, 31% \t train_loss: 0.33 took: 1.59s\n",
            "Epoch 46, 42% \t train_loss: 0.33 took: 1.79s\n",
            "Epoch 46, 52% \t train_loss: 0.32 took: 1.80s\n",
            "Epoch 46, 63% \t train_loss: 0.37 took: 1.76s\n",
            "Epoch 46, 73% \t train_loss: 0.30 took: 1.76s\n",
            "Epoch 46, 84% \t train_loss: 0.31 took: 1.77s\n",
            "Epoch 46, 94% \t train_loss: 0.31 took: 1.70s\n",
            "Validation loss = 0.59\n",
            "Epoch 47, 10% \t train_loss: 0.30 took: 2.00s\n",
            "Epoch 47, 21% \t train_loss: 0.32 took: 1.75s\n",
            "Epoch 47, 31% \t train_loss: 0.35 took: 1.79s\n",
            "Epoch 47, 42% \t train_loss: 0.32 took: 1.74s\n",
            "Epoch 47, 52% \t train_loss: 0.29 took: 1.74s\n",
            "Epoch 47, 63% \t train_loss: 0.37 took: 1.74s\n",
            "Epoch 47, 73% \t train_loss: 0.28 took: 1.74s\n",
            "Epoch 47, 84% \t train_loss: 0.31 took: 1.78s\n",
            "Epoch 47, 94% \t train_loss: 0.28 took: 1.74s\n",
            "Validation loss = 0.58\n",
            "Epoch 48, 10% \t train_loss: 0.32 took: 1.97s\n",
            "Epoch 48, 21% \t train_loss: 0.31 took: 1.73s\n",
            "Epoch 48, 31% \t train_loss: 0.33 took: 1.75s\n",
            "Epoch 48, 42% \t train_loss: 0.31 took: 1.74s\n",
            "Epoch 48, 52% \t train_loss: 0.30 took: 1.72s\n",
            "Epoch 48, 63% \t train_loss: 0.38 took: 1.83s\n",
            "Epoch 48, 73% \t train_loss: 0.29 took: 1.69s\n",
            "Epoch 48, 84% \t train_loss: 0.32 took: 1.72s\n",
            "Epoch 48, 94% \t train_loss: 0.30 took: 1.78s\n",
            "Validation loss = 0.62\n",
            "Epoch 49, 10% \t train_loss: 0.30 took: 1.93s\n",
            "Epoch 49, 21% \t train_loss: 0.32 took: 1.74s\n",
            "Epoch 49, 31% \t train_loss: 0.32 took: 1.73s\n",
            "Epoch 49, 42% \t train_loss: 0.32 took: 1.75s\n",
            "Epoch 49, 52% \t train_loss: 0.29 took: 1.76s\n",
            "Epoch 49, 63% \t train_loss: 0.35 took: 1.76s\n",
            "Epoch 49, 73% \t train_loss: 0.30 took: 1.79s\n",
            "Epoch 49, 84% \t train_loss: 0.33 took: 1.68s\n",
            "Epoch 49, 94% \t train_loss: 0.28 took: 1.68s\n",
            "Validation loss = 0.59\n",
            "Epoch 50, 10% \t train_loss: 0.28 took: 1.94s\n",
            "Epoch 50, 21% \t train_loss: 0.32 took: 1.72s\n",
            "Epoch 50, 31% \t train_loss: 0.30 took: 1.76s\n",
            "Epoch 50, 42% \t train_loss: 0.29 took: 1.75s\n",
            "Epoch 50, 52% \t train_loss: 0.30 took: 1.71s\n",
            "Epoch 50, 63% \t train_loss: 0.35 took: 1.71s\n",
            "Epoch 50, 73% \t train_loss: 0.28 took: 1.71s\n",
            "Epoch 50, 84% \t train_loss: 0.32 took: 1.76s\n",
            "Epoch 50, 94% \t train_loss: 0.29 took: 1.76s\n",
            "Validation loss = 0.57\n",
            "Training Finished, took 1799.82s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AxPOijFvRIj",
        "colab_type": "code",
        "outputId": "790c9979-c1c1-48ca-c786-3133942eb2a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "plot_losses(train_history, val_history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfEAAAGDCAYAAAA72Cm3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTNLttd5vT2S",
        "colab_type": "code",
        "outputId": "11502da5-6471-4c6b-d7d3-fa98f5256b45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "net.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): LambdaLayer()\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): LambdaLayer()\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=64, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZcb1aM6vVZU",
        "colab_type": "code",
        "outputId": "a7f743a2-8f91-45e9-cad4-48348be06f6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "def dataset_accuracy(net, data_loader, name=\"\"):\n",
        "    net = net.to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in data_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum()\n",
        "    accuracy = 100 * float(correct) / total\n",
        "    print('Accuracy of the network on the {} {} images: {:.2f} %'.format(total, name, accuracy))\n",
        "\n",
        "def train_set_accuracy(net):\n",
        "    dataset_accuracy(net, train_loader, \"train\")\n",
        "\n",
        "def val_set_accuracy(net):\n",
        "    dataset_accuracy(net, val_loader, \"validation\")  \n",
        "    \n",
        "def test_set_accuracy(net):\n",
        "    dataset_accuracy(net, test_loader, \"test\")\n",
        "\n",
        "def compute_accuracy(net):\n",
        "    train_set_accuracy(net)\n",
        "    val_set_accuracy(net)\n",
        "    test_set_accuracy(net)\n",
        "    \n",
        "print(\"Computing accuracy...\")\n",
        "compute_accuracy(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computing accuracy...\n",
            "Accuracy of the network on the 4238 train images: 88.44 %\n",
            "Accuracy of the network on the 533 validation images: 77.30 %\n",
            "Accuracy of the network on the 527 test images: 80.27 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}