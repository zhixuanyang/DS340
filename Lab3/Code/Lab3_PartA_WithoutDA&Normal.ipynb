{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab3-PartA-WithoutDA&Normal.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hiCp9PbwCoW",
        "colab_type": "code",
        "outputId": "8f0fa141-6be4-4264-9769-00475d9f1cd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "%matplotlib inline\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from __future__ import print_function, division\n",
        "import itertools\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import numpy as np\n",
        "import torch.nn.init as init\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3XSZcdmAefM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "colors = [[31, 120, 180], [51, 160, 44]]\n",
        "colors = [(r / 255, g / 255, b / 255) for (r, g, b) in colors]\n",
        "\n",
        "\n",
        "def plot_losses(train_history, val_history):\n",
        "    x = np.arange(1, len(train_history) + 1)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, train_history, color=colors[0], label=\"Training loss\", linewidth=2)\n",
        "    plt.plot(x, val_history, color=colors[1], label=\"Validation loss\", linewidth=2)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title(\"Evolution of the training and validation loss\")\n",
        "    plt.show()\n",
        "seed = 5678\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.cuda.manual_seed(seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_HKF9C50BRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "n_training_samples = 45000\n",
        "n_val_samples = 5000\n",
        "n_test_samples = 10000\n",
        "train_sampler = SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
        "val_sampler = SubsetRandomSampler(np.arange(n_training_samples, n_training_samples + n_val_samples, dtype=np.int64))\n",
        "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQr3tsszOgut",
        "colab_type": "code",
        "outputId": "ba8130e2-1506-46d4-f647-fd2326e16340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "num_workers = 2\n",
        "test_batch_size = 4\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root='/content/drive/My Drive/Colab Notebooks', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=test_batch_size, sampler=train_sampler,\n",
        "                                          num_workers=num_workers)\n",
        "\n",
        "test_set = torchvision.datasets.CIFAR10(root='/content/drive/My Drive/Colab Notebooks', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=test_batch_size, sampler=test_sampler,\n",
        "                                         num_workers=num_workers)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC16BvrGQpI7",
        "colab_type": "code",
        "outputId": "44f36cda-22ed-43d2-e789-aa0a504faa4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('the number of training samples is',n_training_samples)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the number of training samples is 45000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5dPo39fQsVV",
        "colab_type": "code",
        "outputId": "4d75b40d-ae85-4801-d31b-3b1304da0397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('the number of validation samples is',n_val_samples)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the number of validation samples is 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q2Vi6wSXBA6",
        "colab_type": "code",
        "outputId": "506045ef-7d45-45a3-968c-6ca99b01fab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('the number of testing samples is',n_test_samples)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the number of testing samples is 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnj7yIzd_KY5",
        "colab_type": "code",
        "outputId": "764b58c6-ce8f-42b2-b7b1-8bd8d74c2fe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def _weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            if option == 'A':\n",
        "                \"\"\"\n",
        "                For CIFAR10 ResNet paper uses option A.\n",
        "                \"\"\"\n",
        "                self.shortcut = LambdaLayer(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "            elif option == 'B':\n",
        "                self.shortcut = nn.Sequential(\n",
        "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                     nn.BatchNorm2d(self.expansion * planes)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "net = ResNet(BasicBlock, [3, 3, 3])\n",
        "print(net)\n",
        "\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): LambdaLayer()\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): LambdaLayer()\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Np7CRd4BnsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createLoss():\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    return criterion\n",
        "def createOptimizer(net):  \n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
        "    return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1n39z-zC_LN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_train_loader(batch_size):\n",
        "    return torch.utils.data.DataLoader(train_set, batch_size=batch_size, sampler=train_sampler,\n",
        "                                              num_workers=num_workers)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(train_set, batch_size=128, sampler=val_sampler,\n",
        "                                          num_workers=num_workers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM5Ll7h9eo7z",
        "colab_type": "code",
        "outputId": "c6c47044-0671-4212-f2a0-e82eef6d8926",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "val_loader"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7fa42b662320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLQUvof6knI6",
        "colab_type": "code",
        "outputId": "2194513a-9c12-494a-ba6e-438a04e8d43f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def train(net, batch_size, n_epochs, learning_rate):\n",
        "    \"\"\"\n",
        "    Train a neural network and print statistics of the training\n",
        "    \n",
        "    :param net: (PyTorch Neural Network)\n",
        "    :param batch_size: (int)\n",
        "    :param n_epochs: (int)  Number of iterations on the training set\n",
        "    :param learning_rate: (float) learning rate used by the optimizer\n",
        "    \"\"\"\n",
        "    print(\"===== HYPERPARAMETERS =====\")\n",
        "    print(\"batch_size=\", batch_size)\n",
        "    print(\"n_epochs=\", n_epochs)\n",
        "    print(\"learning_rate=\", learning_rate)\n",
        "    print(\"=\" * 30)\n",
        "    \n",
        "    train_loader = get_train_loader(batch_size)\n",
        "    n_minibatches = len(train_loader)\n",
        "    train_history = []\n",
        "    val_history = []\n",
        "    criterion = createLoss()\n",
        "    optimizer = createOptimizer(net)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,40], gamma=0.1)\n",
        "    training_start_time = time.time()\n",
        "    best_error = np.inf\n",
        "    best_model_path = \"/content/drive/My Drive/Colab Notebooks/best_model.pth\"\n",
        "    \n",
        "    net = net.to(device)\n",
        "\n",
        "    for epoch in range(n_epochs):  \n",
        "        running_loss = 0.0\n",
        "        print_every = n_minibatches // 10\n",
        "        start_time = time.time()\n",
        "        total_train_loss = 0\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            total_train_loss += loss.item()\n",
        "            if (i + 1) % (print_every + 1) == 0:    \n",
        "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
        "                      epoch + 1, int(100 * (i + 1) / n_minibatches), running_loss / print_every,\n",
        "                      time.time() - start_time))\n",
        "                running_loss = 0.0\n",
        "                start_time = time.time()\n",
        "        train_history.append(total_train_loss / len(train_loader))\n",
        "        \n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "          for inputs, labels in val_loader:\n",
        "              inputs, labels = inputs.to(device), labels.to(device)\n",
        "              predictions = net(inputs)\n",
        "              val_loss = criterion(predictions, labels)\n",
        "              total_val_loss += val_loss.item()\n",
        "        val_history.append(total_val_loss / len(val_loader))\n",
        "        if total_val_loss < best_error:\n",
        "            best_error = total_val_loss\n",
        "            torch.save(net.state_dict(), best_model_path)\n",
        "        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(val_loader)))\n",
        "        scheduler.step()\n",
        "    print(\"Training Finished, took {:.2f}s\".format(time.time() - training_start_time))\n",
        "    \n",
        "    net.load_state_dict(torch.load(best_model_path))\n",
        "    \n",
        "    return train_history, val_history\n",
        "\n",
        "train_history, val_history = train(net, batch_size=32, n_epochs=50, learning_rate=0.1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== HYPERPARAMETERS =====\n",
            "batch_size= 32\n",
            "n_epochs= 50\n",
            "learning_rate= 0.1\n",
            "==============================\n",
            "Epoch 1, 10% \t train_loss: 2.18 took: 2.48s\n",
            "Epoch 1, 20% \t train_loss: 1.80 took: 2.21s\n",
            "Epoch 1, 30% \t train_loss: 1.67 took: 2.20s\n",
            "Epoch 1, 40% \t train_loss: 1.64 took: 2.21s\n",
            "Epoch 1, 50% \t train_loss: 1.53 took: 2.22s\n",
            "Epoch 1, 60% \t train_loss: 1.46 took: 2.22s\n",
            "Epoch 1, 70% \t train_loss: 1.41 took: 2.20s\n",
            "Epoch 1, 80% \t train_loss: 1.37 took: 2.19s\n",
            "Epoch 1, 90% \t train_loss: 1.31 took: 2.23s\n",
            "Validation loss = 1.15\n",
            "Epoch 2, 10% \t train_loss: 1.16 took: 2.27s\n",
            "Epoch 2, 20% \t train_loss: 1.13 took: 2.20s\n",
            "Epoch 2, 30% \t train_loss: 1.10 took: 2.20s\n",
            "Epoch 2, 40% \t train_loss: 1.06 took: 2.22s\n",
            "Epoch 2, 50% \t train_loss: 1.05 took: 2.24s\n",
            "Epoch 2, 60% \t train_loss: 1.03 took: 2.22s\n",
            "Epoch 2, 70% \t train_loss: 1.01 took: 2.21s\n",
            "Epoch 2, 80% \t train_loss: 0.97 took: 2.21s\n",
            "Epoch 2, 90% \t train_loss: 0.94 took: 2.21s\n",
            "Validation loss = 0.86\n",
            "Epoch 3, 10% \t train_loss: 0.83 took: 2.26s\n",
            "Epoch 3, 20% \t train_loss: 0.89 took: 2.22s\n",
            "Epoch 3, 30% \t train_loss: 0.86 took: 2.21s\n",
            "Epoch 3, 40% \t train_loss: 0.86 took: 2.23s\n",
            "Epoch 3, 50% \t train_loss: 0.83 took: 2.21s\n",
            "Epoch 3, 60% \t train_loss: 0.81 took: 2.22s\n",
            "Epoch 3, 70% \t train_loss: 0.80 took: 2.22s\n",
            "Epoch 3, 80% \t train_loss: 0.77 took: 2.20s\n",
            "Epoch 3, 90% \t train_loss: 0.76 took: 2.21s\n",
            "Validation loss = 0.72\n",
            "Epoch 4, 10% \t train_loss: 0.68 took: 2.27s\n",
            "Epoch 4, 20% \t train_loss: 0.73 took: 2.20s\n",
            "Epoch 4, 30% \t train_loss: 0.66 took: 2.22s\n",
            "Epoch 4, 40% \t train_loss: 0.67 took: 2.20s\n",
            "Epoch 4, 50% \t train_loss: 0.70 took: 2.22s\n",
            "Epoch 4, 60% \t train_loss: 0.66 took: 2.21s\n",
            "Epoch 4, 70% \t train_loss: 0.67 took: 2.21s\n",
            "Epoch 4, 80% \t train_loss: 0.66 took: 2.20s\n",
            "Epoch 4, 90% \t train_loss: 0.69 took: 2.19s\n",
            "Validation loss = 0.64\n",
            "Epoch 5, 10% \t train_loss: 0.58 took: 2.31s\n",
            "Epoch 5, 20% \t train_loss: 0.58 took: 2.26s\n",
            "Epoch 5, 30% \t train_loss: 0.58 took: 2.25s\n",
            "Epoch 5, 40% \t train_loss: 0.61 took: 2.26s\n",
            "Epoch 5, 50% \t train_loss: 0.60 took: 2.23s\n",
            "Epoch 5, 60% \t train_loss: 0.57 took: 2.24s\n",
            "Epoch 5, 70% \t train_loss: 0.58 took: 2.22s\n",
            "Epoch 5, 80% \t train_loss: 0.61 took: 2.20s\n",
            "Epoch 5, 90% \t train_loss: 0.61 took: 2.20s\n",
            "Validation loss = 0.61\n",
            "Epoch 6, 10% \t train_loss: 0.49 took: 2.27s\n",
            "Epoch 6, 20% \t train_loss: 0.54 took: 2.19s\n",
            "Epoch 6, 30% \t train_loss: 0.51 took: 2.22s\n",
            "Epoch 6, 40% \t train_loss: 0.48 took: 2.21s\n",
            "Epoch 6, 50% \t train_loss: 0.51 took: 2.23s\n",
            "Epoch 6, 60% \t train_loss: 0.53 took: 2.26s\n",
            "Epoch 6, 70% \t train_loss: 0.54 took: 2.24s\n",
            "Epoch 6, 80% \t train_loss: 0.50 took: 2.26s\n",
            "Epoch 6, 90% \t train_loss: 0.52 took: 2.22s\n",
            "Validation loss = 0.60\n",
            "Epoch 7, 10% \t train_loss: 0.41 took: 2.28s\n",
            "Epoch 7, 20% \t train_loss: 0.43 took: 2.21s\n",
            "Epoch 7, 30% \t train_loss: 0.43 took: 2.22s\n",
            "Epoch 7, 40% \t train_loss: 0.45 took: 2.23s\n",
            "Epoch 7, 50% \t train_loss: 0.47 took: 2.21s\n",
            "Epoch 7, 60% \t train_loss: 0.44 took: 2.23s\n",
            "Epoch 7, 70% \t train_loss: 0.46 took: 2.22s\n",
            "Epoch 7, 80% \t train_loss: 0.47 took: 2.23s\n",
            "Epoch 7, 90% \t train_loss: 0.45 took: 2.21s\n",
            "Validation loss = 0.54\n",
            "Epoch 8, 10% \t train_loss: 0.36 took: 2.28s\n",
            "Epoch 8, 20% \t train_loss: 0.37 took: 2.22s\n",
            "Epoch 8, 30% \t train_loss: 0.39 took: 2.21s\n",
            "Epoch 8, 40% \t train_loss: 0.39 took: 2.21s\n",
            "Epoch 8, 50% \t train_loss: 0.41 took: 2.20s\n",
            "Epoch 8, 60% \t train_loss: 0.39 took: 2.21s\n",
            "Epoch 8, 70% \t train_loss: 0.40 took: 2.23s\n",
            "Epoch 8, 80% \t train_loss: 0.44 took: 2.22s\n",
            "Epoch 8, 90% \t train_loss: 0.41 took: 2.23s\n",
            "Validation loss = 0.56\n",
            "Epoch 9, 10% \t train_loss: 0.31 took: 2.28s\n",
            "Epoch 9, 20% \t train_loss: 0.34 took: 2.22s\n",
            "Epoch 9, 30% \t train_loss: 0.33 took: 2.20s\n",
            "Epoch 9, 40% \t train_loss: 0.34 took: 2.25s\n",
            "Epoch 9, 50% \t train_loss: 0.35 took: 2.22s\n",
            "Epoch 9, 60% \t train_loss: 0.34 took: 2.22s\n",
            "Epoch 9, 70% \t train_loss: 0.36 took: 2.20s\n",
            "Epoch 9, 80% \t train_loss: 0.34 took: 2.22s\n",
            "Epoch 9, 90% \t train_loss: 0.39 took: 2.21s\n",
            "Validation loss = 0.59\n",
            "Epoch 10, 10% \t train_loss: 0.27 took: 2.27s\n",
            "Epoch 10, 20% \t train_loss: 0.26 took: 2.22s\n",
            "Epoch 10, 30% \t train_loss: 0.30 took: 2.24s\n",
            "Epoch 10, 40% \t train_loss: 0.30 took: 2.22s\n",
            "Epoch 10, 50% \t train_loss: 0.30 took: 2.23s\n",
            "Epoch 10, 60% \t train_loss: 0.32 took: 2.23s\n",
            "Epoch 10, 70% \t train_loss: 0.31 took: 2.23s\n",
            "Epoch 10, 80% \t train_loss: 0.33 took: 2.24s\n",
            "Epoch 10, 90% \t train_loss: 0.32 took: 2.23s\n",
            "Validation loss = 0.61\n",
            "Epoch 11, 10% \t train_loss: 0.24 took: 2.29s\n",
            "Epoch 11, 20% \t train_loss: 0.24 took: 2.22s\n",
            "Epoch 11, 30% \t train_loss: 0.23 took: 2.20s\n",
            "Epoch 11, 40% \t train_loss: 0.26 took: 2.21s\n",
            "Epoch 11, 50% \t train_loss: 0.28 took: 2.24s\n",
            "Epoch 11, 60% \t train_loss: 0.28 took: 2.24s\n",
            "Epoch 11, 70% \t train_loss: 0.27 took: 2.22s\n",
            "Epoch 11, 80% \t train_loss: 0.29 took: 2.21s\n",
            "Epoch 11, 90% \t train_loss: 0.29 took: 2.22s\n",
            "Validation loss = 0.57\n",
            "Epoch 12, 20% \t train_loss: 0.21 took: 2.20s\n",
            "Epoch 12, 30% \t train_loss: 0.23 took: 2.20s\n",
            "Epoch 12, 40% \t train_loss: 0.23 took: 2.23s\n",
            "Epoch 12, 50% \t train_loss: 0.24 took: 2.20s\n",
            "Epoch 12, 60% \t train_loss: 0.24 took: 2.22s\n",
            "Epoch 12, 70% \t train_loss: 0.24 took: 2.22s\n",
            "Epoch 12, 80% \t train_loss: 0.26 took: 2.20s\n",
            "Epoch 12, 90% \t train_loss: 0.28 took: 2.20s\n",
            "Validation loss = 0.62\n",
            "Epoch 13, 10% \t train_loss: 0.19 took: 2.27s\n",
            "Epoch 13, 20% \t train_loss: 0.16 took: 2.20s\n",
            "Epoch 13, 30% \t train_loss: 0.21 took: 2.21s\n",
            "Epoch 13, 40% \t train_loss: 0.18 took: 2.22s\n",
            "Epoch 13, 50% \t train_loss: 0.20 took: 2.20s\n",
            "Epoch 13, 60% \t train_loss: 0.22 took: 2.21s\n",
            "Epoch 13, 70% \t train_loss: 0.20 took: 2.23s\n",
            "Epoch 13, 80% \t train_loss: 0.23 took: 2.22s\n",
            "Epoch 13, 90% \t train_loss: 0.23 took: 2.22s\n",
            "Validation loss = 0.64\n",
            "Epoch 14, 10% \t train_loss: 0.15 took: 2.31s\n",
            "Epoch 14, 20% \t train_loss: 0.16 took: 2.21s\n",
            "Epoch 14, 30% \t train_loss: 0.16 took: 2.21s\n",
            "Epoch 14, 40% \t train_loss: 0.16 took: 2.21s\n",
            "Epoch 14, 50% \t train_loss: 0.18 took: 2.21s\n",
            "Epoch 14, 60% \t train_loss: 0.20 took: 2.21s\n",
            "Epoch 14, 70% \t train_loss: 0.21 took: 2.24s\n",
            "Epoch 14, 80% \t train_loss: 0.21 took: 2.21s\n",
            "Epoch 14, 90% \t train_loss: 0.20 took: 2.21s\n",
            "Validation loss = 0.65\n",
            "Epoch 15, 10% \t train_loss: 0.15 took: 2.29s\n",
            "Epoch 15, 20% \t train_loss: 0.10 took: 2.20s\n",
            "Epoch 15, 30% \t train_loss: 0.13 took: 2.20s\n",
            "Epoch 15, 40% \t train_loss: 0.14 took: 2.20s\n",
            "Epoch 15, 50% \t train_loss: 0.15 took: 2.22s\n",
            "Epoch 15, 60% \t train_loss: 0.16 took: 2.21s\n",
            "Epoch 15, 70% \t train_loss: 0.17 took: 2.23s\n",
            "Epoch 15, 80% \t train_loss: 0.17 took: 2.24s\n",
            "Epoch 15, 90% \t train_loss: 0.19 took: 2.22s\n",
            "Validation loss = 0.69\n",
            "Epoch 16, 10% \t train_loss: 0.11 took: 2.27s\n",
            "Epoch 16, 20% \t train_loss: 0.13 took: 2.22s\n",
            "Epoch 16, 30% \t train_loss: 0.11 took: 2.21s\n",
            "Epoch 16, 40% \t train_loss: 0.14 took: 2.21s\n",
            "Epoch 16, 50% \t train_loss: 0.14 took: 2.22s\n",
            "Epoch 16, 60% \t train_loss: 0.16 took: 2.21s\n",
            "Epoch 16, 70% \t train_loss: 0.16 took: 2.23s\n",
            "Epoch 16, 80% \t train_loss: 0.14 took: 2.22s\n",
            "Epoch 16, 90% \t train_loss: 0.16 took: 2.21s\n",
            "Validation loss = 0.71\n",
            "Epoch 17, 10% \t train_loss: 0.10 took: 2.27s\n",
            "Epoch 17, 20% \t train_loss: 0.09 took: 2.22s\n",
            "Epoch 17, 30% \t train_loss: 0.10 took: 2.26s\n",
            "Epoch 17, 40% \t train_loss: 0.13 took: 2.23s\n",
            "Epoch 17, 50% \t train_loss: 0.13 took: 2.23s\n",
            "Epoch 17, 60% \t train_loss: 0.12 took: 2.24s\n",
            "Epoch 17, 70% \t train_loss: 0.14 took: 2.20s\n",
            "Epoch 17, 80% \t train_loss: 0.13 took: 2.20s\n",
            "Epoch 17, 90% \t train_loss: 0.15 took: 2.22s\n",
            "Validation loss = 0.72\n",
            "Epoch 18, 10% \t train_loss: 0.07 took: 2.29s\n",
            "Epoch 18, 20% \t train_loss: 0.08 took: 2.20s\n",
            "Epoch 18, 30% \t train_loss: 0.09 took: 2.22s\n",
            "Epoch 18, 40% \t train_loss: 0.09 took: 2.20s\n",
            "Epoch 18, 50% \t train_loss: 0.10 took: 2.21s\n",
            "Epoch 18, 60% \t train_loss: 0.10 took: 2.25s\n",
            "Epoch 18, 70% \t train_loss: 0.13 took: 2.27s\n",
            "Epoch 18, 80% \t train_loss: 0.12 took: 2.25s\n",
            "Epoch 18, 90% \t train_loss: 0.13 took: 2.26s\n",
            "Validation loss = 0.71\n",
            "Epoch 19, 10% \t train_loss: 0.07 took: 2.28s\n",
            "Epoch 19, 20% \t train_loss: 0.09 took: 2.21s\n",
            "Epoch 19, 30% \t train_loss: 0.07 took: 2.21s\n",
            "Epoch 19, 40% \t train_loss: 0.09 took: 2.22s\n",
            "Epoch 19, 50% \t train_loss: 0.10 took: 2.21s\n",
            "Epoch 19, 60% \t train_loss: 0.08 took: 2.21s\n",
            "Epoch 19, 70% \t train_loss: 0.09 took: 2.24s\n",
            "Epoch 19, 80% \t train_loss: 0.09 took: 2.21s\n",
            "Epoch 19, 90% \t train_loss: 0.10 took: 2.30s\n",
            "Validation loss = 0.83\n",
            "Epoch 20, 10% \t train_loss: 0.11 took: 2.28s\n",
            "Epoch 20, 20% \t train_loss: 0.07 took: 2.22s\n",
            "Epoch 20, 30% \t train_loss: 0.08 took: 2.22s\n",
            "Epoch 20, 40% \t train_loss: 0.09 took: 2.24s\n",
            "Epoch 20, 50% \t train_loss: 0.09 took: 2.28s\n",
            "Epoch 20, 60% \t train_loss: 0.10 took: 2.45s\n",
            "Epoch 20, 70% \t train_loss: 0.08 took: 2.29s\n",
            "Epoch 20, 80% \t train_loss: 0.10 took: 2.24s\n",
            "Epoch 20, 90% \t train_loss: 0.10 took: 2.33s\n",
            "Validation loss = 0.82\n",
            "Epoch 21, 10% \t train_loss: 0.07 took: 2.36s\n",
            "Epoch 21, 20% \t train_loss: 0.06 took: 2.22s\n",
            "Epoch 21, 30% \t train_loss: 0.06 took: 2.29s\n",
            "Epoch 21, 40% \t train_loss: 0.07 took: 2.30s\n",
            "Epoch 21, 50% \t train_loss: 0.09 took: 2.22s\n",
            "Epoch 21, 60% \t train_loss: 0.08 took: 2.23s\n",
            "Epoch 21, 70% \t train_loss: 0.08 took: 2.25s\n",
            "Epoch 21, 80% \t train_loss: 0.07 took: 2.24s\n",
            "Epoch 21, 90% \t train_loss: 0.08 took: 2.24s\n",
            "Validation loss = 0.80\n",
            "Epoch 22, 10% \t train_loss: 0.06 took: 2.33s\n",
            "Epoch 22, 20% \t train_loss: 0.06 took: 2.23s\n",
            "Epoch 22, 30% \t train_loss: 0.06 took: 2.30s\n",
            "Epoch 22, 40% \t train_loss: 0.04 took: 2.29s\n",
            "Epoch 22, 50% \t train_loss: 0.05 took: 2.26s\n",
            "Epoch 22, 60% \t train_loss: 0.06 took: 2.25s\n",
            "Epoch 22, 70% \t train_loss: 0.06 took: 2.32s\n",
            "Epoch 22, 80% \t train_loss: 0.06 took: 2.35s\n",
            "Epoch 22, 90% \t train_loss: 0.07 took: 2.34s\n",
            "Validation loss = 0.86\n",
            "Epoch 23, 10% \t train_loss: 0.07 took: 2.35s\n",
            "Epoch 23, 20% \t train_loss: 0.05 took: 2.27s\n",
            "Epoch 23, 30% \t train_loss: 0.06 took: 2.22s\n",
            "Epoch 23, 40% \t train_loss: 0.05 took: 2.21s\n",
            "Epoch 23, 50% \t train_loss: 0.06 took: 2.24s\n",
            "Epoch 23, 60% \t train_loss: 0.05 took: 2.25s\n",
            "Epoch 23, 70% \t train_loss: 0.06 took: 2.23s\n",
            "Epoch 23, 80% \t train_loss: 0.07 took: 2.25s\n",
            "Epoch 23, 90% \t train_loss: 0.07 took: 2.30s\n",
            "Validation loss = 0.86\n",
            "Epoch 24, 10% \t train_loss: 0.05 took: 2.32s\n",
            "Epoch 24, 20% \t train_loss: 0.04 took: 2.24s\n",
            "Epoch 24, 30% \t train_loss: 0.06 took: 2.23s\n",
            "Epoch 24, 40% \t train_loss: 0.06 took: 2.26s\n",
            "Epoch 24, 50% \t train_loss: 0.06 took: 2.25s\n",
            "Epoch 24, 60% \t train_loss: 0.07 took: 2.22s\n",
            "Epoch 24, 70% \t train_loss: 0.05 took: 2.21s\n",
            "Epoch 24, 80% \t train_loss: 0.07 took: 2.22s\n",
            "Epoch 24, 90% \t train_loss: 0.07 took: 2.25s\n",
            "Validation loss = 0.85\n",
            "Epoch 25, 20% \t train_loss: 0.05 took: 2.25s\n",
            "Epoch 25, 30% \t train_loss: 0.04 took: 2.32s\n",
            "Epoch 25, 40% \t train_loss: 0.04 took: 2.32s\n",
            "Epoch 25, 50% \t train_loss: 0.04 took: 2.32s\n",
            "Epoch 25, 60% \t train_loss: 0.04 took: 2.31s\n",
            "Epoch 25, 70% \t train_loss: 0.04 took: 2.31s\n",
            "Epoch 25, 80% \t train_loss: 0.06 took: 2.30s\n",
            "Epoch 25, 90% \t train_loss: 0.08 took: 2.29s\n",
            "Validation loss = 0.95\n",
            "Epoch 26, 10% \t train_loss: 0.06 took: 2.35s\n",
            "Epoch 26, 20% \t train_loss: 0.06 took: 2.30s\n",
            "Epoch 26, 30% \t train_loss: 0.06 took: 2.23s\n",
            "Epoch 26, 40% \t train_loss: 0.04 took: 2.25s\n",
            "Epoch 26, 50% \t train_loss: 0.06 took: 2.26s\n",
            "Epoch 26, 60% \t train_loss: 0.07 took: 2.27s\n",
            "Epoch 26, 70% \t train_loss: 0.05 took: 2.26s\n",
            "Epoch 26, 80% \t train_loss: 0.07 took: 2.29s\n",
            "Epoch 26, 90% \t train_loss: 0.06 took: 2.26s\n",
            "Validation loss = 0.93\n",
            "Epoch 27, 10% \t train_loss: 0.03 took: 2.31s\n",
            "Epoch 27, 20% \t train_loss: 0.03 took: 2.23s\n",
            "Epoch 27, 30% \t train_loss: 0.03 took: 2.23s\n",
            "Epoch 27, 40% \t train_loss: 0.04 took: 2.25s\n",
            "Epoch 27, 50% \t train_loss: 0.05 took: 2.26s\n",
            "Epoch 27, 60% \t train_loss: 0.04 took: 2.28s\n",
            "Epoch 27, 70% \t train_loss: 0.04 took: 2.26s\n",
            "Epoch 27, 80% \t train_loss: 0.04 took: 2.28s\n",
            "Epoch 27, 90% \t train_loss: 0.05 took: 2.30s\n",
            "Validation loss = 0.88\n",
            "Epoch 28, 10% \t train_loss: 0.04 took: 2.32s\n",
            "Epoch 28, 20% \t train_loss: 0.03 took: 2.26s\n",
            "Epoch 28, 30% \t train_loss: 0.03 took: 2.27s\n",
            "Epoch 28, 40% \t train_loss: 0.02 took: 2.25s\n",
            "Epoch 28, 50% \t train_loss: 0.03 took: 2.22s\n",
            "Epoch 28, 60% \t train_loss: 0.04 took: 2.22s\n",
            "Epoch 28, 70% \t train_loss: 0.05 took: 2.22s\n",
            "Epoch 28, 80% \t train_loss: 0.05 took: 2.21s\n",
            "Epoch 28, 90% \t train_loss: 0.06 took: 2.26s\n",
            "Validation loss = 0.93\n",
            "Epoch 29, 10% \t train_loss: 0.04 took: 2.34s\n",
            "Epoch 29, 20% \t train_loss: 0.03 took: 2.23s\n",
            "Epoch 29, 30% \t train_loss: 0.03 took: 2.22s\n",
            "Epoch 29, 40% \t train_loss: 0.03 took: 2.20s\n",
            "Epoch 29, 50% \t train_loss: 0.03 took: 2.20s\n",
            "Epoch 29, 60% \t train_loss: 0.03 took: 2.22s\n",
            "Epoch 29, 70% \t train_loss: 0.04 took: 2.23s\n",
            "Epoch 29, 80% \t train_loss: 0.05 took: 2.23s\n",
            "Epoch 29, 90% \t train_loss: 0.03 took: 2.21s\n",
            "Validation loss = 0.92\n",
            "Epoch 30, 10% \t train_loss: 0.04 took: 2.27s\n",
            "Epoch 30, 20% \t train_loss: 0.03 took: 2.23s\n",
            "Epoch 30, 30% \t train_loss: 0.03 took: 2.21s\n",
            "Epoch 30, 40% \t train_loss: 0.02 took: 2.23s\n",
            "Epoch 30, 50% \t train_loss: 0.03 took: 2.23s\n",
            "Epoch 30, 60% \t train_loss: 0.04 took: 2.21s\n",
            "Epoch 30, 70% \t train_loss: 0.03 took: 2.20s\n",
            "Epoch 30, 80% \t train_loss: 0.02 took: 2.21s\n",
            "Epoch 30, 90% \t train_loss: 0.03 took: 2.24s\n",
            "Validation loss = 0.93\n",
            "Epoch 31, 10% \t train_loss: 0.02 took: 2.29s\n",
            "Epoch 31, 20% \t train_loss: 0.01 took: 2.23s\n",
            "Epoch 31, 30% \t train_loss: 0.01 took: 2.25s\n",
            "Epoch 31, 40% \t train_loss: 0.01 took: 2.24s\n",
            "Epoch 31, 50% \t train_loss: 0.01 took: 2.25s\n",
            "Epoch 31, 60% \t train_loss: 0.01 took: 2.21s\n",
            "Epoch 31, 70% \t train_loss: 0.01 took: 2.21s\n",
            "Epoch 31, 80% \t train_loss: 0.01 took: 2.24s\n",
            "Epoch 31, 90% \t train_loss: 0.01 took: 2.21s\n",
            "Validation loss = 0.89\n",
            "Epoch 32, 10% \t train_loss: 0.01 took: 2.31s\n",
            "Epoch 32, 20% \t train_loss: 0.00 took: 2.27s\n",
            "Epoch 32, 30% \t train_loss: 0.01 took: 2.26s\n",
            "Epoch 32, 40% \t train_loss: 0.01 took: 2.23s\n",
            "Epoch 32, 50% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 32, 60% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 32, 70% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 32, 80% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 32, 90% \t train_loss: 0.00 took: 2.21s\n",
            "Validation loss = 0.88\n",
            "Epoch 33, 10% \t train_loss: 0.00 took: 2.34s\n",
            "Epoch 33, 20% \t train_loss: 0.00 took: 2.24s\n",
            "Epoch 33, 30% \t train_loss: 0.00 took: 2.20s\n",
            "Epoch 33, 40% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 33, 50% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 33, 60% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 33, 70% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 33, 80% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 33, 90% \t train_loss: 0.00 took: 2.21s\n",
            "Validation loss = 0.90\n",
            "Epoch 34, 10% \t train_loss: 0.00 took: 2.27s\n",
            "Epoch 34, 20% \t train_loss: 0.00 took: 2.20s\n",
            "Epoch 34, 30% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 34, 40% \t train_loss: 0.00 took: 2.24s\n",
            "Epoch 34, 50% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 34, 60% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 34, 70% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 34, 80% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 34, 90% \t train_loss: 0.00 took: 2.26s\n",
            "Validation loss = 0.92\n",
            "Epoch 35, 10% \t train_loss: 0.00 took: 2.28s\n",
            "Epoch 35, 20% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 35, 30% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 35, 40% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 35, 50% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 35, 60% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 35, 70% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 35, 80% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 35, 90% \t train_loss: 0.00 took: 2.20s\n",
            "Validation loss = 0.91\n",
            "Epoch 36, 10% \t train_loss: 0.00 took: 2.27s\n",
            "Epoch 36, 20% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 36, 30% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 36, 40% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 36, 50% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 36, 60% \t train_loss: 0.00 took: 2.24s\n",
            "Epoch 36, 70% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 36, 80% \t train_loss: 0.00 took: 2.20s\n",
            "Epoch 36, 90% \t train_loss: 0.00 took: 2.20s\n",
            "Validation loss = 0.90\n",
            "Epoch 37, 10% \t train_loss: 0.00 took: 2.30s\n",
            "Epoch 37, 20% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 37, 30% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 37, 40% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 37, 50% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 37, 60% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 37, 70% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 37, 80% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 37, 90% \t train_loss: 0.00 took: 2.23s\n",
            "Validation loss = 0.93\n",
            "Epoch 38, 10% \t train_loss: 0.00 took: 2.30s\n",
            "Epoch 38, 20% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 38, 30% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 38, 40% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 38, 50% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 38, 60% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 38, 70% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 38, 80% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 38, 90% \t train_loss: 0.00 took: 2.19s\n",
            "Validation loss = 0.90\n",
            "Epoch 39, 10% \t train_loss: 0.00 took: 2.28s\n",
            "Epoch 39, 20% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 39, 30% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 39, 40% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 39, 50% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 39, 60% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 39, 70% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 39, 80% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 39, 90% \t train_loss: 0.00 took: 2.21s\n",
            "Validation loss = 0.90\n",
            "Epoch 40, 10% \t train_loss: 0.00 took: 2.28s\n",
            "Epoch 40, 20% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 40, 30% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 40, 40% \t train_loss: 0.00 took: 2.20s\n",
            "Epoch 40, 50% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 40, 60% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 40, 70% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 40, 80% \t train_loss: 0.00 took: 2.20s\n",
            "Epoch 40, 90% \t train_loss: 0.00 took: 2.21s\n",
            "Validation loss = 0.93\n",
            "Epoch 41, 10% \t train_loss: 0.00 took: 2.28s\n",
            "Epoch 41, 20% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 41, 30% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 41, 40% \t train_loss: 0.00 took: 2.20s\n",
            "Epoch 41, 50% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 41, 60% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 41, 70% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 41, 80% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 41, 90% \t train_loss: 0.00 took: 2.23s\n",
            "Validation loss = 0.99\n",
            "Epoch 42, 10% \t train_loss: 0.00 took: 2.28s\n",
            "Epoch 42, 20% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 42, 30% \t train_loss: 0.00 took: 2.20s\n",
            "Epoch 42, 40% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 42, 50% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 42, 60% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 42, 70% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 42, 80% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 42, 90% \t train_loss: 0.00 took: 2.23s\n",
            "Validation loss = 0.94\n",
            "Epoch 43, 10% \t train_loss: 0.00 took: 2.29s\n",
            "Epoch 43, 20% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 43, 30% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 43, 40% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 43, 50% \t train_loss: 0.00 took: 2.24s\n",
            "Epoch 43, 60% \t train_loss: 0.00 took: 2.24s\n",
            "Epoch 43, 70% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 43, 80% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 43, 90% \t train_loss: 0.00 took: 2.20s\n",
            "Validation loss = 0.95\n",
            "Epoch 44, 10% \t train_loss: 0.00 took: 2.28s\n",
            "Epoch 44, 20% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 44, 30% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 44, 40% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 44, 50% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 44, 60% \t train_loss: 0.00 took: 2.19s\n",
            "Epoch 44, 70% \t train_loss: 0.00 took: 2.25s\n",
            "Epoch 44, 80% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 44, 90% \t train_loss: 0.00 took: 2.22s\n",
            "Validation loss = 0.94\n",
            "Epoch 45, 10% \t train_loss: 0.00 took: 2.29s\n",
            "Epoch 45, 20% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 45, 30% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 45, 40% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 45, 50% \t train_loss: 0.00 took: 2.27s\n",
            "Epoch 45, 60% \t train_loss: 0.00 took: 2.26s\n",
            "Epoch 45, 70% \t train_loss: 0.00 took: 2.27s\n",
            "Epoch 45, 80% \t train_loss: 0.00 took: 2.26s\n",
            "Epoch 45, 90% \t train_loss: 0.00 took: 2.29s\n",
            "Validation loss = 0.95\n",
            "Epoch 46, 10% \t train_loss: 0.00 took: 2.26s\n",
            "Epoch 46, 20% \t train_loss: 0.00 took: 2.20s\n",
            "Epoch 46, 30% \t train_loss: 0.00 took: 2.24s\n",
            "Epoch 46, 40% \t train_loss: 0.00 took: 2.26s\n",
            "Epoch 46, 50% \t train_loss: 0.00 took: 2.27s\n",
            "Epoch 46, 60% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 46, 70% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 46, 80% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 46, 90% \t train_loss: 0.00 took: 2.20s\n",
            "Validation loss = 0.91\n",
            "Epoch 47, 10% \t train_loss: 0.00 took: 2.26s\n",
            "Epoch 47, 20% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 47, 30% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 47, 40% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 47, 50% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 47, 60% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 47, 70% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 47, 80% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 47, 90% \t train_loss: 0.00 took: 2.22s\n",
            "Validation loss = 0.97\n",
            "Epoch 48, 10% \t train_loss: 0.00 took: 2.27s\n",
            "Epoch 48, 20% \t train_loss: 0.00 took: 2.18s\n",
            "Epoch 48, 30% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 48, 40% \t train_loss: 0.00 took: 2.20s\n",
            "Epoch 48, 50% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 48, 60% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 48, 70% \t train_loss: 0.00 took: 2.25s\n",
            "Epoch 48, 80% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 48, 90% \t train_loss: 0.00 took: 2.24s\n",
            "Validation loss = 0.95\n",
            "Epoch 49, 10% \t train_loss: 0.00 took: 2.27s\n",
            "Epoch 49, 20% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 49, 30% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 49, 40% \t train_loss: 0.00 took: 2.20s\n",
            "Epoch 49, 50% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 49, 60% \t train_loss: 0.00 took: 2.23s\n",
            "Epoch 49, 70% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 49, 80% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 49, 90% \t train_loss: 0.00 took: 2.22s\n",
            "Validation loss = 0.94\n",
            "Epoch 50, 10% \t train_loss: 0.00 took: 2.27s\n",
            "Epoch 50, 20% \t train_loss: 0.00 took: 2.20s\n",
            "Epoch 50, 30% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 50, 40% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 50, 50% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 50, 60% \t train_loss: 0.00 took: 2.21s\n",
            "Epoch 50, 70% \t train_loss: 0.00 took: 2.22s\n",
            "Epoch 50, 80% \t train_loss: 0.00 took: 2.24s\n",
            "Epoch 50, 90% \t train_loss: 0.00 took: 2.23s\n",
            "Validation loss = 0.96\n",
            "Training Finished, took 1160.84s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BOOrUQC3oVs",
        "colab_type": "code",
        "outputId": "40d3b4a2-e9f2-424e-ea34-2af48719c65a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "plot_losses(train_history, val_history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfEAAAGDCAYAAAA72Cm3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeYqbOh_PRYT",
        "colab_type": "code",
        "outputId": "23cd09a9-24a7-4072-d1b6-e37e8c2b5cff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "net.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): LambdaLayer()\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): LambdaLayer()\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx04vlqK33Cn",
        "colab_type": "code",
        "outputId": "96211b44-1709-420e-d394-8e4526372af2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "def dataset_accuracy(net, data_loader, name=\"\"):\n",
        "    net = net.to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in data_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum()\n",
        "    accuracy = 100 * float(correct) / total\n",
        "    print('Accuracy of the network on the {} {} images: {:.2f} %'.format(total, name, accuracy))\n",
        "\n",
        "def train_set_accuracy(net):\n",
        "    dataset_accuracy(net, train_loader, \"train\")\n",
        "\n",
        "def val_set_accuracy(net):\n",
        "    dataset_accuracy(net, val_loader, \"validation\")  \n",
        "    \n",
        "def test_set_accuracy(net):\n",
        "    dataset_accuracy(net, test_loader, \"test\")\n",
        "\n",
        "def compute_accuracy(net):\n",
        "    train_set_accuracy(net)\n",
        "    val_set_accuracy(net)\n",
        "    test_set_accuracy(net)\n",
        "    \n",
        "print(\"Computing accuracy...\")\n",
        "compute_accuracy(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computing accuracy...\n",
            "Accuracy of the network on the 45000 train images: 89.04 %\n",
            "Accuracy of the network on the 5000 validation images: 82.04 %\n",
            "Accuracy of the network on the 10000 test images: 80.66 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}