{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hiCp9PbwCoW",
        "colab_type": "code",
        "outputId": "75b8c1b2-a241-4294-b56d-a07dfbd7b772",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "%matplotlib inline\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from __future__ import print_function, division\n",
        "import itertools\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import numpy as np\n",
        "import torch.nn.init as init\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3XSZcdmAefM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "colors = [[31, 120, 180], [51, 160, 44]]\n",
        "colors = [(r / 255, g / 255, b / 255) for (r, g, b) in colors]\n",
        "\n",
        "\n",
        "def plot_losses(train_history, val_history):\n",
        "    x = np.arange(1, len(train_history) + 1)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, train_history, color=colors[0], label=\"Training loss\", linewidth=2)\n",
        "    plt.plot(x, val_history, color=colors[1], label=\"Validation loss\", linewidth=2)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title(\"Evolution of the training and validation loss\")\n",
        "    plt.show()\n",
        "seed = 5678\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.cuda.manual_seed(seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_HKF9C50BRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "n_training_samples = 45000\n",
        "n_val_samples = 5000\n",
        "n_test_samples = 10000\n",
        "train_sampler = SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
        "val_sampler = SubsetRandomSampler(np.arange(n_training_samples, n_training_samples + n_val_samples, dtype=np.int64))\n",
        "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQr3tsszOgut",
        "colab_type": "code",
        "outputId": "0667f413-00b4-4069-8961-cea9808be010",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "num_workers = 2\n",
        "test_batch_size = 4\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(size = (32,32),padding = 4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root='/content/drive/My Drive/Colab Notebooks', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=test_batch_size, sampler=train_sampler,\n",
        "                                          num_workers=num_workers)\n",
        "\n",
        "test_set = torchvision.datasets.CIFAR10(root='/content/drive/My Drive/Colab Notebooks', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=test_batch_size, sampler=test_sampler,\n",
        "                                         num_workers=num_workers)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnj7yIzd_KY5",
        "colab_type": "code",
        "outputId": "8eeefc74-ecb2-47d8-81c5-f22bbc3314de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def _weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            if option == 'A':\n",
        "                \"\"\"\n",
        "                For CIFAR10 ResNet paper uses option A.\n",
        "                \"\"\"\n",
        "                self.shortcut = LambdaLayer(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "            elif option == 'B':\n",
        "                self.shortcut = nn.Sequential(\n",
        "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                     nn.BatchNorm2d(self.expansion * planes)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "net = ResNet(BasicBlock, [3, 3, 3])\n",
        "print(net)\n",
        "\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): LambdaLayer()\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): LambdaLayer()\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Np7CRd4BnsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createLoss():\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    return criterion\n",
        "def createOptimizer(net):  \n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
        "    return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1n39z-zC_LN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_train_loader(batch_size):\n",
        "    return torch.utils.data.DataLoader(train_set, batch_size=batch_size, sampler=train_sampler,\n",
        "                                              num_workers=num_workers)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(train_set, batch_size=128, sampler=val_sampler,\n",
        "                                          num_workers=num_workers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM5Ll7h9eo7z",
        "colab_type": "code",
        "outputId": "284ab429-18ae-4def-941e-12343f64ce39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "val_loader"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f7faea515f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLQUvof6knI6",
        "colab_type": "code",
        "outputId": "f85109ce-16ce-427c-9e2d-90706116e22a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def train(net, batch_size, n_epochs, learning_rate):\n",
        "    \"\"\"\n",
        "    Train a neural network and print statistics of the training\n",
        "    \n",
        "    :param net: (PyTorch Neural Network)\n",
        "    :param batch_size: (int)\n",
        "    :param n_epochs: (int)  Number of iterations on the training set\n",
        "    :param learning_rate: (float) learning rate used by the optimizer\n",
        "    \"\"\"\n",
        "    print(\"===== HYPERPARAMETERS =====\")\n",
        "    print(\"batch_size=\", batch_size)\n",
        "    print(\"n_epochs=\", n_epochs)\n",
        "    print(\"learning_rate=\", learning_rate)\n",
        "    print(\"=\" * 30)\n",
        "    \n",
        "    train_loader = get_train_loader(batch_size)\n",
        "    n_minibatches = len(train_loader)\n",
        "    train_history = []\n",
        "    val_history = []\n",
        "    criterion = createLoss()\n",
        "    optimizer = createOptimizer(net)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,40], gamma=0.1)\n",
        "    training_start_time = time.time()\n",
        "    best_error = np.inf\n",
        "    best_model_path = \"/content/drive/My Drive/Colab Notebooks/best_model.pth\"\n",
        "    \n",
        "    net = net.to(device)\n",
        "\n",
        "    for epoch in range(n_epochs):  \n",
        "        running_loss = 0.0\n",
        "        print_every = n_minibatches // 10\n",
        "        start_time = time.time()\n",
        "        total_train_loss = 0\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            total_train_loss += loss.item()\n",
        "            if (i + 1) % (print_every + 1) == 0:    \n",
        "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
        "                      epoch + 1, int(100 * (i + 1) / n_minibatches), running_loss / print_every,\n",
        "                      time.time() - start_time))\n",
        "                running_loss = 0.0\n",
        "                start_time = time.time()\n",
        "        train_history.append(total_train_loss / len(train_loader))\n",
        "        \n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "          for inputs, labels in val_loader:\n",
        "              inputs, labels = inputs.to(device), labels.to(device)\n",
        "              predictions = net(inputs)\n",
        "              val_loss = criterion(predictions, labels)\n",
        "              total_val_loss += val_loss.item()\n",
        "        val_history.append(total_val_loss / len(val_loader))\n",
        "        if total_val_loss < best_error:\n",
        "            best_error = total_val_loss\n",
        "            torch.save(net.state_dict(), best_model_path)\n",
        "        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(val_loader)))\n",
        "        scheduler.step()\n",
        "    print(\"Training Finished, took {:.2f}s\".format(time.time() - training_start_time))\n",
        "    \n",
        "    net.load_state_dict(torch.load(best_model_path))\n",
        "    \n",
        "    return train_history, val_history\n",
        "\n",
        "train_history, val_history = train(net, batch_size=32, n_epochs=50, learning_rate=0.1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== HYPERPARAMETERS =====\n",
            "batch_size= 32\n",
            "n_epochs= 50\n",
            "learning_rate= 0.1\n",
            "==============================\n",
            "Epoch 1, 10% \t train_loss: 2.25 took: 3.03s\n",
            "Epoch 1, 20% \t train_loss: 1.94 took: 2.75s\n",
            "Epoch 1, 30% \t train_loss: 1.79 took: 2.76s\n",
            "Epoch 1, 40% \t train_loss: 1.74 took: 2.74s\n",
            "Epoch 1, 50% \t train_loss: 1.62 took: 2.72s\n",
            "Epoch 1, 60% \t train_loss: 1.51 took: 2.75s\n",
            "Epoch 1, 70% \t train_loss: 1.47 took: 2.74s\n",
            "Epoch 1, 80% \t train_loss: 1.44 took: 2.74s\n",
            "Epoch 1, 90% \t train_loss: 1.34 took: 2.77s\n",
            "Validation loss = 1.27\n",
            "Epoch 2, 10% \t train_loss: 1.24 took: 2.89s\n",
            "Epoch 2, 20% \t train_loss: 1.23 took: 2.75s\n",
            "Epoch 2, 30% \t train_loss: 1.17 took: 2.73s\n",
            "Epoch 2, 40% \t train_loss: 1.12 took: 2.72s\n",
            "Epoch 2, 50% \t train_loss: 1.13 took: 2.72s\n",
            "Epoch 2, 60% \t train_loss: 1.10 took: 2.72s\n",
            "Epoch 2, 70% \t train_loss: 1.07 took: 2.73s\n",
            "Epoch 2, 80% \t train_loss: 1.03 took: 2.73s\n",
            "Epoch 2, 90% \t train_loss: 1.02 took: 2.74s\n",
            "Validation loss = 0.96\n",
            "Epoch 3, 10% \t train_loss: 0.95 took: 2.79s\n",
            "Epoch 3, 20% \t train_loss: 0.96 took: 2.72s\n",
            "Epoch 3, 30% \t train_loss: 0.95 took: 2.72s\n",
            "Epoch 3, 40% \t train_loss: 0.92 took: 2.69s\n",
            "Epoch 3, 50% \t train_loss: 0.90 took: 2.74s\n",
            "Epoch 3, 60% \t train_loss: 0.87 took: 2.70s\n",
            "Epoch 3, 70% \t train_loss: 0.87 took: 2.71s\n",
            "Epoch 3, 80% \t train_loss: 0.82 took: 2.73s\n",
            "Epoch 3, 90% \t train_loss: 0.83 took: 2.75s\n",
            "Validation loss = 0.80\n",
            "Epoch 4, 10% \t train_loss: 0.77 took: 2.76s\n",
            "Epoch 4, 20% \t train_loss: 0.82 took: 2.70s\n",
            "Epoch 4, 30% \t train_loss: 0.74 took: 2.74s\n",
            "Epoch 4, 40% \t train_loss: 0.73 took: 2.68s\n",
            "Epoch 4, 50% \t train_loss: 0.77 took: 2.69s\n",
            "Epoch 4, 60% \t train_loss: 0.73 took: 2.69s\n",
            "Epoch 4, 70% \t train_loss: 0.74 took: 2.71s\n",
            "Epoch 4, 80% \t train_loss: 0.75 took: 2.68s\n",
            "Epoch 4, 90% \t train_loss: 0.75 took: 2.71s\n",
            "Validation loss = 0.68\n",
            "Epoch 5, 10% \t train_loss: 0.68 took: 2.76s\n",
            "Epoch 5, 20% \t train_loss: 0.69 took: 2.68s\n",
            "Epoch 5, 30% \t train_loss: 0.68 took: 2.71s\n",
            "Epoch 5, 40% \t train_loss: 0.72 took: 2.68s\n",
            "Epoch 5, 50% \t train_loss: 0.68 took: 2.67s\n",
            "Epoch 5, 60% \t train_loss: 0.67 took: 2.68s\n",
            "Epoch 5, 70% \t train_loss: 0.66 took: 2.68s\n",
            "Epoch 5, 80% \t train_loss: 0.66 took: 2.64s\n",
            "Epoch 5, 90% \t train_loss: 0.69 took: 2.66s\n",
            "Validation loss = 0.62\n",
            "Epoch 6, 10% \t train_loss: 0.64 took: 2.76s\n",
            "Epoch 6, 20% \t train_loss: 0.66 took: 2.69s\n",
            "Epoch 6, 30% \t train_loss: 0.61 took: 2.68s\n",
            "Epoch 6, 40% \t train_loss: 0.61 took: 2.66s\n",
            "Epoch 6, 50% \t train_loss: 0.61 took: 2.65s\n",
            "Epoch 6, 60% \t train_loss: 0.64 took: 2.69s\n",
            "Epoch 6, 70% \t train_loss: 0.64 took: 2.67s\n",
            "Epoch 6, 80% \t train_loss: 0.59 took: 2.69s\n",
            "Epoch 6, 90% \t train_loss: 0.60 took: 2.68s\n",
            "Validation loss = 0.61\n",
            "Epoch 7, 10% \t train_loss: 0.59 took: 2.72s\n",
            "Epoch 7, 20% \t train_loss: 0.55 took: 2.68s\n",
            "Epoch 7, 30% \t train_loss: 0.58 took: 2.65s\n",
            "Epoch 7, 40% \t train_loss: 0.58 took: 2.66s\n",
            "Epoch 7, 50% \t train_loss: 0.58 took: 2.67s\n",
            "Epoch 7, 60% \t train_loss: 0.55 took: 2.65s\n",
            "Epoch 7, 70% \t train_loss: 0.60 took: 2.65s\n",
            "Epoch 7, 80% \t train_loss: 0.55 took: 2.65s\n",
            "Epoch 7, 90% \t train_loss: 0.56 took: 2.65s\n",
            "Validation loss = 0.54\n",
            "Epoch 8, 10% \t train_loss: 0.53 took: 2.74s\n",
            "Epoch 8, 20% \t train_loss: 0.55 took: 2.63s\n",
            "Epoch 8, 30% \t train_loss: 0.55 took: 2.68s\n",
            "Epoch 8, 40% \t train_loss: 0.53 took: 2.67s\n",
            "Epoch 8, 50% \t train_loss: 0.54 took: 2.64s\n",
            "Epoch 8, 60% \t train_loss: 0.54 took: 2.66s\n",
            "Epoch 8, 70% \t train_loss: 0.53 took: 2.66s\n",
            "Epoch 8, 80% \t train_loss: 0.55 took: 2.62s\n",
            "Epoch 8, 90% \t train_loss: 0.55 took: 2.65s\n",
            "Validation loss = 0.52\n",
            "Epoch 9, 10% \t train_loss: 0.50 took: 2.74s\n",
            "Epoch 9, 20% \t train_loss: 0.51 took: 2.70s\n",
            "Epoch 9, 30% \t train_loss: 0.49 took: 2.67s\n",
            "Epoch 9, 40% \t train_loss: 0.50 took: 2.67s\n",
            "Epoch 9, 50% \t train_loss: 0.52 took: 2.68s\n",
            "Epoch 9, 60% \t train_loss: 0.52 took: 2.69s\n",
            "Epoch 9, 70% \t train_loss: 0.52 took: 2.70s\n",
            "Epoch 9, 80% \t train_loss: 0.48 took: 2.68s\n",
            "Epoch 9, 90% \t train_loss: 0.52 took: 2.66s\n",
            "Validation loss = 0.52\n",
            "Epoch 10, 10% \t train_loss: 0.47 took: 2.74s\n",
            "Epoch 10, 20% \t train_loss: 0.51 took: 2.64s\n",
            "Epoch 10, 30% \t train_loss: 0.48 took: 2.65s\n",
            "Epoch 10, 40% \t train_loss: 0.50 took: 2.67s\n",
            "Epoch 10, 50% \t train_loss: 0.48 took: 2.64s\n",
            "Epoch 10, 60% \t train_loss: 0.48 took: 2.67s\n",
            "Epoch 10, 70% \t train_loss: 0.47 took: 2.67s\n",
            "Epoch 10, 80% \t train_loss: 0.47 took: 2.66s\n",
            "Epoch 10, 90% \t train_loss: 0.47 took: 2.67s\n",
            "Validation loss = 0.54\n",
            "Epoch 11, 10% \t train_loss: 0.48 took: 2.78s\n",
            "Epoch 11, 20% \t train_loss: 0.46 took: 2.69s\n",
            "Epoch 11, 30% \t train_loss: 0.47 took: 2.67s\n",
            "Epoch 11, 40% \t train_loss: 0.47 took: 2.65s\n",
            "Epoch 11, 50% \t train_loss: 0.47 took: 2.67s\n",
            "Epoch 11, 60% \t train_loss: 0.42 took: 2.63s\n",
            "Epoch 11, 70% \t train_loss: 0.44 took: 2.66s\n",
            "Epoch 11, 80% \t train_loss: 0.47 took: 2.64s\n",
            "Epoch 11, 90% \t train_loss: 0.46 took: 2.65s\n",
            "Validation loss = 0.48\n",
            "Epoch 12, 10% \t train_loss: 0.43 took: 2.75s\n",
            "Epoch 12, 20% \t train_loss: 0.44 took: 2.69s\n",
            "Epoch 12, 30% \t train_loss: 0.45 took: 2.65s\n",
            "Epoch 12, 40% \t train_loss: 0.43 took: 2.66s\n",
            "Epoch 12, 50% \t train_loss: 0.42 took: 2.68s\n",
            "Epoch 12, 60% \t train_loss: 0.43 took: 2.68s\n",
            "Epoch 12, 70% \t train_loss: 0.44 took: 2.61s\n",
            "Epoch 12, 80% \t train_loss: 0.47 took: 2.64s\n",
            "Epoch 12, 90% \t train_loss: 0.46 took: 2.71s\n",
            "Validation loss = 0.45\n",
            "Epoch 13, 10% \t train_loss: 0.40 took: 2.77s\n",
            "Epoch 13, 20% \t train_loss: 0.43 took: 2.69s\n",
            "Epoch 13, 30% \t train_loss: 0.42 took: 2.62s\n",
            "Epoch 13, 40% \t train_loss: 0.41 took: 2.59s\n",
            "Epoch 13, 50% \t train_loss: 0.43 took: 2.64s\n",
            "Epoch 13, 60% \t train_loss: 0.43 took: 2.61s\n",
            "Epoch 13, 70% \t train_loss: 0.40 took: 2.65s\n",
            "Epoch 13, 80% \t train_loss: 0.42 took: 2.61s\n",
            "Epoch 13, 90% \t train_loss: 0.42 took: 2.63s\n",
            "Validation loss = 0.44\n",
            "Epoch 14, 10% \t train_loss: 0.40 took: 2.72s\n",
            "Epoch 14, 20% \t train_loss: 0.41 took: 2.62s\n",
            "Epoch 14, 30% \t train_loss: 0.41 took: 2.65s\n",
            "Epoch 14, 40% \t train_loss: 0.40 took: 2.61s\n",
            "Epoch 14, 50% \t train_loss: 0.40 took: 2.62s\n",
            "Epoch 14, 60% \t train_loss: 0.40 took: 2.58s\n",
            "Epoch 14, 70% \t train_loss: 0.41 took: 2.61s\n",
            "Epoch 14, 80% \t train_loss: 0.40 took: 2.64s\n",
            "Epoch 14, 90% \t train_loss: 0.40 took: 2.62s\n",
            "Validation loss = 0.47\n",
            "Epoch 15, 10% \t train_loss: 0.37 took: 2.70s\n",
            "Epoch 15, 20% \t train_loss: 0.37 took: 2.64s\n",
            "Epoch 15, 30% \t train_loss: 0.39 took: 2.63s\n",
            "Epoch 15, 40% \t train_loss: 0.35 took: 2.67s\n",
            "Epoch 15, 50% \t train_loss: 0.38 took: 2.65s\n",
            "Epoch 15, 60% \t train_loss: 0.40 took: 2.64s\n",
            "Epoch 15, 70% \t train_loss: 0.40 took: 2.62s\n",
            "Epoch 15, 80% \t train_loss: 0.39 took: 2.67s\n",
            "Epoch 15, 90% \t train_loss: 0.40 took: 2.68s\n",
            "Validation loss = 0.43\n",
            "Epoch 16, 10% \t train_loss: 0.36 took: 2.72s\n",
            "Epoch 16, 20% \t train_loss: 0.38 took: 2.62s\n",
            "Epoch 16, 30% \t train_loss: 0.36 took: 2.66s\n",
            "Epoch 16, 40% \t train_loss: 0.37 took: 2.65s\n",
            "Epoch 16, 50% \t train_loss: 0.36 took: 2.69s\n",
            "Epoch 16, 60% \t train_loss: 0.37 took: 2.61s\n",
            "Epoch 16, 70% \t train_loss: 0.39 took: 2.63s\n",
            "Epoch 16, 80% \t train_loss: 0.37 took: 2.60s\n",
            "Epoch 16, 90% \t train_loss: 0.38 took: 2.64s\n",
            "Validation loss = 0.44\n",
            "Epoch 17, 10% \t train_loss: 0.37 took: 2.71s\n",
            "Epoch 17, 20% \t train_loss: 0.35 took: 2.64s\n",
            "Epoch 17, 30% \t train_loss: 0.36 took: 2.64s\n",
            "Epoch 17, 40% \t train_loss: 0.37 took: 2.63s\n",
            "Epoch 17, 50% \t train_loss: 0.36 took: 2.64s\n",
            "Epoch 17, 60% \t train_loss: 0.34 took: 2.63s\n",
            "Epoch 17, 70% \t train_loss: 0.39 took: 2.62s\n",
            "Epoch 17, 80% \t train_loss: 0.36 took: 2.67s\n",
            "Epoch 17, 90% \t train_loss: 0.37 took: 2.62s\n",
            "Validation loss = 0.42\n",
            "Epoch 18, 10% \t train_loss: 0.32 took: 2.71s\n",
            "Epoch 18, 20% \t train_loss: 0.33 took: 2.64s\n",
            "Epoch 18, 30% \t train_loss: 0.34 took: 2.67s\n",
            "Epoch 18, 40% \t train_loss: 0.35 took: 2.66s\n",
            "Epoch 18, 50% \t train_loss: 0.36 took: 2.70s\n",
            "Epoch 18, 60% \t train_loss: 0.35 took: 2.66s\n",
            "Epoch 18, 70% \t train_loss: 0.36 took: 2.63s\n",
            "Epoch 18, 80% \t train_loss: 0.37 took: 2.64s\n",
            "Epoch 18, 90% \t train_loss: 0.36 took: 2.63s\n",
            "Validation loss = 0.40\n",
            "Epoch 19, 10% \t train_loss: 0.33 took: 2.77s\n",
            "Epoch 19, 20% \t train_loss: 0.36 took: 2.62s\n",
            "Epoch 19, 30% \t train_loss: 0.33 took: 2.65s\n",
            "Epoch 19, 40% \t train_loss: 0.33 took: 2.62s\n",
            "Epoch 19, 50% \t train_loss: 0.33 took: 2.65s\n",
            "Epoch 19, 60% \t train_loss: 0.33 took: 2.62s\n",
            "Epoch 19, 70% \t train_loss: 0.34 took: 2.64s\n",
            "Epoch 19, 80% \t train_loss: 0.35 took: 2.65s\n",
            "Epoch 19, 90% \t train_loss: 0.36 took: 2.64s\n",
            "Validation loss = 0.42\n",
            "Epoch 20, 10% \t train_loss: 0.33 took: 2.71s\n",
            "Epoch 20, 20% \t train_loss: 0.31 took: 2.65s\n",
            "Epoch 20, 30% \t train_loss: 0.32 took: 2.65s\n",
            "Epoch 20, 40% \t train_loss: 0.33 took: 2.67s\n",
            "Epoch 20, 50% \t train_loss: 0.35 took: 2.66s\n",
            "Epoch 20, 60% \t train_loss: 0.33 took: 2.65s\n",
            "Epoch 20, 70% \t train_loss: 0.32 took: 2.74s\n",
            "Epoch 20, 80% \t train_loss: 0.34 took: 2.69s\n",
            "Epoch 20, 90% \t train_loss: 0.30 took: 2.66s\n",
            "Validation loss = 0.42\n",
            "Epoch 21, 10% \t train_loss: 0.33 took: 2.78s\n",
            "Epoch 21, 20% \t train_loss: 0.32 took: 2.72s\n",
            "Epoch 21, 30% \t train_loss: 0.31 took: 2.73s\n",
            "Epoch 21, 40% \t train_loss: 0.31 took: 2.64s\n",
            "Epoch 21, 50% \t train_loss: 0.32 took: 2.67s\n",
            "Epoch 21, 60% \t train_loss: 0.31 took: 2.65s\n",
            "Epoch 21, 70% \t train_loss: 0.32 took: 2.65s\n",
            "Epoch 21, 80% \t train_loss: 0.32 took: 2.65s\n",
            "Epoch 21, 90% \t train_loss: 0.31 took: 2.70s\n",
            "Validation loss = 0.41\n",
            "Epoch 22, 10% \t train_loss: 0.29 took: 2.84s\n",
            "Epoch 22, 20% \t train_loss: 0.31 took: 2.69s\n",
            "Epoch 22, 30% \t train_loss: 0.29 took: 2.68s\n",
            "Epoch 22, 40% \t train_loss: 0.32 took: 2.64s\n",
            "Epoch 22, 50% \t train_loss: 0.31 took: 2.67s\n",
            "Epoch 22, 60% \t train_loss: 0.31 took: 2.66s\n",
            "Epoch 22, 70% \t train_loss: 0.34 took: 2.66s\n",
            "Epoch 22, 80% \t train_loss: 0.29 took: 2.66s\n",
            "Epoch 22, 90% \t train_loss: 0.32 took: 2.73s\n",
            "Validation loss = 0.46\n",
            "Epoch 23, 10% \t train_loss: 0.30 took: 2.80s\n",
            "Epoch 23, 20% \t train_loss: 0.28 took: 2.75s\n",
            "Epoch 23, 30% \t train_loss: 0.29 took: 2.70s\n",
            "Epoch 23, 40% \t train_loss: 0.30 took: 2.80s\n",
            "Epoch 23, 50% \t train_loss: 0.29 took: 2.73s\n",
            "Epoch 23, 60% \t train_loss: 0.31 took: 2.74s\n",
            "Epoch 23, 70% \t train_loss: 0.31 took: 2.73s\n",
            "Epoch 23, 80% \t train_loss: 0.31 took: 2.71s\n",
            "Epoch 23, 90% \t train_loss: 0.32 took: 2.79s\n",
            "Validation loss = 0.38\n",
            "Epoch 24, 10% \t train_loss: 0.30 took: 2.86s\n",
            "Epoch 24, 20% \t train_loss: 0.28 took: 2.80s\n",
            "Epoch 24, 30% \t train_loss: 0.29 took: 2.81s\n",
            "Epoch 24, 40% \t train_loss: 0.32 took: 2.76s\n",
            "Epoch 24, 50% \t train_loss: 0.30 took: 2.71s\n",
            "Epoch 24, 60% \t train_loss: 0.30 took: 2.71s\n",
            "Epoch 24, 70% \t train_loss: 0.30 took: 2.72s\n",
            "Epoch 24, 80% \t train_loss: 0.29 took: 2.76s\n",
            "Epoch 24, 90% \t train_loss: 0.30 took: 2.72s\n",
            "Validation loss = 0.38\n",
            "Epoch 25, 10% \t train_loss: 0.28 took: 2.80s\n",
            "Epoch 25, 20% \t train_loss: 0.26 took: 2.72s\n",
            "Epoch 25, 30% \t train_loss: 0.29 took: 2.73s\n",
            "Epoch 25, 40% \t train_loss: 0.29 took: 2.72s\n",
            "Epoch 25, 50% \t train_loss: 0.27 took: 2.75s\n",
            "Epoch 25, 60% \t train_loss: 0.31 took: 2.72s\n",
            "Epoch 25, 70% \t train_loss: 0.30 took: 2.76s\n",
            "Epoch 25, 80% \t train_loss: 0.28 took: 2.76s\n",
            "Epoch 25, 90% \t train_loss: 0.29 took: 2.71s\n",
            "Validation loss = 0.41\n",
            "Epoch 26, 10% \t train_loss: 0.27 took: 2.83s\n",
            "Epoch 26, 20% \t train_loss: 0.27 took: 2.72s\n",
            "Epoch 26, 30% \t train_loss: 0.27 took: 2.76s\n",
            "Epoch 26, 40% \t train_loss: 0.26 took: 2.73s\n",
            "Epoch 26, 50% \t train_loss: 0.30 took: 2.74s\n",
            "Epoch 26, 60% \t train_loss: 0.28 took: 2.75s\n",
            "Epoch 26, 70% \t train_loss: 0.29 took: 2.77s\n",
            "Epoch 26, 80% \t train_loss: 0.26 took: 2.76s\n",
            "Epoch 26, 90% \t train_loss: 0.28 took: 2.77s\n",
            "Validation loss = 0.41\n",
            "Epoch 27, 10% \t train_loss: 0.27 took: 2.83s\n",
            "Epoch 27, 20% \t train_loss: 0.26 took: 2.71s\n",
            "Epoch 27, 30% \t train_loss: 0.27 took: 2.75s\n",
            "Epoch 27, 40% \t train_loss: 0.28 took: 2.75s\n",
            "Epoch 27, 50% \t train_loss: 0.27 took: 2.75s\n",
            "Epoch 27, 60% \t train_loss: 0.26 took: 2.74s\n",
            "Epoch 27, 70% \t train_loss: 0.27 took: 2.75s\n",
            "Epoch 27, 80% \t train_loss: 0.27 took: 2.72s\n",
            "Epoch 27, 90% \t train_loss: 0.27 took: 2.74s\n",
            "Validation loss = 0.39\n",
            "Epoch 28, 10% \t train_loss: 0.25 took: 2.81s\n",
            "Epoch 28, 20% \t train_loss: 0.24 took: 2.73s\n",
            "Epoch 28, 30% \t train_loss: 0.27 took: 2.75s\n",
            "Epoch 28, 40% \t train_loss: 0.27 took: 2.75s\n",
            "Epoch 28, 50% \t train_loss: 0.25 took: 2.75s\n",
            "Epoch 28, 60% \t train_loss: 0.24 took: 2.77s\n",
            "Epoch 28, 70% \t train_loss: 0.26 took: 2.75s\n",
            "Epoch 28, 80% \t train_loss: 0.26 took: 2.78s\n",
            "Epoch 28, 90% \t train_loss: 0.28 took: 2.76s\n",
            "Validation loss = 0.37\n",
            "Epoch 29, 10% \t train_loss: 0.24 took: 2.83s\n",
            "Epoch 29, 20% \t train_loss: 0.26 took: 2.78s\n",
            "Epoch 29, 30% \t train_loss: 0.25 took: 2.76s\n",
            "Epoch 29, 40% \t train_loss: 0.24 took: 2.75s\n",
            "Epoch 29, 50% \t train_loss: 0.27 took: 2.75s\n",
            "Epoch 29, 60% \t train_loss: 0.26 took: 2.76s\n",
            "Epoch 29, 70% \t train_loss: 0.27 took: 2.79s\n",
            "Epoch 29, 80% \t train_loss: 0.26 took: 2.77s\n",
            "Epoch 29, 90% \t train_loss: 0.27 took: 2.75s\n",
            "Validation loss = 0.38\n",
            "Epoch 30, 10% \t train_loss: 0.24 took: 2.81s\n",
            "Epoch 30, 20% \t train_loss: 0.24 took: 2.78s\n",
            "Epoch 30, 30% \t train_loss: 0.24 took: 2.74s\n",
            "Epoch 30, 40% \t train_loss: 0.25 took: 2.73s\n",
            "Epoch 30, 50% \t train_loss: 0.26 took: 2.76s\n",
            "Epoch 30, 60% \t train_loss: 0.24 took: 2.73s\n",
            "Epoch 30, 70% \t train_loss: 0.26 took: 2.78s\n",
            "Epoch 30, 80% \t train_loss: 0.23 took: 2.76s\n",
            "Epoch 30, 90% \t train_loss: 0.27 took: 2.78s\n",
            "Validation loss = 0.39\n",
            "Epoch 31, 10% \t train_loss: 0.22 took: 2.86s\n",
            "Epoch 31, 20% \t train_loss: 0.21 took: 2.75s\n",
            "Epoch 31, 30% \t train_loss: 0.20 took: 2.77s\n",
            "Epoch 31, 40% \t train_loss: 0.18 took: 2.73s\n",
            "Epoch 31, 50% \t train_loss: 0.18 took: 2.74s\n",
            "Epoch 31, 60% \t train_loss: 0.19 took: 2.72s\n",
            "Epoch 31, 70% \t train_loss: 0.18 took: 2.77s\n",
            "Epoch 31, 80% \t train_loss: 0.18 took: 2.75s\n",
            "Epoch 31, 90% \t train_loss: 0.18 took: 2.76s\n",
            "Validation loss = 0.32\n",
            "Epoch 32, 10% \t train_loss: 0.17 took: 2.83s\n",
            "Epoch 32, 20% \t train_loss: 0.16 took: 2.75s\n",
            "Epoch 32, 30% \t train_loss: 0.17 took: 2.80s\n",
            "Epoch 32, 40% \t train_loss: 0.17 took: 2.75s\n",
            "Epoch 32, 50% \t train_loss: 0.16 took: 2.73s\n",
            "Epoch 32, 60% \t train_loss: 0.16 took: 2.76s\n",
            "Epoch 32, 70% \t train_loss: 0.17 took: 2.80s\n",
            "Epoch 32, 80% \t train_loss: 0.17 took: 2.77s\n",
            "Epoch 32, 90% \t train_loss: 0.18 took: 2.76s\n",
            "Validation loss = 0.33\n",
            "Epoch 33, 10% \t train_loss: 0.14 took: 2.86s\n",
            "Epoch 33, 20% \t train_loss: 0.17 took: 2.76s\n",
            "Epoch 33, 30% \t train_loss: 0.15 took: 2.76s\n",
            "Epoch 33, 40% \t train_loss: 0.17 took: 2.74s\n",
            "Epoch 33, 50% \t train_loss: 0.16 took: 2.74s\n",
            "Epoch 33, 60% \t train_loss: 0.17 took: 2.74s\n",
            "Epoch 33, 70% \t train_loss: 0.16 took: 2.74s\n",
            "Epoch 33, 80% \t train_loss: 0.15 took: 2.80s\n",
            "Epoch 33, 90% \t train_loss: 0.17 took: 2.74s\n",
            "Validation loss = 0.33\n",
            "Epoch 34, 10% \t train_loss: 0.15 took: 2.83s\n",
            "Epoch 34, 20% \t train_loss: 0.16 took: 2.77s\n",
            "Epoch 34, 30% \t train_loss: 0.15 took: 2.73s\n",
            "Epoch 34, 40% \t train_loss: 0.16 took: 2.78s\n",
            "Epoch 34, 50% \t train_loss: 0.14 took: 2.76s\n",
            "Epoch 34, 60% \t train_loss: 0.15 took: 2.79s\n",
            "Epoch 34, 70% \t train_loss: 0.13 took: 2.74s\n",
            "Epoch 34, 80% \t train_loss: 0.16 took: 2.83s\n",
            "Epoch 34, 90% \t train_loss: 0.15 took: 2.82s\n",
            "Validation loss = 0.32\n",
            "Epoch 35, 10% \t train_loss: 0.15 took: 2.91s\n",
            "Epoch 35, 20% \t train_loss: 0.14 took: 2.79s\n",
            "Epoch 35, 30% \t train_loss: 0.14 took: 2.74s\n",
            "Epoch 35, 40% \t train_loss: 0.15 took: 2.75s\n",
            "Epoch 35, 50% \t train_loss: 0.14 took: 2.73s\n",
            "Epoch 35, 60% \t train_loss: 0.15 took: 2.72s\n",
            "Epoch 35, 70% \t train_loss: 0.13 took: 2.73s\n",
            "Epoch 35, 80% \t train_loss: 0.14 took: 2.75s\n",
            "Epoch 35, 90% \t train_loss: 0.15 took: 2.77s\n",
            "Validation loss = 0.32\n",
            "Epoch 36, 10% \t train_loss: 0.15 took: 2.83s\n",
            "Epoch 36, 20% \t train_loss: 0.14 took: 2.74s\n",
            "Epoch 36, 30% \t train_loss: 0.15 took: 2.75s\n",
            "Epoch 36, 40% \t train_loss: 0.15 took: 2.74s\n",
            "Epoch 36, 50% \t train_loss: 0.16 took: 2.74s\n",
            "Epoch 36, 60% \t train_loss: 0.15 took: 2.73s\n",
            "Epoch 36, 70% \t train_loss: 0.14 took: 2.73s\n",
            "Epoch 36, 80% \t train_loss: 0.15 took: 2.69s\n",
            "Epoch 36, 90% \t train_loss: 0.14 took: 2.73s\n",
            "Validation loss = 0.35\n",
            "Epoch 37, 10% \t train_loss: 0.14 took: 2.82s\n",
            "Epoch 37, 20% \t train_loss: 0.14 took: 2.75s\n",
            "Epoch 37, 30% \t train_loss: 0.13 took: 2.73s\n",
            "Epoch 37, 40% \t train_loss: 0.13 took: 2.75s\n",
            "Epoch 37, 50% \t train_loss: 0.14 took: 2.76s\n",
            "Epoch 37, 60% \t train_loss: 0.14 took: 2.73s\n",
            "Epoch 37, 70% \t train_loss: 0.16 took: 2.71s\n",
            "Epoch 37, 80% \t train_loss: 0.14 took: 2.70s\n",
            "Epoch 37, 90% \t train_loss: 0.14 took: 2.75s\n",
            "Validation loss = 0.33\n",
            "Epoch 38, 10% \t train_loss: 0.13 took: 2.82s\n",
            "Epoch 38, 20% \t train_loss: 0.14 took: 2.73s\n",
            "Epoch 38, 30% \t train_loss: 0.14 took: 2.75s\n",
            "Epoch 38, 40% \t train_loss: 0.13 took: 2.73s\n",
            "Epoch 38, 50% \t train_loss: 0.14 took: 2.67s\n",
            "Epoch 38, 60% \t train_loss: 0.14 took: 2.71s\n",
            "Epoch 38, 70% \t train_loss: 0.13 took: 2.71s\n",
            "Epoch 38, 80% \t train_loss: 0.13 took: 2.70s\n",
            "Epoch 38, 90% \t train_loss: 0.16 took: 2.70s\n",
            "Validation loss = 0.32\n",
            "Epoch 39, 10% \t train_loss: 0.13 took: 2.79s\n",
            "Epoch 39, 20% \t train_loss: 0.14 took: 2.73s\n",
            "Epoch 39, 30% \t train_loss: 0.14 took: 2.72s\n",
            "Epoch 39, 40% \t train_loss: 0.14 took: 2.73s\n",
            "Epoch 39, 50% \t train_loss: 0.14 took: 2.72s\n",
            "Epoch 39, 60% \t train_loss: 0.14 took: 2.72s\n",
            "Epoch 39, 70% \t train_loss: 0.13 took: 2.72s\n",
            "Epoch 39, 80% \t train_loss: 0.14 took: 2.72s\n",
            "Epoch 39, 90% \t train_loss: 0.14 took: 2.72s\n",
            "Validation loss = 0.33\n",
            "Epoch 40, 10% \t train_loss: 0.12 took: 2.80s\n",
            "Epoch 40, 20% \t train_loss: 0.13 took: 2.73s\n",
            "Epoch 40, 30% \t train_loss: 0.14 took: 2.72s\n",
            "Epoch 40, 40% \t train_loss: 0.13 took: 2.73s\n",
            "Epoch 40, 50% \t train_loss: 0.14 took: 2.74s\n",
            "Epoch 40, 60% \t train_loss: 0.14 took: 2.73s\n",
            "Epoch 40, 70% \t train_loss: 0.14 took: 2.74s\n",
            "Epoch 40, 80% \t train_loss: 0.15 took: 2.73s\n",
            "Epoch 40, 90% \t train_loss: 0.13 took: 2.73s\n",
            "Validation loss = 0.35\n",
            "Epoch 41, 10% \t train_loss: 0.14 took: 2.79s\n",
            "Epoch 41, 20% \t train_loss: 0.13 took: 2.70s\n",
            "Epoch 41, 30% \t train_loss: 0.13 took: 2.71s\n",
            "Epoch 41, 40% \t train_loss: 0.13 took: 2.69s\n",
            "Epoch 41, 50% \t train_loss: 0.12 took: 2.67s\n",
            "Epoch 41, 60% \t train_loss: 0.12 took: 2.69s\n",
            "Epoch 41, 70% \t train_loss: 0.13 took: 2.71s\n",
            "Epoch 41, 80% \t train_loss: 0.13 took: 2.72s\n",
            "Epoch 41, 90% \t train_loss: 0.13 took: 2.67s\n",
            "Validation loss = 0.34\n",
            "Epoch 42, 10% \t train_loss: 0.12 took: 2.79s\n",
            "Epoch 42, 20% \t train_loss: 0.13 took: 2.69s\n",
            "Epoch 42, 30% \t train_loss: 0.13 took: 2.69s\n",
            "Epoch 42, 40% \t train_loss: 0.14 took: 2.72s\n",
            "Epoch 42, 50% \t train_loss: 0.12 took: 2.67s\n",
            "Epoch 42, 60% \t train_loss: 0.15 took: 2.72s\n",
            "Epoch 42, 70% \t train_loss: 0.13 took: 2.68s\n",
            "Epoch 42, 80% \t train_loss: 0.13 took: 2.73s\n",
            "Epoch 42, 90% \t train_loss: 0.12 took: 2.71s\n",
            "Validation loss = 0.33\n",
            "Epoch 43, 10% \t train_loss: 0.12 took: 2.80s\n",
            "Epoch 43, 20% \t train_loss: 0.13 took: 2.72s\n",
            "Epoch 43, 30% \t train_loss: 0.13 took: 2.75s\n",
            "Epoch 43, 40% \t train_loss: 0.13 took: 2.77s\n",
            "Epoch 43, 50% \t train_loss: 0.13 took: 2.72s\n",
            "Epoch 43, 60% \t train_loss: 0.13 took: 2.76s\n",
            "Epoch 43, 70% \t train_loss: 0.14 took: 2.75s\n",
            "Epoch 43, 80% \t train_loss: 0.13 took: 2.72s\n",
            "Epoch 43, 90% \t train_loss: 0.14 took: 2.73s\n",
            "Validation loss = 0.32\n",
            "Epoch 44, 10% \t train_loss: 0.13 took: 2.78s\n",
            "Epoch 44, 20% \t train_loss: 0.12 took: 2.73s\n",
            "Epoch 44, 30% \t train_loss: 0.13 took: 2.72s\n",
            "Epoch 44, 40% \t train_loss: 0.12 took: 2.73s\n",
            "Epoch 44, 50% \t train_loss: 0.13 took: 2.71s\n",
            "Epoch 44, 60% \t train_loss: 0.12 took: 2.72s\n",
            "Epoch 44, 70% \t train_loss: 0.12 took: 2.71s\n",
            "Epoch 44, 80% \t train_loss: 0.12 took: 2.69s\n",
            "Epoch 44, 90% \t train_loss: 0.13 took: 2.72s\n",
            "Validation loss = 0.33\n",
            "Epoch 45, 10% \t train_loss: 0.13 took: 2.79s\n",
            "Epoch 45, 20% \t train_loss: 0.12 took: 2.69s\n",
            "Epoch 45, 30% \t train_loss: 0.12 took: 2.70s\n",
            "Epoch 45, 40% \t train_loss: 0.13 took: 2.68s\n",
            "Epoch 45, 50% \t train_loss: 0.13 took: 2.72s\n",
            "Epoch 45, 60% \t train_loss: 0.12 took: 2.72s\n",
            "Epoch 45, 70% \t train_loss: 0.12 took: 2.76s\n",
            "Epoch 45, 80% \t train_loss: 0.12 took: 2.76s\n",
            "Epoch 45, 90% \t train_loss: 0.12 took: 2.76s\n",
            "Validation loss = 0.35\n",
            "Epoch 46, 10% \t train_loss: 0.12 took: 2.78s\n",
            "Epoch 46, 20% \t train_loss: 0.12 took: 2.70s\n",
            "Epoch 46, 30% \t train_loss: 0.14 took: 2.73s\n",
            "Epoch 46, 40% \t train_loss: 0.13 took: 2.73s\n",
            "Epoch 46, 50% \t train_loss: 0.13 took: 2.70s\n",
            "Epoch 46, 60% \t train_loss: 0.12 took: 2.73s\n",
            "Epoch 46, 70% \t train_loss: 0.13 took: 2.69s\n",
            "Epoch 46, 80% \t train_loss: 0.12 took: 2.73s\n",
            "Epoch 46, 90% \t train_loss: 0.12 took: 2.71s\n",
            "Validation loss = 0.32\n",
            "Epoch 47, 10% \t train_loss: 0.12 took: 2.77s\n",
            "Epoch 47, 20% \t train_loss: 0.12 took: 2.72s\n",
            "Epoch 47, 30% \t train_loss: 0.14 took: 2.68s\n",
            "Epoch 47, 40% \t train_loss: 0.12 took: 2.70s\n",
            "Epoch 47, 50% \t train_loss: 0.13 took: 2.71s\n",
            "Epoch 47, 60% \t train_loss: 0.13 took: 2.69s\n",
            "Epoch 47, 70% \t train_loss: 0.13 took: 2.73s\n",
            "Epoch 47, 80% \t train_loss: 0.12 took: 2.71s\n",
            "Epoch 47, 90% \t train_loss: 0.14 took: 2.71s\n",
            "Validation loss = 0.33\n",
            "Epoch 48, 10% \t train_loss: 0.13 took: 2.78s\n",
            "Epoch 48, 20% \t train_loss: 0.12 took: 2.71s\n",
            "Epoch 48, 30% \t train_loss: 0.12 took: 2.70s\n",
            "Epoch 48, 40% \t train_loss: 0.12 took: 2.72s\n",
            "Epoch 48, 50% \t train_loss: 0.13 took: 2.72s\n",
            "Epoch 48, 60% \t train_loss: 0.12 took: 2.69s\n",
            "Epoch 48, 70% \t train_loss: 0.11 took: 2.72s\n",
            "Epoch 48, 80% \t train_loss: 0.14 took: 2.70s\n",
            "Epoch 48, 90% \t train_loss: 0.13 took: 2.70s\n",
            "Validation loss = 0.34\n",
            "Epoch 49, 10% \t train_loss: 0.12 took: 2.81s\n",
            "Epoch 49, 20% \t train_loss: 0.13 took: 2.73s\n",
            "Epoch 49, 30% \t train_loss: 0.12 took: 2.72s\n",
            "Epoch 49, 40% \t train_loss: 0.11 took: 2.70s\n",
            "Epoch 49, 50% \t train_loss: 0.13 took: 2.72s\n",
            "Epoch 49, 60% \t train_loss: 0.13 took: 2.74s\n",
            "Epoch 49, 70% \t train_loss: 0.13 took: 2.69s\n",
            "Epoch 49, 80% \t train_loss: 0.12 took: 2.70s\n",
            "Epoch 49, 90% \t train_loss: 0.13 took: 2.71s\n",
            "Validation loss = 0.35\n",
            "Epoch 50, 10% \t train_loss: 0.10 took: 2.78s\n",
            "Epoch 50, 20% \t train_loss: 0.12 took: 2.75s\n",
            "Epoch 50, 30% \t train_loss: 0.13 took: 2.84s\n",
            "Epoch 50, 40% \t train_loss: 0.13 took: 2.71s\n",
            "Epoch 50, 50% \t train_loss: 0.12 took: 2.69s\n",
            "Epoch 50, 60% \t train_loss: 0.13 took: 2.73s\n",
            "Epoch 50, 70% \t train_loss: 0.13 took: 2.69s\n",
            "Epoch 50, 80% \t train_loss: 0.13 took: 2.72s\n",
            "Epoch 50, 90% \t train_loss: 0.12 took: 2.72s\n",
            "Validation loss = 0.31\n",
            "Training Finished, took 1429.27s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BOOrUQC3oVs",
        "colab_type": "code",
        "outputId": "4f3d778c-efd8-4c93-be9f-28430c55ef44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "plot_losses(train_history, val_history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfEAAAGDCAYAAAA72Cm3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdd3yV9dnH8c+VTSZZrIS99wqgoghO\nFAW3olWpdda6WrXaoVjr0z7Vp46qba1VrFVx454IoqKyVGTPAGGGBLJ3fs8f52SShAA5OQn5vl+v\nvMy57/v87uucHLnOb5tzDhEREWl9AvwdgIiIiBweJXEREZFWSklcRESklVISFxERaaWUxEVERFop\nJXEREZFWSklc/MbMnJn1OcznnmBma5s6pkbct7+ZfW9mOWZ2cyOfc9iv0xfMbKWZTWzqa/3JF++x\nmfXwlhvkffyBmV3ZmGsP416/MbOnjyTeesqdYWZfNnW50nIc1gdO2hYzSwU6AmXVDs9yzv2iGWNw\nQF/n3AYA59wXQP/mun81dwLznHMj6jppZvOB/zrnfPEPcg9gMxDsnCs93HKcc4N9ce3Rzjl3RlOU\n4/1S9F/nXHK1sv+nKcqWtkdJXBrrbOfcp/4OogXoDsz2dxD1MbOgI0nwItK6qDldDpuZhZrZfjMb\nUu1YopkVmFkH7+NrzGyDmWWa2dtm1qWesuab2dXVHlc2A5rZAu/hH8ws18wuNrOJZpZW7fqB3jL2\ne5uAp1Y7N8vMnjCz97zN4N+aWe8GXtdUbxn7vWUO9B7/DJgEPO6No1+t5z0AnFDt/OPVTp9iZuu9\nZT5hZlbteVeZ2Woz22dmH5lZ93pCq3gf9nvLP9b7Pn1lZg+bWQYw08x6m9lnZpZhZnvN7AUza1/t\nfqlmdor395lm9oqZ/cf73qw0s5TDvHaUmX3nPfeqmb1sZn+s5z1uTIy3m9lyM8vylhVW7fwdZrbT\nzHaY2VX1vF94PytLah27zcze9v4+xRtztpltM7OZDZRV+Rk1s0Aze8gb+yZgSq1rf+r9m+aY2SYz\nu857PAL4AOji/RvmmlkX73v732rPr/Mz2Jj3piFmdpyZLfY+b7GZHVft3AxvrDlmttnMLvMe72Nm\nn3ufs9fMXm7MvaSZOOf0o58Gf4BU4JR6zj0DPFDt8Y3Ah97fTwL2AqOAUOBvwIJq1zqgj/f3+cDV\n1c7NAL6s61rv44lAmvf3YGAD8BsgxHvfHKC/9/wsIAMYi6f16QVgdj2vpx+QB5zqLfdOb9khdcVZ\nx/MPOO+N/V2gPdANSAcme89N85Y/0Bvb74CF9ZTdw1tWUK33qRS4yfv8dkAfb/yhQCKe5P9IXX9P\nYCZQCJwJBAJ/Ar451Gu97/sW4Bbv+3YeUAz8sZ7X0pgYFwFdgDhgNXC999xkYDcwBIgAXqz9+ahW\nTrj3s9C32rHFwCXVPkdD8VRohnnLPaeu97v63xa4HlgDdPXGN6/WtVOA3oABJwL5wKjan91qMc3E\n08QOB/8M1vve1PH6Z+D9/8h77T7gcjyflenex/He9zGbqv9nOgODvb+/BPzW+x6FAcf7+98k/VT9\nqCYujTXHWyuo+LnGe/xF4JJq113qPQZwGfCMc26Zc64IuBs41jx9u03pGCAS+LNzrtg59xmepDm9\n2jVvOucWOU9T8wtAnX3awMXAe865T5xzJcBDeBLjcfVc31h/ds7td85txfMPfsX9rwf+5Jxb7Y3t\nf4ARDdTG67LDOfc351ypc67AObfBG3+Rcy4d+CueRFKfL51z7zvnyoDngeGHce0xeBLDY865Eufc\nG3gSTZ0aGeNjzrkdzrlM4B2q3rOLgGedcyucc3l4EmB998kH3sL7WTCzvsAA4G3v+fnOuR+dc+XO\nueV4ElZD71WFi/B86djmje9Pte77nnNuo/P4HPgYTytNYzTmM1jfe9OQKcB659zz3s/KS3i+iJzt\nPV8ODDGzds65nc65ld7jJXi6kbo45wqdcxoo14IoiUtjneOca1/t51/e4/OAcDMb503OI4A3vee6\n4KmdAeCcy8VTI05q4ti6ANucc+XVjm2pdZ9d1X7Px5P06yureszlwDaOPOb67t8deLTiyxGQiaf2\ndij321b9gZl1NLPZZrbdzLKB/wIJhxBbmNU/yrq+a7sA251z1XdUqhHXYcRY33vWpVbZW2jYi1R9\nobsUmONN7ng/t/PMLN3MsvB8qWrovarQYAxmdoaZfWOebqT9eFovGlNuRdkH+ww29vNcb7nV4k7y\nfhm6GM/r32merqcB3mvuxPOZXORt4q+3+0Kan5K4HBFvjewVPP9ITgfedc7leE/vwJOkgMr+wHhg\nex1F5eFp+qzQ6RDC2AF0NbPqn+du9dynMWVVj9nwNJk2tqxD3RZwG3BdrS9I7ZxzCw+h7NrH/8d7\nbKhzLhr4CZ5/hH1pJ5Dkfb8qdG3g+iOJcWetsrsd5PpPgEQzG4HnM/pitXMv4qmVd3XOxQD/aGQc\n9cZgZqHA63hq0B2dc+2B96uVe7DPyJF+BhtVrlfl/yfOuY+cc6fiaUpfA/zLe3yXc+4a51wX4Drg\nSWtBUybbOiVxaQov4vkWfxk1/4F8CfipmY3w/sP2P8C3zrnUOsr4HjjPzMK9/0D8rNb53UCveu7/\nLZ7ayJ1mFmyeKTxnc3ijyF8BppjZyWYWDPwKKALqSqp1aSjOuvwDuNvMBgOYWYyZXVjPtel4mjwP\nVn4UkAtkmVkScMchxHO4vsYzBfEXZhZkZtPwjEHwRYyvADPMbJCZhQP3NnSxt0n6VeBBPP3Cn9SK\nI9M5V2hmY/HU1Bsbw81mlmxmscBd1c6F4OnrTwdKzewM4LRq53cD8WYW00DZR/IZrM/7QD8zu9T7\nN7oYGAS8620Zmeb9ol2E529TDmBmF5pZxXS4fXi+hJTXUb74gZK4NNY71UbT5ppZRZM5zrlv8dSk\nu+AZeVtx/FPg93hqJTvxDPS5hLo9jGcg1G7gOTz91tXNBJ7zNjtfVP2Ec64YT9I+A89AuieBK5xz\naw71RTrn1uKpFf7NW9bZeKbXFTeyiEeBC8wz0vyxRtzvTeB/gdneZuUV3tdR17X5wAPAV9734Zh6\nir0Pz2DCLOA94I1Gxn7YvO/PeXi+fO3H8x6+iychNGmMzrkPgEeAz/AM+PqsEU97ETgFeNXVnIL3\nc+APZpYD3IMngTbGv4CPgB+AZVSL39sSdbO3rH14vhi8Xe38GjxfcDd5/441Zmw0wWewTs65DOAs\nPF8KMvA0k5/lnNuLJxf8Ek9tPRPPuIAbvE8dA3xrZrne13GLc27TkcQiTcdqdmGJiDQNM/sW+Idz\n7ll/xyJytFJNXESahJmdaGadvE21V+KZsvWhv+MSOZppxTYRaSr98TQhRwCbgAucczv9G5LI0U3N\n6SIiIq2UmtNFRERaKSVxERGRVqrV9YknJCS4Hj16+DsMERGRZrN06dK9zrnE2sdbXRLv0aMHS5Ys\nOfiFIiIiRwkzq3N5YTWni4iItFJK4iIiIq2UkriIiEgr1er6xEVEpPFKSkpIS0ujsLDQ36FII4SF\nhZGcnExwcHCjrlcSFxE5iqWlpREVFUWPHj2ouVOstDTOOTIyMkhLS6Nnz56Neo6a00VEjmKFhYXE\nx8crgbcCZkZ8fPwhtZooiYuIHOWUwFuPQ/1bKYmLiIjPZGRkMGLECEaMGEGnTp1ISkqqfFxc3PAW\n6UuWLOHmm28+6D2OO+64Jol1/vz5nHXWWU1SVnNRn7iIiPhMfHw833//PQAzZ84kMjKS22+/vfJ8\naWkpQUF1p6KUlBRSUlIOeo+FCxc2TbCtkGriIiLSrGbMmMH111/PuHHjuPPOO1m0aBHHHnssI0eO\n5LjjjmPt2rVAzZrxzJkzueqqq5g4cSK9evXiscceqywvMjKy8vqJEydywQUXMGDAAC677DIqdup8\n//33GTBgAKNHj+bmm28+aI07MzOTc845h2HDhnHMMcewfPlyAD7//PPKloSRI0eSk5PDzp07mTBh\nAiNGjGDIkCF88cUXTf6e1Uc1cRGRNqLH3e/5pNzUP0055OekpaWxcOFCAgMDyc7O5osvviAoKIhP\nP/2U3/zmN7z++usHPGfNmjXMmzePnJwc+vfvzw033HDAVKzvvvuOlStX0qVLF8aPH89XX31FSkoK\n1113HQsWLKBnz55Mnz79oPHde++9jBw5kjlz5vDZZ59xxRVX8P333/PQQw/xxBNPMH78eHJzcwkL\nC+Opp57i9NNP57e//S1lZWXk5+cf8vtxuNp0El+wLp0vN+xlYv9Ejuud4O9wRETajAsvvJDAwEAA\nsrKyuPLKK1m/fj1mRklJSZ3PmTJlCqGhoYSGhtKhQwd2795NcnJyjWvGjh1beWzEiBGkpqYSGRlJ\nr169KqdtTZ8+naeeeqrB+L788svKLxInnXQSGRkZZGdnM378eH75y19y2WWXcd5555GcnMyYMWO4\n6qqrKCkp4ZxzzmHEiBFH9N4cijadxBenZvLUF5sIDwlUEheRo97h1Jh9JSIiovL33//+90yaNIk3\n33yT1NRUJk6cWOdzQkNDK38PDAyktLT0sK45EnfddRdTpkzh/fffZ/z48Xz00UdMmDCBBQsW8N57\n7zFjxgx++ctfcsUVVzTpfevTpvvEYyNCANiX3/AISRER8Z2srCySkpIAmDVrVpOX379/fzZt2kRq\naioAL7/88kGfc8IJJ/DCCy8Anr72hIQEoqOj2bhxI0OHDuXXv/41Y8aMYc2aNWzZsoWOHTtyzTXX\ncPXVV7Ns2bImfw31adNJPN6bxDPylMRFRPzlzjvv5O6772bkyJFNXnMGaNeuHU8++SSTJ09m9OjR\nREVFERMT0+BzZs6cydKlSxk2bBh33XUXzz33HACPPPIIQ4YMYdiwYQQHB3PGGWcwf/58hg8fzsiR\nI3n55Ze55ZZbmvw11McqRu41ecFmzwBnAXucc0PquWYi8AgQDOx1zp14sHJTUlJcU+0nvmBdOlc8\nu4jxveN54epjmqRMEZGWZPXq1QwcONDfYfhdbm4ukZGROOe48cYb6du3L7fddpu/w6pTXX8zM1vq\nnDtgvp0va+KzgMn1nTSz9sCTwFTn3GDgQh/GUqe4SNXERUTagn/961+MGDGCwYMHk5WVxXXXXefv\nkJqEzwa2OecWmFmPBi65FHjDObfVe/0eX8VSn3j1iYuItAm33XZbi615Hwl/9on3A2LNbL6ZLTWz\neofymdm1ZrbEzJakp6c3WQCx4Z4knplXjK+6FURERHzFn0k8CBgNTAFOB35vZv3qutA595RzLsU5\nl5KYmNhkAYQFBxIREkhJmSOnqOkHU4iIiPiSP5N4GvCRcy7PObcXWAAMb+4gKqeZqV9cRERaGX8m\n8beA480syMzCgXHA6uYOQtPMRESktfJZEjezl4Cvgf5mlmZmPzOz683segDn3GrgQ2A5sAh42jm3\nwlfx1KeiX1w1cRGRpjdp0iQ++uijGsceeeQRbrjhhnqfM3HiRCqmEp955pns37//gGtmzpzJQw89\n1OC958yZw6pVqyof33PPPXz66aeHEn6dWtKWpb4cnX7QFeadcw8CD/oqhsaIU01cRMRnpk+fzuzZ\nszn99NMrj82ePZu//OUvjXr++++/f9j3njNnDmeddRaDBg0C4A9/+MNhl9VStekV26AqiWuamYhI\n07vgggt47733KC72/BubmprKjh07OOGEE7jhhhtISUlh8ODB3HvvvXU+v0ePHuzduxeABx54gH79\n+nH88cdXblcKnjngY8aMYfjw4Zx//vnk5+ezcOFC3n77be644w5GjBjBxo0bmTFjBq+99hoAc+fO\nZeTIkQwdOpSrrrqKoqKiyvvde++9jBo1iqFDh7JmzZoGX5+/tyxt0xugQFUSz1RNXESOcinPDfVJ\nuUuu/LHec3FxcYwdO5YPPviAadOmMXv2bC666CLMjAceeIC4uDjKyso4+eSTWb58OcOGDauznKVL\nlzJ79my+//57SktLGTVqFKNHjwbgvPPO45prrgHgd7/7Hf/+97+56aabmDp1KmeddRYXXHBBjbIK\nCwuZMWMGc+fOpV+/flxxxRX8/e9/59ZbbwUgISGBZcuW8eSTT/LQQw/x9NNP1/v6/L1laZuviccr\niYuI+FRFkzp4mtIr9vN+5ZVXGDVqFCNHjmTlypU1+q9r++KLLzj33HMJDw8nOjqaqVOnVp5bsWIF\nJ5xwAkOHDuWFF15g5cqVDcazdu1aevbsSb9+nlnNV155JQsWLKg8f9555wEwevToyk1T6vPll19y\n+eWXA3VvWfrYY4+xf/9+goKCGDNmDM8++ywzZ87kxx9/JCoqqsGyG6PN18RjlcRFpI1oqMbsS9Om\nTeO2225j2bJl5OfnM3r0aDZv3sxDDz3E4sWLiY2NZcaMGRQWFh5W+TNmzGDOnDkMHz6cWbNmMX/+\n/COKt2I70yPZyrS5tixVTVwD20REfCoyMpJJkyZx1VVXVdbCs7OziYiIICYmht27d/PBBx80WMaE\nCROYM2cOBQUF5OTk8M4771Sey8nJoXPnzpSUlFRuHwoQFRVFTk7OAWX179+f1NRUNmzYAMDzzz/P\niScedP+tOvl7y1LVxDXFTETE56ZPn865555b2axesXXngAED6Nq1K+PHj2/w+aNGjeLiiy9m+PDh\ndOjQgTFjxlSeu//++xk3bhyJiYmMGzeuMnFfcsklXHPNNTz22GOVA9oAwsLCePbZZ7nwwgspLS1l\nzJgxXH/99Yf1umbOnMlVV13FsGHDCA8Pr7Fl6bx58wgICGDw4MGcccYZzJ49mwcffJDg4GAiIyP5\nz3/+c1j3rM5nW5H6SlNuRQqQVVDC8D98TFRoED/OPP3gTxARaUW0FWnr01K2Im0VokKDCAwwcopK\nKS4t93c4IiIijdbmk3hAgFU1qWuuuIiItCJtPokDxEUEAxqhLiIirYuSOFrwRUSObq1t7FNbdqh/\nKyVxID7CMycwU83pInKUCQsLIyMjQ4m8FXDOkZGRQVhYWKOf0+anmAHEhnub03OVxEXk6JKcnExa\nWhrp6en+DkUaISwsjOTk5EZfryQOxHlr4lrwRUSONsHBwfTs2dPfYYiPqDmdqoFtGp0uIiKtiZI4\nVTVxDWwTEZHWREkcjU4XEZHWSUkcJXEREWmdlMSBOO+KbZpiJiIirYmSOBBbMbAtr1hzKUVEpNVQ\nEgdCgwKJCg2itNyRXXh4G8CLiIg0NyVxr1j1i4uISCujJO5VNbityM+RiIiINI6SuFfl4La8Ej9H\nIiIi0jhK4l5xkaqJi4hI66Ik7lU1zUw1cRERaR2UxL3UJy4iIq2NkrhXVRJXTVxERFoHJXEv1cRF\nRKS1URL3Uk1cRERaG58lcTN7xsz2mNmKg1w3xsxKzewCX8XSGFVTzFQTFxGR1sGXNfFZwOSGLjCz\nQOB/gY99GEejVE0x04ptIiLSOvgsiTvnFgCZB7nsJuB1YI+v4misqNAgggONvOIyCkvK/B2OiIjI\nQfmtT9zMkoBzgb/7K4bqzIxYb5P6Pm1JKiIirYA/B7Y9AvzaOVd+sAvN7FozW2JmS9LT030WUJw2\nQRERkVYkyI/3TgFmmxlAAnCmmZU65+bUvtA59xTwFEBKSorPNvxWEhcRkdbEb0ncOdez4nczmwW8\nW1cCb06x4UriIiLSevgsiZvZS8BEIMHM0oB7gWAA59w/fHXfIxGvmriIiLQiPkvizrnph3DtDF/F\ncSgqmtM1sE1ERFoDrdhWTUUSz1BNXEREWgEl8Wo0sE1ERFoTJfFqlMRFRKQ1URKvRklcRERaEyXx\nauK0YpuIiLQiSuLVxFaOTi+hvNxna8qIiIg0CSXxaoIDA4gKC6Ks3JFdqH3FRUSkZVMSryVe08xE\nRKSVUBKvpXLBFyVxERFp4ZTEa9GCLyIi0looideiaWYiItJaKInXUrmTmaaZiYhIC6ckXkvlTma5\nSuIiItKyKYnXEqudzEREpJVQEq9FU8xERKS1UBKvpaJPXFPMRESkpVMSryU+MhRQTVxERFo+JfFa\n4tQnLiIirYSSOOBc1WYnESGBhAQGkF9cRmFJmR+jEhERaVibTuLPrXiGM189mdfXvlJ5zMy0apuI\niLQKbTqJO1fOnvw9pOVsq3E8Vuuni4hIK9Cmk3hSVDLAAUlc08xERKQ1aNNJPDmqKwDbc9NqHNc0\nMxERaQ3adBJPiqyqiVcf3KaauIiItAZtOolHh8YQFRJFQWkB+wozK49r6VUREWkN2nQSh6om9er9\n4hqdLiIirUGbT+IVTerV+8XjNTpdRERagTafxCsHt+VUJfGK5vRMJXEREWnB2nwSr2uaWdXAtiK/\nxCQiItIYbT6J11kTr5hill/il5hEREQaw2dJ3MyeMbM9ZrainvOXmdlyM/vRzBaa2XBfxdKQqpp4\n9SQeDHhGp5eVuzqfJyIi4m++rInPAiY3cH4zcKJzbihwP/CUD2OpV4fwjgRaEOkFeygsLQQgKDCA\nmHbBOAdZBaqNi4hIy+SzJO6cWwBkNnB+oXNun/fhN0Cyr2JpSFBAEF0iuwCwI3d75fH4ysFt6hcX\nEZGWqaX0if8M+MBfN69oUq97hLpq4iIi0jIF+TsAM5uEJ4kf38A11wLXAnTr1q3JY6hrDfU41cRF\nRKSF82tN3MyGAU8D05xzGfVd55x7yjmX4pxLSUxMbPI4qq+hXqGyOV0j1EVEpIXyWxI3s27AG8Dl\nzrl1/ooDGp5mppq4iIi0VD5rTjezl4CJQIKZpQH3AsEAzrl/APcA8cCTZgZQ6pxL8VU8DWlwwZdc\nrdomIiItk8+SuHNu+kHOXw1c7av7H4qKJL4jdzvlrpwAC9BOZiIi0uK1lNHpfhURHEFsWBxFZUVk\nFOwFtJOZiIi0fEriXrUHt8WFayczERFp2ZTEvZJrzRWPi9ROZiIi0rIpiXvVHtxWURPPVJ+4iIi0\nUEriXhXN6RULvoSHBBIaFEBhSTn5xaX+DE1ERKROSuJeFXPFK2riZlZt/XTVxkVEpOVREvdqeP10\nJXEREWl5lMS9EsM7EBIQQmZhJvkl+YCmmYmISMumJO4VYAF0iUoCYLummYmISCugJF5N7d3M4tSc\nLiIiLZiSeDVVC77USuKaZiYiIi2Qkng1VYPbvM3pqomLiEgLpiReTdU0MzWni4hIy6ckXk3tBV/i\nI0IB2JOjPcVFRKTlURKvJsk7On1H7nbKysvolRgBwPrdOZSXO3+GJiIicgAl8WrCgtqR0C6R0vJS\n9uTvJiEylI7RoeQVl5Gakefv8ERERGpQEq+lcpqZt198cOcYAFbuzPZbTCIiInVREq+l9m5mg7tE\nA7Byh5K4iIi0LEritdQe3FaRxFcpiYuISAujJF5LclTNBV8Gd/E0p6/amYVzGtwmIiIth5J4LUmV\nfeKe5vTk2HZEhQWxN7dYU81ERKRFURKvpaImXtGcbmYM6lzRL57lt7hERERqUxKvJS4snrCgdmQV\nZZFT7OkHr2hS1+A2ERFpSZTEazEzkiIrtiStObhNSVxERFoSJfE6VK2hXnOa2SrNFRcRkRZESbwO\nSbVGqPdOjCQkKICtmflkF5b4MzQREZFKSuJ1qFy1zTu4LTgwgAEdowDNFxcRkZZDSbwOlQu+eJvT\nQf3iIiLS8iiJ16H2+umAppmJiEiLoyReh86RXTCMXXm7KC339IEPqly5TTVxERFpGZTE6xASGEKH\niI6UuTJ25e4CYGDnKMxgw55cCkvK/ByhiIiID5O4mT1jZnvMbEU9583MHjOzDWa23MxG+SqWw1F7\nmll4SBC9EiIoLXes35Prz9BEREQA39bEZwGTGzh/BtDX+3Mt8HcfxnLIKga3peVW9YtXrdymfnER\nEfE/nyVx59wCILOBS6YB/3Ee3wDtzayzr+I5VHUNbtMIdRERaUn82SeeBGyr9jjNe+wAZnatmS0x\nsyXp6enNE1zlgi9VIWqEuoiItCStYmCbc+4p51yKcy4lMTGxWe5Ze8EXqGpOX70zh7Jy7S0uIiL+\n5c8kvh3oWu1xsvdYi1B9wRfnPAk7LiKEzjFhFJSUkZqR58/wRERE/JrE3wau8I5SPwbIcs7t9GM8\nNcSExhARHEleSR5ZRfsrj6tfXEREWgpfTjF7Cfga6G9maWb2MzO73syu917yPrAJ2AD8C/i5r2I5\nHGZWbZpZ9ZXbNEJdRERahiBfFeycm36Q8w640Vf3bwpJUcmszVxNWs42hiQOBaptS6qauIiI+Fmr\nGNjmL8neEeo1B7d5m9N3Zlf2lYuIiPiDkngDKhd8qTbNLKl9O2LaBZOZV8yu7EJ/hSYiIqIk3pCK\nueJbslIrj5lZ5XxxNamLiIg/KYk3YGD8YAxjdcZKCkryK49rhLqIiLQESuINiAmNYWD8IErKS/h+\nz3eVx6v6xTVCXURE/EdJ/CDGdB4HwKKd31Yeq9oIRTVxERHxHyXxg6hI4ot3flN5rFdCBKFBAaTt\nKyCroMRfoYmISBunJH4QIzqMIiQghLWZa9hf6Fm5LSgwgAGdtBmKiIj4l5L4QYQFhTGswwgcjiW7\nFlUeH1Sx6MtONamLiIh/KIk3wtjOxwCwuEa/uKaZiYiIfzUqiZtZhJkFeH/vZ2ZTzSzYt6G1HGMr\nB7dV9YsP7qxpZiIi4l+NrYkvAMLMLAn4GLgcmOWroFqaAfGDiAiOZFvOVnbm7vAc6xRNgMGG9FwK\nS8r8HKGIiLRFjU3i5pzLB84DnnTOXQgM9l1YLUtQQBApncYAVVPN2oUE0jsxkrJyx9pdOf4MT0RE\n2qhGJ3EzOxa4DHjPeyzQNyG1THVNNau+GYqIiEhza2wSvxW4G3jTObfSzHoB83wXVstTfXBbxe5l\nVWuoa5qZiIg0v0btJ+6c+xz4HMA7wG2vc+5mXwbW0vSM6UVCu0T2FqSzcf8G+sT21cptIiLiV40d\nnf6imUWbWQSwAlhlZnf4NrSWxcyqNal7+sUrmtNX78qmtKzcb7GJiEjb1Njm9EHOuWzgHOADoCee\nEeptyjhvk3rF4Lb24SH0SoigsKScpVv3+TM0ERFpgxqbxIO988LPAd52zpUAzndhtUwVNfFluxdT\nWl4KwKmDOgLw8crdfotLRPcJHAMAACAASURBVETapsYm8X8CqUAEsMDMugNtriO4Y0QnukX3IK8k\nj1V7VwBw6kBvEl+1q3LAm4iISHNoVBJ3zj3mnEtyzp3pPLYAk3wcW4s0rtbqbSO7xZIQGcK2fQWs\n0XxxERFpRo0d2BZjZn81syXen//DUytvc8bU6hcPDDBOqayNq0ldRESaT2Ob058BcoCLvD/ZwLO+\nCqolG91pDIbxY/oPFJTkA3DaoKomdRERkebS2CTe2zl3r3Nuk/fnPqCXLwNrqWJCYxgYP4iS8hK+\n3/MdAMf1TiA8JJCVO7LZvr/AzxGKiEhb0dgkXmBmx1c8MLPxQJvNVmMq+8U9TephwYFM7JcIwCeq\njYuISDNpbBK/HnjCzFLNLBV4HLjOZ1G1cFVLsFato37aoE6A+sVFRKT5NHZ0+g/OueHAMGCYc24k\ncJJPI2vBhncYSUhACGsz17C/cD8AkwZ0ICjA+HZzJvvzi/0coYiItAWNrYkD4JzL9q7cBvBLH8TT\nKoQFhTGswwgcjiW7FgEQ0y6YY3rFU1bu+GzNHj9HKCIibcEhJfFarMmiaIXGVk41q96krqlmIiLS\nfI4kibfp5cnG1toMBaicL/75unQKS8r8EpeIiLQdDSZxM8sxs+w6fnKALgcr3Mwmm9laM9tgZnfV\ncb6bmc0zs+/MbLmZnXkEr6VZDYgfRGRwFNtytrIzdwcAXdq3Y2hSDAUlZXy5Ya+fIxQRkaNdg0nc\nORflnIuu4yfKOdfgXuRmFgg8AZwBDAKmm9mgWpf9DnjFO1DuEuDJw38pzSsoIIjRnVKAqqlmoIVf\nRESk+RxJc/rBjAU2eBeHKQZmA9NqXeOAaO/vMcAOH8bT5Kr2Fz9wqtnc1XsoK2/TPQ4iIuJjvkzi\nScC2ao/TvMeqmwn8xMzSgPeBm3wYT5Mb1/lYwNMvXrGDWb+OkXSPCycjr5hl2mNcRER8yJdJvDGm\nA7Occ8nAmcDzZnZATGZ2bcXmK+np6c0eZH16xPSkY3hHMgoz+DH9BwDMjNMGq0ldRER8z5dJfDvQ\ntdrjZO+x6n4GvALgnPsaCAMSahfknHvKOZfinEtJTEz0UbiHzsw4pcfpAHy0+YPK45Wrt63crT3G\nRUTEZ3yZxBcDfc2sp5mF4Bm49nata7YCJwOY2UA8SbzlVLUb4fSengH1n6Z+RGl5KQCjusUSHxHC\nlsx81u3O9Wd4IiJyFPNZEnfOlQK/AD4CVuMZhb7SzP5gZlO9l/0KuMbMfgBeAma4VlZ1HRg/iK5R\n3cgozGDZ7iVA7T3G1aQuIiK+4dM+cefc+865fs653s65B7zH7nHOve39fZVzbrxzbrhzboRz7mNf\nxuMLZsbpPc8Aajepa/U2ERHxLX8PbDsqnOZN4p9t+YTiMs/mJ+P7ePYY/3F7Fju0x7iIiPiAkngT\n6NW+N31j+5FTnMPX278CPHuMT+jrGYT36WrVxkVEpOkpiTcRNamLiEhzUxJvIhVN6gvS5lNQkg/A\nSQM6EBhgfLMpg6yCEn+GJyIiRyEl8SbSJTKJoYnDKCwtYEHa5wC0Dw9hXM84Sssdry7ZdpASRERE\nDo2SeBOqmDP+0eb3K49dfXxPAB77bD2ZecV+iUtERI5OSuJN6JQepxNgASzc/iXZRVkATOrfgRP6\nJpBdWMrDn67zc4QiInI0URJvQgntEkjpNIbS8lI+2zoX8Mwj//2UQQQYvPDtFtbuyvFzlCIicrRQ\nEm9ip/XwDHD7uNoo9X4do7hsXHfKHfzxvVVaT11ERJqEkngTO6n7KQQFBLFk1yL2FuytPH7bKf2I\nDgviiw17mbd2jx8jFBGRo4WSeBOLDo3h2C7HU+7K+TT1o8rjcREh3HJyPwD++N5qikvL/RWiiIgc\nJZTEfaBi4ZfqTeoAlx/TnV4JEWzam8fz32zxR2giInIUURL3gQldJxIaGMby9B/YkVu1hXpIUAC/\nPXMgAI/OXacpZyIickSUxH0gPDicCV0nAvDx5g9rnDtpQAdO6OOZcvaIppyJiMgRUBL3kfqa1M2M\n31VMOVu0lXW7NeVMREQOj5K4jxyXdDxRIVGs27eWzfs31TjXv1MUl47tRlm5435NORMRkcOkJO4j\nIYEhTOp2ClBzGdYKt53Sj6iwIL5YrylnIiJyeJTEfei0npMBz/aktWvb8ZGh3HJyX8Az5aykTFPO\nRETk0CiJ+1BKp7HEhcWxLWcrS3cvOeD8Fcf0oGe8Z8rZrIWpzR+giIi0akriPhQUEMS0vucD8LsF\nv2ZP3u4a50OCAvj9WZ4pZw9+vFbrqouIyCFREvexa4ffwKiOKewtSOf2ebdQWFpQ4/xJAzpycUpX\nikvLueXl7ygsKfNTpCIi0tooiftYcGAwf5n4V5Iik1iVsZL7vrrngP7xe84aRI/4cNbsyuHBj9b6\nKVIREWltlMSbQfuwWP560uNEBEfwSeqH/Hv5P2ucjwgN4tGLRxIUYPz7q80sWJfup0hFRKQ1URJv\nJr1j+/DAhL9gGP/4/gnmbvmkxvnhXdtz6yme0eq3v/aDlmQVEZGDUhJvRscnT+Dm0b8E4J4vfsOa\njNU1zt9wYh/G9IhlT04Rd72xXIvAiIhIg5TEm9lPBl/J2b2nUVRWyK8+u4m9+VVN54EBxsMXjSAq\nNIiPV+3m5SXb/BipiIi0dErizczMuPvYexjeYSS783dz+7xbKSorqjyfHBvOH88ZAsB976xiU3qu\nv0IVEZEWTkncD0ICQ3hw4sN0iujMir3L+ePCe2s0nU8bkcS04V0oKCnj1pe/12puIiJSJyVxP4lr\nF8/DJ/2NdkHt+GDTe9z86Q3M3fIJxWWeAW1/mDaEpPbtWL49S1uWiohInay1DZ5KSUlxS5YcuIRp\na/X51nnc/fntFJd7kndMaAyn9zyTs/tMIzurE9Of/gYHzL7mGMb1jPdvsCIi4hdmttQ5l3LAcV8m\ncTObDDwKBAJPO+f+XMc1FwEzAQf84Jy7tKEyj7YkDrCvMJMPNr3HOxvmsH5fVa27T2xfIkuPY+6y\nZDpEJDDn5+PpHNPOj5GKiIg/NHsSN7NAYB1wKpAGLAamO+dWVbumL/AKcJJzbp+ZdXDONbgv59GY\nxKtbm7mGdze8xQeb3mN/0T7PQRdAfubxdAu8mFeuO5bosGD/BikiIs2qviTuyz7xscAG59wm51wx\nMBuYVuuaa4AnnHP7AA6WwNuC/nED+NXYX/PBhXN5cOIjTOg6kYAAIzx+ARuzv+OG/y6luFQD3URE\nxLdJPAmoPtE5zXusun5APzP7ysy+8Ta/C5411yd1P5m/nvQ3bhj5CwBiOr/LVxv3aCEYEREB/D86\nPQjoC0wEpgP/MrP2tS8ys2vNbImZLUlPb3vril866HKSIpMheBdR8Yt447vt/PUTjVgXEWnrfJnE\ntwNdqz1O9h6rLg142zlX4pzbjKcPvW/tgpxzTznnUpxzKYmJiT4LuKUKDQzltjF3ANC+01wCg/L4\n27wNzF681c+RiYiIP/kyiS8G+ppZTzMLAS4B3q51zRw8tXDMLAFP8/omH8bUap3YdRLjOh9LQVku\nE1IWA/DbOSuYt7bNDyMQEWmzfJbEnXOlwC+Aj4DVwCvOuZVm9gczm+q97CMgw8xWAfOAO5xzGb6K\nqTUzM3419tcEWiAr9n/E9PEBlJU7bnxxGSu2Z/k7PBER8QOf9ok75953zvVzzvV2zj3gPXaPc+5t\n7+/OOfdL59wg59xQ59xsX8bT2vVq35uLBkzH4dgbOJtpwzuTX1zGT59bTNq+fH+HJyIizczfA9vk\nEF074gZiw+L4bs8yTk/ZxbG94knPKeLKZxexfX+Bv8MTEZFmpCTeykSFRPPzkTcB8MT3D/PwJYPo\n3zGKjel5TH38S77ZpN4IEZG2Qkm8FZra51z6xw1kd94u5mz4Dy9fewwn9E0gI6+Yy/79Lc9+tVnz\nyEVE2gAl8VYoMCCQO8beBcDzK2eRX57OrBljuW5CL8rKHfe9u4pfvfoDhSVlfo5URER8SUm8lRrR\ncRSTe55JUVkRjyz5PwIDjLvPGMjfpo+kXXAgb3y3nQv/+TU71E8uInLUUhJvxW4afRthQe34bMsn\nLNm5CICzh3XhjRuOo2tsO37cnsXZ6icXETlqKYm3Yh0jOvHToVcD8L/fPsCKdM+a6gM7R/POL45v\nsn5y5xxrMlazNXtLU4YvIiJHyKf7ifvC0b4V6aEqKivi4rfOJS3Hs9dMz5henNVnGmf2Oou4sET+\n8tEa/rnAswjeWcM686dzhxLViK1MnXOs3Psjn6Z+zNwtn7AzbwehgaE8N+VF+sT28+lrEhGRmpp9\nP3FfURI/0O68Xby46nk+2PQumYWZAARYAMd0OY6z+0wjd/8AfvfmGvKKy+gWF87j00cyLPmAfWYo\nd+WsSF/Op1s8iXt33q7Kc6GBYRSVFdIzphf/mfIS7YLDm+31iYi0dUribUBpeQkLt3/FOxve4ou0\n+ZSWlwIQFRJFSocT+XZjIXtziwgMcKT0iGVQ5ygcDufKKSwrYtGOr9mdv7uyvA7hHTip+6mc0v00\n+sUNYMZ7l7IpayNT+5zLPeP/4K+XKSLS5iiJtzH7C/fx4eb3eXfDW6zJXN3o53WM6MTJ3U/l5O6n\nMTRxGAFWNWxiw771XPnedIrKirj/hD9xRq+zfBG6iIjUoiTehq3PXMu3O7+hzJURQADrdufw7vJd\nFJSUEx0WwvmjkumVEMWA+IEMThhaI3HX9ua613jg6/sIDwrnv2e/Qrfo7s34SkRE2iYlcalh+/4C\nbp79HUu37CPA4NaT+3HjpD4EBliDz3PO8ZsFd/JJ6of0jxvIs2f+l5DAkGaKWkSkbaoviWuKWRuV\n1L4ds685hp9P7I0D/vrpOi74x0KWp+1v8Hlmxm+PvZekyGTWZq7m0SX/1zwBi4jIAZTE27DgwADu\nPH0A//npWDpEhfLdtv1Me/Ir7npjOXtzi+p9XmRIJH868SGCAoJ4ec2LzN86txmjFhGRCkriwgl9\nE5n7yxO5bkIvggKM2Yu3Men/5vPsV5spKSuv8zmDEgZz06jbAPjDV/ewK3dnc4YsIiIoiYtXVFgw\nd58xkA9vmcCEvonkFJZy37urmPK3L1i4cW+dz7l00OWckHwi2cXZ/GbBnZSWlzRz1CIibZuSuNTQ\nOzGS5346hqevSKFbXDjrdudy6dPfcsMLS0nbl1/jWjPj3vH30yG8A8vTv+ef3z/pp6hFRNomJXE5\ngJlxysCOfHzrBO44rT/tggP5YMUuTvq/z7nvnZWk51T1l7cPi+WBCX8hwAKY9eO/eXjxg+SV5Pkx\nehGRtkNJXOoVFhzIjZP68NmvTmTa8C4Ul5Xz7MJUJjw4jwc/WkNWgaf5fGTH0dyWcgdmxgur/sNF\nc6bx2ZZPD3vDlbqk5Wzjxo+v5bw3z2bz/k1NVq6ISGumeeLSaKt2ZvPXT9by6eo9AESHBXHthF78\n9LieRIQGsTpjJX/6+o+sylgBwPHJE7hj7N0kRSUf9j2dc7y+7lUeXfIQBaWevdHjw+L55+Rn6BHT\n68hflIhIK6DFXqTJLNu6j4c+XsvCjZ59yuMjQvj5xD5cNq4bwYHwxrpXeWLZY+SW5BAaGMbPhl3L\n5YNnEBx48N3TqtuVt4v7v7qHb3d+DcCpPSaTVbSPRTu/Jb5dAv88/d9tMpEXlxWzau8KhncYiVnD\ni/OIyNFBSVya3MKNe3nwo7V8t82zQEyn6DAuGdOVi1K6EhKax6NLHuKDTe8Bni1Sf33M70jpNOag\n5TrneHfjWzy06H/JK8klJrQ9dx3zW07tMZnC0gJum3sTi3dVJPJn6BHT06evsyUpKy/jts9uZOH2\nr7g15XZ+MvhKf4ckIs1ASVx8wjnHZ2v28ODHa1mzKweAAIMT+yVy8ZhuRMds5qHF/8PW7FQAkqO6\nMjB+EP3jBjIgfiAD4gbRPqxqW9S9+ek88PV9fJH2OQAndp3Eb469h/h2CZXXFJYWcOvcX7Bk1yIS\n2iXyz9OfoXtMj2Z7zf70+NJHmLXi3wBEBEcy57z3iA2L83NUIuJrSuLiU+Xljq83ZfDS4q18vHI3\nxd5FYhKjQjl3ZEdC2s/nrU3PV/ZrV9c5ogsD4gfSNbobb61/g6yiLCKDo7hj3N2c2eusOpuMC0ry\nufWzX7B012IS23Xgn5OfOeo3Y/k09SPu+vx2Ai2QHjE92bh/Axf2v4RfH/Nbf4cmIj6mJC7NJjOv\nmDe+S2P24m1s2JNbefyYXjGcPy6Y0HY7WJO5mjUZq1m3by2FtRL7cUnj+d2x99EhomOD9ykoyeeW\nuTeybPeSgyZy5xw783awaf9GBsQNJCE88chfaDPasG8dM97/CYWlBfxyzJ2M63ws0985H8OYPfUN\nerZve2MDRNoSJXFpds45lm3dx0uLt/Hu8h0Ulnhq56cP7shvzxxEt7hwysrL2JKdypqMVazft57+\ncf05veeZjR6w5UnkP2fZ7qV0CO/AP09/luSoruzJ383qjJWs2ruS1RmrWJWxkqwiT999aGAolwy8\njCuHXEV0aIzPXn9TySrK4op3L2F7bhpn9jqb+45/ADPjT1/fz+vrXuH45Ak8cvIT/g5TRHxISVz8\nKruwhOe/3sIT8zeQX1xGSGAAV5/Qkxsn9iEiNOiIys4vyeeWT2/guz3LiAltT5AFklGYccB17UNj\nSYpKZuXeHwGIDolmxtCruWjAdMKCwo4oBl8pKy/jlrk/55sdCxkYP4h/TX6uMtbMggzOfXMKeSV5\nPH7qPzmmy3F+jlZEfEVJXFqE3dmF/O+Ha3jju+0AdIgK5deTB3DuiCQCDrKXeUPyS/K5+dMb+H7P\nMsCToAfGD2ZQwmDPf+MH0zGiE2bGyr0r+NvSh1myaxEAHcM7cu2IG5nS+2yCAo7sC0WFclde7aeM\nMleOc+WV/40KiSYwIPCg5fxt6cM8t+IZYsPieH7KbDpFdq5xftaP/+bxZY/QJ7YvL5z1aqPKFJHW\nR0lcWpRlW/dx3zur+MG7f/mIru2596xBjOwWe9hlFpYWsnTXYrpH9yApKrnBJnnnHN/sWMjflj3C\nusw1gGca3I2jbubEricd1vzrvJI8nln+FK+seanOAXzVJbRL5MzeZ3F273Pq7c/+JPVD7v78DgIt\nkCdO+1ed0/OKyoq44M2p7Mzbwe+Onck5/c4/5LhFpOVTEpcWp7zc8eb32/nzh2sq12M/fXBHLhrd\nlRP7JRIU6PtVgctdOR9v/pC/f/cY23M9rQODE4ZyxZCfMrHrSY2q2Za7ct7f9C6PL32EvQXplccD\nLRAzI9ACCbCAyp9y58grqRrwNzhhKGf3nsZpPSdX9tGvz1zLTz+4nMLSAm4fexeXDLys3vt/vPlD\nfrPgDuLD4nnjvPeICI443LdDRFoovyRxM5sMPAoEAk875/5cz3XnA68BY5xzDWZoJfGjT25RKU/O\n38DTX2yunJqWEBnKuSO7cP6oZAZ0ivZ5DCVlJbyx7lWeXv5P9hVmApAUmcylgy5nap9zaBccXufz\nVu5dwUOL/sSP6csBGJIwjNvH3sWQxKH13ss5xw/p3/Puhrf4JPXDyg1jQgJCOLHbJE7rcQaPLHmI\n7blpTOk9lZnj/3jQVoWrPvgJP6Yv56qh1/DzUTcf7ttQKasoi9UZK2kXFE5kcCQRIZFEBkfQLihc\nTfYiftDsSdzMAoF1wKlAGrAYmO6cW1XruijgPSAE+IWSeNu1K6uQN75L47VlaWxKr9oJbUiXaC4Y\nnczU4UnERYT4NIaCknze2fgWL6z8D9tz0wBP//r5/S/m4gHTK6emZRTs5fFlj/LOhjkAxLdL4KbR\nt3Fmr7MIsMa3IBSWFjBv61ze3fA2i3Z+g6Pq/8faA9ka8mP6D/z0/Z8QGhjK6+e8c0DfeWMVlRUx\ne/ULPLP8XzVaC6qLCI4gIjiC6NAYJnU7mQv6X1xjMR4RaXr+SOLHAjOdc6d7H98N4Jz7U63rHgE+\nAe4AblcSF+cc323bz+vL0nj7hx3kFJYCEBxonDygIxeMTubEfokE+7C5vay8jM+3zeO/K2exPP0H\nz/0Dgpncawrdorsz68d/k1eSS1BAEJcOupyrhl5LZEjkEd1zV94u3t/4Du9tfJsyV8Y/Tn+GThGd\nGv3833x+Jx+nfsAZvaZw/wl1NnrVq6Jb4Yllj7IzbwcA/eMGEhwQTF5Jrvcnr85tZivel8sGXU6f\n2H6HdF8RaRx/JPELgMnOuau9jy8HxjnnflHtmlHAb51z55vZfOpJ4mZ2LXAtQLdu3UZv2bLFJzFL\ny1NYUsanq3fz2tI0FqxPp9z7cU2IDOGcEUlcMNr3ze0/7Pme/658jvlb59aoKR+fPIHbUu5oMUu+\n7sjdzgVvTqW4vJhZZ77YYJN+dd/tXsrDix+q3H2ud/s+3JpyO8cmjT/g2nJXTn5JPnkluWzJTuWV\n1S/x+bZ5le/L2M7HcNmgKzg2afwhtUiISMNaXBI3swDgM2CGcy61oSRenWribdfu7ELe/G47ry1L\nq7ESXHM1t2/L3sqLq55n4/71XDHkKo5PnuCzex2uirXVh3cYydOTn2uwL31r9hb+tvRh5m2dC3i6\nBG4YeRNn9552SP3e27K38tLq//LOhjmVo/J7xPTk0kGXc0bPKfWOJzhS+woz2Za9lX5xA1rsPH+R\nptLimtPNLAbYCFT8a9wJyASmNpTIlcTFOccPaVm8tnQbb/+wg+xqze0nDejAmUM6M2lAB6LDDm3r\n06NBbnEu5745hX2FmZzS/TTCgyMOmKde7sooKivi6+0LKXOlhAW14/LBM7h88AzCjyDhZhdl8eb6\n13ll9Yvszt8NQGhgGMd2OY6J3U7i+OQTa2x201glZSVsyd7M+n3rWJe5jg371rF+37rKmQATuk7k\n/yY9pm1Z5ajmjyQehGdg28nAdjwD2y51zq2s5/r5qCYuh6iiuf3VpWl8Ua25PTjQOLZXAqcN7sip\nAzvSMbrt1NReX/sKf/rm/oNeZxhn9zmH60fceNB16g9FaXkJn6Z+witrXqwcTwAQYAGM7DiaiV1P\n4sRuk+gSmVR5zjlHVtF+tmZvqfzZlrOV1KzNbM7aRGl56QH3CQ8Kp7S8lOLyYh6a9CgTu53UZK9B\npKXx1xSzM4FH8Ewxe8Y594CZ/QFY4px7u9a181ESlyOwK6uQd3/cwSerdrM4NbMyoYNnMZnTBnXk\n9MGd6J14ZAPQWjrnHHO3fExWUVbVHPWAQAItgACr+m+v9r19vhf7nrzdLNg2n/nbPmPxzkWUuapk\n3C9uAD1jerItexvbcraQU5xTZxmGkRzVlT6x/egb249+cf3oE9uPLpFJvLLmJR5a9Gc6RXTm1Wlz\nfNZ0L+JvWuxF2pTMvGLmrtnNx6t2s2BdOkWl5ZXneiVEcNKADpw8sCMp3WN9OspdquQUZ/NV2pd8\nvu0zvkr7gvzS/BrnI4Ij6BrVjW7RPegW3Y2u0d3pHt2dXu371NvMX1peypXvXcrazNXMGPIzfjH6\n1uZ4KSLNTklc2qz84lK+WL+Xj1ftZu7q3ewvKKk8Fx0WxIn9OnDKwA6c2C+R9uG+nYcuHsVlxSze\n+S2ZhRl0jepG1+huxIXFH1a/9or05fz0/Z8QGBDIS2e/rm1Z5aikJC4ClJaVs2zrfuau2c3cNXtq\njHIPDDBGd49lUv8OTOibwKDO0Ros1Uo88PV9vLnuNVI6jeXvpz2tv5scdZTEReqwJSOPT1fv4bM1\nu/l2cyal1TrSEyJDmdA3gQl9Ezm+bwIJkaF+jFQasr9wPxfMmcr+on388YQ/M7nXFH+HJNKklMRF\nDiK7sIQF69L5fF06C9anszu7qMb5IV2iOaFvIif0TWBk11jahWgN8ZbkrfVvcv/Ce4gPi+f1c98h\nMiTK3yEdVEFJPn///nG+2fE1YzqNZWrfc+kfN6DZ40jP38OSXYuIb5dA9+gedAjv2CJbM8rKywiw\ngGaPzTlHRsFeNuxbz4b96+kR07PZ14lQEhc5BM451u/J9ST19eks2pxZY3BcUIAxuEs0o7rFktIj\njtHdYukU03amsbVE5a6cqz+4kuXp33PJwMu4fexd/g6pQct2LeEPC+8hLWdbjeP94gYwtc85TO45\n5bDm1TdWWXkZ3+xYyJvrXuOLtM8pc2WV59oFtaNbdA+6R/ege0wPukd3p3tMD3q170NoYPO3SG3J\nSuW1tS/z7sa36BDekd8dN5OhicN9cq+Cknw27t9QmbA37FvP+n3ryCraX+O6GUN+xo2jbmm2LxRK\n4iJHoLCkjEWbM1mwPp2vN2Wwemd2jSlsAEnt2zG6eywp3n71rnGa7tTc1meu5SfvXozD8Z8psxkQ\nP9DfIR2goCSfx5c9ystrXgSgb2w/rhvxcxbvXMQHm94luzgb8KxJP7HbSUztcy5jOx/TZLvHpefv\n4a31b/LW+jcq18kPtCDGdRlHXkk+W7JS2V+0r87nBgcEMyhhCCM7jGJEx1EM7zCCqBDfLHtcWl7K\nF9s+59W1s1m085sa5wxj+qCfcMOIXxzRtELnHFuzt/Bj+g8sT/+BFenL2bB/PeWu/IBrI4Oj6BPb\nly6RSXy0+X3KXBnn9j2fu475fbPs7KckLtKE8opK+X7bfpZs2cfSLfv4bus+copqLkgyuEs0kwd3\nYvLgTvTt2PKbdo8Wf138F15c9TxDEobxzJnPH9Ya7s459haks2n/RtLz9zCqU0qNxWkO15Jdi7n/\nq3vYnptGoAVx1bBruGroNQQHelYXLCorYsG2ebyzYQ5fb19YuSZ9x/COnNVnGtP6nndYcZSVl/Ht\nzq95Y+2rNWrdSZFJnNPvAs7ucw4J1XaiyyrKYmt2KluyUtmSnUpqViqpWZtIzdpcY/8Aw+gd24cR\nHUYxosMohiYOo0tk0hHVTvfmpzNn/eu8se5V9uTvATwr/03udSbn9D2Pz7fO4/mVsyhzZSRFJvGb\nY+9lXJdjG1V2UVkRsmcvNQAAFY5JREFUP+z5juV7fuDH9B9YsXc5WUVZNa4JtEB6xPSkb2w/+sT2\npU+sZ12CjtW6GL7Y9jl3ff4risqKOLn7qdx/wp8JCfTtzBYlcREfKit3rN+Tw5It+/h6Ywbz1+4h\nr7iqebJ3YoQnoQ/pzJAuGvXuS7nFuVz4/+3deZDc5X3n8fe3u+fouXoOzT0aHSMxIKODIMvCUAaB\nSSBmgdgEm0pqUym2SGUdr9nN4nhNbbHrTby1dnaNialscOKs1+Vd7JiYxXGwkYUMtjmFJCQkoWNG\nGkmjuY+eo3v6mmf/6J9GMzqQRkxP063Pq6qrf7+nWz3ffkrdn35+1/PsPQxE+3n0hsf4navuu+Bz\nnXP0Rfo4OtpB52gHR8Od3n3HORef+Y366/lE2z3ctuz2ec9YF0lE+Ku3vs4/HHwagKuq2nnspj9/\nz/3fvZO9/KTjOX585NmZTe6GsalxM/de9UluXnrrewbHVHKKN3te5xcnXuSXJ37B8NQwkB5139y6\nhU+uvo9NTZvn9SNnLBZmz8Db7O7bye7+Xewb3EtiOjHnOaUFZayeuTBPO6ur2llVtYriQPCcPumb\n7KV3soe+yV76Ir0cGTnMyydemrkoUGvFcu5rv5+72u6moig082/fHdrPl195jEPD7wJw96rf4eGN\nfzrnOaeNTo3yq+6Xeen4dl479euZ6/ufVlNcw9ra9aytW8+62vVcU7PmnFrPZ2fvDv7ti59jMjHB\nRxpv4GtbHn9fly2+GIW4yCKaSqT49ZFBfrqvl637556b3lwZ5KZVS7i2OcS1TRVc01hBcYEOkltI\nLxz9KV96+RFCRSGeuffHhIoq6Y/00TnaQcfoETpnQrvjvNOrQnoe+ZWVq6goquD1U68RS00BUOQv\n4pbWW7mr7Z4LbuaedtMMRPo5NdFN11gXf7/nKbonuvFbgAfXPcQfrv1XM6Pvi3HOsbNvB88e/kde\n7NpKLJU+4LKyqIpPtP0L7l39qZlz40enRvnVyZd46cR2Xj31ClOzAqu5rIV7Vn+Su1fdy5KS2nn1\n54XEUjEODO5jd/9OdvXt5MDQvpkfC7P5zMfS8lYay5oYig7SO9lzwSv0+czHzUu38LtXf4YPN3zk\ngj94k9MJvrvvO3xr918Tn45TE1zCn33kUW5d9nFOjB3npRPbefnEdnb375qzefyqqnauq7+etbXr\nWVe3nsbSpsv+UX1w+F0+t/WPGJ4a5tol63j8ticzdhyDQlwkS5Kpad44Nszz7/Tys3299I/PPerd\n7zNW15XNhPra5hBXN1RQWhTIUsW5zznHZ7c+xBs9r9FQ2shEfIKJxPlDI1RUSVtlGytCbaysTN9W\nVLZRM+viMxPxCV7s2so/dTzHzr4z3z9LgrXcufIThIoqOTXRPXPrmTh1zgj1UkbfFzMWC/N85094\n9vAzHB45NNO+vu46/OZnd//OOYF1dfU13NJ6Kzcv3cKqqqsWZQvQYHSQw8MHOTRykCMjhzg0fJBj\n4aNzDpwDKPQVUl/aQH1pAw0z9418tPkm6ksbLvnvHQt38l9e+U+83b8LgPrSBvome2ce91uA6xs2\ncvPSLXxs6S00ljUtzBv1HB/r4rMvPETP5ClWhtr45u1/s6BzEZymEBf5AJiedrx9cpRdJ0Z5pzvM\nO6fCHOmfOOcgOYCWqiCr68q4qr6cVXVlrK4rZ3VdmcL9EnWFj/HAc58iPh0Hzg3rtspVrKxso6q4\nel7hdmqim3/u+DE/6fgxJ8aPX/B51cXVNJW10FTWzLq69dzXfj8B38LMrOecY//QPp49/Aw/6/zn\nmUvY+i3AxoYPc3PrFj62dAsN8wjDTIqn4nSOdtAf6aO2pJb6koZ59/t7mXbT/PDg9/nmW48TSUYo\nKyjnxpab+NjSLdzYfFPGTzfsn+zjT7b+EZ3hDhpLm3jyN5+itWLZgv4NhbjIB1Q0nmJ/zxj7TqVD\nfW/3GEf6x0mkzv/ZbK4Mzozc13q3xlCx9rOfR+doB8PRIVZUrrzsy7peiHOOvQNv8/OuFwBoKmum\n2QvtprKmRZuMJZKI8PKJ7fjMz0ebb8yJ8+MzZTg6xKmJbtqrr7nk3RULZXRqlIe3fZZ3BvdQXVzN\nEx//nwt6doRCXCSHJFLTdA1FONI/zqG+CQ73T3C4f5zOgUniqXNPf1lSVsi1TSHWtYRmwr2hQsEu\nspgiiQiPbH+YN3pe4ys3f5Xbl9+xYK+tEBfJA8nUNMeHIxzsG2dvd5i9J8Ps7Q7POXDutEK/j9ry\nIpaUFVFb7t285bryIq6qL2fFktIsvAuR/BVPxXmr901uaL5xQV9XIS6Sp5xznByJsqc7zJ6Tp/e1\njxE+T7CfbUVNKbddk56W9cPLqghoWlaRDySFuMgVJhpPMTgRo388xsBEjIHxGAPjUwyMx+gbi7Hz\n+Mg507Le0l7HbVfXcUt7HaHg4u5TFJELU4iLyByzp2X9+YE+OgbOnC/t9xnXt1axYWkla1tCrGsO\n0Vpdon3sIlmiEBeR93RscJKfe4H+5rERUmed91ZRHEgfDd9Sybrm9AF0TaFibYIXWQQKcRG5ZOFo\ngjePDc8cPLenO8zgROyc5/ksPe96Q0UxDaFiGiqKqffuG0PFtNeXU6N52EXeN4W4iFw25xx9YzH2\ndKcPnNtzMsz+njEGJmJc7Cukvb6cG1bWsLmths0rqqksyexEESL5SCEuIgsukZqmfzxGb3iKvrEp\nerz73rEpTo1G2dsdnjMPuxmsaazghpU13LCyhjVNFRQF/BT4jcKAjwKfD59P+91FzqYQF5FFF0um\n2H1ilFc7hnilc4jdx0fPe7Ga2QI+o8DvozDgo6wowPqWEB9ZUcOmFdW015cr5OWKpBAXkayLxlPs\nPD7Cq51DvNo5RNdQhGRqmkRqmkTKXTTgQ8ECPry8ms0rqtm0opo1jRU6sE6uCApxEfnAc86RSDkv\n1KcZnIjz5rFh3jg6zOtHhzgVnprz/PKiAP/xrjXcv3FplioWWRwXCnFNhyQiHxhmRmEgvX8coLKk\nkFV1ZTywqXXmynSve4H+xtFhuoYjPPrsXtY0VXBtUyjL1YssPm2HEpGcYGYsrS7hvutb+Np963np\nkS38y83LSKQcn396F9F46uIvIpJnFOIikrO+9NvXsKqujI6BSb7y/IFslyOy6BTiIpKzigv8fOPT\nGyjwG999rYttB/qyXZLIolKIi0hO+1BTiC/81tUAfOGZPQyMn3tlOZF8pRAXkZz34I0ruLGthqHJ\nOI/88G1y7awbkcuV0RA3szvM7KCZHTGzL57n8X9nZvvNbI+ZbTOzZZmsR0Tyk89n/Pff3UBlsIBf\nHBrgf7/ale2SRBZFxkLczPzAk8CdwBrgATNbc9bTdgEbnXPrgB8CX81UPSKS3xpCxfzXT64F4CvP\nH+BQ33iWKxLJvEyOxDcBR5xznc65OPA0cM/sJzjntjvnIt7qa0BLBusRkTx357WN3L+xhVhymn/z\n9C5iSZ12JvktkyHeDJyYtX7Sa7uQB4HnM1iPiFwBHrvrQyyvKeHd3nH+8oWD2S5HJKM+EAe2mdnv\nAxuBr13g8YfMbIeZ7RgYGFjc4kQkp5QWBXj809fh9xnf+uVRfnVkMNsliWRMJkO8G5h9QeMWr20O\nM/s48Chwt3PuvOeGOOeecs5tdM5trK2tzUixIpI/Niyt5OHbVgPw0Hd38PWfH2IylsxyVSILL5Mh\n/iaw2sxWmFkh8BngudlPMLPrgL8hHeD9GaxFRK4w//qWVdy9volIPMU3th3m5r/8Bd97vYvkRWZK\nE8klGQtx51wS+BPgZ8AB4AfOuX1m9mUzu9t72teAMuAfzGy3mT13gZcTEZkXv8944jPX8f2HNrO+\npZLBiRiPPvsOv/WNl9m6v0/nkkte0FSkIpL3nHP8ZG8PX/3ZQY4Pp0+I2bSimkfvvIb1SyuzXJ3I\nxWk+cRG54sWSKb73+nGe2HaY0WgCgNvX1HNjWw1rmytZ01hBsNCf5SpFzqUQFxHxhKMJ/vqlDr79\n66PEk2f2kft9xuq6Mta1hFjbXMm65hBXN5ZTFFCwS3YpxEVEztITjrJ1fx97u8Ps7Q5zqG+c6bO+\nEgM+o622jPaGcq72bu0NFTSFijGz7BQuVxyFuIjIRUTjKfb3hNlzMjwT7EcGJjjf12R5cYD2+nSo\nt9WWsaymhGU1pbRUBTVylwV3oRAPZKMYEZEPomChn+uXVXP9suqZtkg8yaG+CQ72jvFu7zgH+8Y5\n2DvO0GScHV0j7OgamfMaZtAUCs6E+rLqEiqCBYSjCcaiCcLebWzqzHo8OU1zVZDW6lKWz/p3y5eU\nEgoWLHY3SA7RSFxE5DIMjMc42DfOu71jHBucpGsowrGhSbpHo+dskn8/QsECllWXUF1aSEWwgIri\ngHdfcGa9uIDy4gDlxQFKiwKUFQUoLQzg82lzf77QSFxEZAHVlhdRW17ETauWzGmPJ6fpHo1ybGiS\n416wRxMpQrOCN+SFbyiYXi/w+zg5EuHYUISuofQPgq7hCMeHJglHE+zpDl9WjWVeoJcVp0O9tMhP\nSaGfksLAWfd+yooCNIaCtFQFaa4KUlGsLQC5QCEuIrKACgM+ViwpZcWS0nn9u9bqEj7aNrfNOcfg\nRJzjwxHC0Thj0WR6M/xU4sxyNMHYVJLxqSQTsQQTsSQTU0km46n0ciwJY/N/HxXFAVqqSmipCtJS\nVUJzZZCSQj+nNzI452Ytn2lLTjumpx0p50hNn7lNO4ff56OpspjW6hJaq0uoLy9e9K0Fzrm8OiBR\nIS4i8gFlZjMj/vlKTTsm416gx5KMx5JE4ykm40kisRSRRIpILB32kXiSsakkPaNRTo5GOTkSYWwq\nyf6eMfb3XMYvgEtU6PfRUhVkqRfqLVVBKoMFlBQFKCsMUFLk97YgBCgt9FNSFCA17YjE0+9pIpZi\nMpZkctb6RCzJaCROOJpgNJI+5mA0miAciTMaTRCJpygrClBZUkBlsIDKksKZ5aqSQkLBAsyM5PQ0\nyZQjkZomOZ2+T6QcydQ0Ab+PiuIA5cUFVAQDc3dteFtcyosWZ3eGQlxEJA/5fZYOl8vYLO6cY2gy\nzsmRKN1eqHePRIl7152fiSazmWWzdHvA58PnM/w+8Pt8+C1di9/nI55McXIkyvHhCCdGogxOxOgc\nnKRzcHIh3vIlO72F4uRINGN/45sPXMdd65oy9vqnKcRFRGQOM2NJWRFLyorYkMHL0kbiSU4MRzkx\nEuH4cISTI1HGp9Kj5TMj7NPL6Xu/zygt8nuj8/R+/rKZ5cCZUbY3qq4MFhDyRtqhYAGlhQHGY0nC\n0QQjk+nR+WgkzmgkMbPsgAK/EfD5KPD70st+HwU+I+A3kinH2NTc3Rmnl8enkoxFE4t2TIFCXERE\nsqKkMEB7QzntDeWL+ndDXqC3Vpcs6t/NhExORSoiIiIZpBAXERHJUQpxERGRHKUQFxERyVEKcRER\nkRylEBcREclRCnEREZEcpRAXERHJUQpxERGRHKUQFxERyVEKcRERkRylEBcREclRCnEREZEcZc65\nbNcwL2Y2AHTN458sAQYzVM6VRn25cNSXC0d9uTDUjwsnE325zDlXe3ZjzoX4fJnZDufcxmzXkQ/U\nlwtHfblw1JcLQ/24cBazL7U5XUREJEcpxEVERHLUlRDiT2W7gDyivlw46suFo75cGOrHhbNofZn3\n+8RFRETy1ZUwEhcREclLeR3iZnaHmR00syNm9sVs15NLzOzbZtZvZu/Maqs2s61mdti7r8pmjbnA\nzJaa2XYz229m+8zs8167+nKezKzYzN4ws7e9vvzPXvsKM3vd+5x/38wKs11rrjAzv5ntMrN/8tbV\nl5fBzI6Z2V4z221mO7y2RfmM522Im5kfeBK4E1gDPGBma7JbVU75X8AdZ7V9EdjmnFsNbPPW5b0l\ngT91zq0BNgOf9f4fqi/nLwbc6pxbD2wA7jCzzcB/A77unFsFjAAPZrHGXPN54MCsdfXl5dvinNsw\n69SyRfmM522IA5uAI865TudcHHgauCfLNeUM59zLwPBZzfcA3/GWvwPcu6hF5SDnXI9zbqe3PE76\nC7MZ9eW8ubQJb7XAuzngVuCHXrv68hKZWQvwCeBvvXVDfbmQFuUzns8h3gycmLV+0muTy1fvnOvx\nlnuB+mwWk2vMbDlwHfA66svL4m3+3Q30A1uBDmDUOZf0nqLP+aV7HPgCMO2t16C+vFwOeMHM3jKz\nh7y2RfmMBzLxopL/nHPOzHRqwyUyszLgGeBh59xYetCTpr68dM65FLDBzCqBHwFXZ7mknGRmdwH9\nzrm3zOyWbNeTB25yznWbWR2w1czenf1gJj/j+TwS7waWzlpv8drk8vWZWSOAd9+f5XpygpkVkA7w\n7znn/tFrVl++D865UWA7cANQaWanByT6nF+aG4G7zewY6V2NtwLfQH15WZxz3d59P+kfl5tYpM94\nPof4m8Bq72jLQuAzwHNZrinXPQf8gbf8B8D/y2ItOcHbz/h3wAHn3P+Y9ZD6cp7MrNYbgWNmQeB2\n0scYbAfu856mvrwEzrn/4Jxrcc4tJ/3d+KJz7vdQX86bmZWaWfnpZeA3gXdYpM94Xl/sxcx+m/R+\nHz/wbefcX2S5pJxhZv8XuIX0bDx9wGPAs8APgFbSM8nd75w7++A3mcXMbgJ+CezlzL7HL5HeL66+\nnAczW0f6ACE/6QHID5xzXzazlaRHk9XALuD3nXOx7FWaW7zN6f/eOXeX+nL+vD77kbcaAP6Pc+4v\nzKyGRfiM53WIi4iI5LN83pwuIiKS1xTiIiIiOUohLiIikqMU4iIiIjlKIS4iIpKjFOIiVxgzS3mz\nLZ2+LdjEDGa2fPbMdyKSWbrsqsiVJ+qc25DtIkTk/dNIXESAmTmRv+rNi/yGma3y2peb2YtmtsfM\ntplZq9deb2Y/8ub3ftvMPuq9lN/MvuXN+f2Cd3U1EckAhbjIlSd41ub0T896LOycWwt8k/TVDgH+\nCviOc24d8D3gCa/9CeAlb37v3wD2ee2rgSedcx8CRoFPZfj9iFyxdMU2kSuMmU0458rO034MuNU5\n1+lN2tLrnKsxs0Gg0TmX8Np7nHNLzGwAaJl9WU5vutWtzrnV3vqfAQXOuT/P/DsTufJoJC4is7kL\nLM/H7Gttp9CxNyIZoxAXkdk+Pev+VW/5FdIzXQH8HukJXQC2AX8MYGZ+MwstVpEikqZfyCJXnqCZ\n7Z61/lPn3OnTzKrMbA/p0fQDXtvngL83s0eAAeAPvfbPA0+Z2YOkR9x/DPRkvHoRmaF94iICzOwT\n3+icG8x2LSJyabQ5XUREJEdpJC4iIpKjNBIXERHJUQpxERGRHKUQFxERyVEKcRERkRylEBcREclR\nCnEREZEc9f8BJpUDiUCSON0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeYqbOh_PRYT",
        "colab_type": "code",
        "outputId": "c914e301-e500-458d-bca1-a49eb7a0f29f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "net.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): LambdaLayer()\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): LambdaLayer()\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx04vlqK33Cn",
        "colab_type": "code",
        "outputId": "ff3fb86f-c59d-4ba9-ed9e-26f28673ba1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "def dataset_accuracy(net, data_loader, name=\"\"):\n",
        "    net = net.to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in data_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum()\n",
        "    accuracy = 100 * float(correct) / total\n",
        "    print('Accuracy of the network on the {} {} images: {:.2f} %'.format(total, name, accuracy))\n",
        "\n",
        "def train_set_accuracy(net):\n",
        "    dataset_accuracy(net, train_loader, \"train\")\n",
        "\n",
        "def val_set_accuracy(net):\n",
        "    dataset_accuracy(net, val_loader, \"validation\")  \n",
        "    \n",
        "def test_set_accuracy(net):\n",
        "    dataset_accuracy(net, test_loader, \"test\")\n",
        "\n",
        "def compute_accuracy(net):\n",
        "    train_set_accuracy(net)\n",
        "    val_set_accuracy(net)\n",
        "    test_set_accuracy(net)\n",
        "    \n",
        "print(\"Computing accuracy...\")\n",
        "compute_accuracy(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computing accuracy...\n",
            "Accuracy of the network on the 45000 train images: 96.40 %\n",
            "Accuracy of the network on the 5000 validation images: 89.90 %\n",
            "Accuracy of the network on the 10000 test images: 89.02 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}